@article{Deepak_2024,
  title={ A semantic ontology infused deep learning model for disaster tweet classification },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-16840-6 },
  journal={ Multimedia Tools and Applications },
  author={ Karnati Sai Venkata GiriGerard Deepak },
  year={ 2024 },
  pages={ 62257-62285 },
  month={ 09 },
  abstract={ Twitter has emerged as a powerful Microblogging site in recent years. It has become one of the immediately reported platforms about any crisis in today’s world. During a crisis twitter is flooded with both informational and non-informational tweets. Informational tweets are the tweets which can help humanitarian response organizations or the victims to get useful information for better decision making This paper proposes a Semantic based Ontology Infused deep learning model for Informative tweet classification during natural disasters. The proposed model makes use of both text and image features present in a tweet during a classification, a Hierarchical ensemble of Vanilla and Bi-LSTM’s is used along with ontologies and external knowledge from OntoCollab for text classification, a dilated convolution-based residual neural network is used for image classification, and a meaningful statistical fusion is performed with the results obtained from text and image for tweet classification. The usage of both image and text features and contextual understanding made the model distinctive and achieved exceptional results. The proposed method has proven, after experiments, to achieve evaluation metrics better than the up-to-date existing methods; evaluated on seven different datasets and compared with eight other methods with an increase in Accuracy and decrease in FNR for seven distinguished datasets which resulted an highest F1 score of 98.416% on Sri Lanka Floods dataset. },
  issn={ 1573-7721 },
}

@article{Gu_2024,
  title={ Cognitive differences in product shape evaluation between real settings and virtual reality: case study of two-wheel electric vehicles },
  volume={ 28 },
  DOI={ doi:10.1007/s10055-024-01034-8 },
  journal={ Virtual Reality },
  author={ Zhengjie DuanJiajun ZhouFangzhou Gu },
  year={ 2024 },
  pages={ 1-14 },
  month={ 07 },
  abstract={ Product shape evaluation is an important part of new product development. In the shape design stage, design schemes are often presented through visual images. The presentation of visual images causes evaluators to form different cognitive experiences and evaluation results. In recent years, virtual reality (VR) technology has been widely used in the field of industrial design, enriching the presentation forms of design scheme images. Although VR technology has shown the potential to improve evaluators’ perception and cognitive experiences in product shape design, research comparing it with traditional methods remains relatively scattered. This study used two-wheel electric vehicles as an example to examine the difference in evaluators’ cognition of product shape in VR and a real setting (RS). First, we established a semantic scale comprising seven pairs of opposite adjectives to evaluate the shape scheme. Second, we built VR and RS evaluation environments using head-mounted displays and paper renderings, respectively. The participants evaluated the vehicle shape design in alternating viewing and underwent semi-structured interviews on cognitive experience. We analyzed the experimental and interview results based on three aspects of product shape cognition. The results demonstrated that volume cognition was significantly more accurate in VR environments. Furthermore, graphic cognition, particularly regarding shape details, differed partially between environments. VR provided a better sense of immersion and more variable viewing angles than RS. Treatment cognition did not exhibit significant differences between environments, as it depended on human experience rather than visualization. These findings suggest that VR tools are more suited for shaping design evaluations early. Selecting suitable visual presentation tools based on evaluators’ cognitive characteristics at different evaluation nodes to display design schemes is a practical, economical, and efficient strategy. },
  issn={ 1434-9957 },
}

@article{Reza_2023,
  title={ Deep learning-based collaborative filtering recommender systems: a comprehensive and systematic review },
  volume={ 35 },
  DOI={ doi:10.1007/s00521-023-08958-3 },
  journal={ Neural Computing and Applications },
  author={ Atena TorkashvandSeyed Mahdi JameiiAkram Reza },
  year={ 2023 },
  pages={ 24783-24827 },
  month={ 10 },
  abstract={ Nowadays, the volume of online information is growing and it is difficult to find the required information. Effective strategies such as recommender systems are required to overcome information overload. Collaborative filtering is a widely used type of recommender system in e-commerce environments and can simply provide suggestions for users. Recently, deep learning approaches were applied in collaborative filtering to tackle some drawbacks. This systematic review aims to provide a comprehensive review of recent research efforts on deep learning-based collaborative filtering recommender systems. We explain the research methodology and paper selection process and the search query. 102 papers are selected out of 719 papers that were published between 2019 and May 2023. Furthermore, the approaches in the selected papers are classified into two main categories: memory-based and model-based techniques. The main ideas, advantages, disadvantages, used tools, type of neural network, applications, and evaluation parameters of each selected paper are also discussed in detail. It was found that CNN (Convolutional Neural Network), AE (Autoencoder), DNN (Deep neural network), and Hybrid networks are the four mostly used neural networks in recommender systems. Also, Python, MATLAB, and Java are the most frequently used tools in the reviewed papers. Regarding the applications of the recommender systems in the reviewed papers, movies, products, and music recommendation are three most frequent applications. We point out the open issues and future research directions. Some key challenges such as cold start, data sparsity, scalability, and accuracy are still open to be addressed. },
  issn={ 1433-3058 },
}

@article{Banerjee_2023,
  title={ A review of platforms for simulating embodied agents in 3D virtual environments },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-022-10253-x },
  journal={ Artificial Intelligence Review },
  author={ Deepti Prit KaurNarinder Pal SinghBonny Banerjee },
  year={ 2023 },
  pages={ 3711-3753 },
  month={ 09 },
  abstract={ The unprecedented rise in research interest in artificial intelligence (AI) and related areas, such as computer vision, machine learning, robotics, and cognitive science, during the last decade has fuelled the development of software platforms that can simulate embodied agents in 3D virtual environments. A simulator that closely mimics the physics of a real-world environment with embodied agents can allow open-ended experimentation, and can circumvent the need for real-world data collection, which is time-consuming, expensive, and in some cases, impossible without privacy invasion, thereby playing a significant role in progressing AI research. In this article, we review 22 simulation platforms reported in the literature. We classify them based on visual environment and physics. We present a comparison of these simulators based on their properties and functionalities from a user’s perspective. While no simulator is better than the others in all respects, a few stand out based on a rubric that encompasses the simulators’ properties, functionalities, availability and support. This review will guide users to choose the appropriate simulator for their application and provide a baseline to researchers for developing state-of-the-art simulators. },
  issn={ 1573-7462 },
}

@article{Chen_2024,
  title={ Defense against adversarial attacks via textual embeddings based on semantic associative field },
  volume={ 36 },
  DOI={ doi:10.1007/s00521-023-08946-7 },
  journal={ Neural Computing and Applications },
  author={ Jiacheng HuangLong Chen },
  year={ 2024 },
  pages={ 289-301 },
  month={ 11 },
  abstract={ Deep neural networks are known to be vulnerable to various types of adversarial attacks, especially word-level attacks, in the field of natural language processing. In recent years, various defense methods are proposed against word-level attacks; however, most of those defense methods only focus on synonyms substitution-based attacks, while word-level attacks are not based on synonym substitution. In this paper, we propose a textual adversarial defense method against word-level adversarial attacks via textual embedding based on the semantic associative field. More specifically, we analyze the reasons why humans can read and understand textual adversarial examples and observe two crucial points: (1) There must be a relation between the original word and the perturbed word or token. (2) Such a kind of relation enables humans to infer original words, while humans have the ability to associations. Motivated by this, we introduce the concept of semantic associative field and propose a new defense method by building a robust word embedding, that is, we calculate the word vector by exerting the related word vector to it with potential function and weighted embedding sampling for simulating the semantic influence between words in same semantic field. We conduct comprehensive experiments and demonstrate that the models using the proposed method can achieve higher accuracy than the baseline defense methods under various adversarial attacks or original testing sets. Moreover, the proposed method is more universal, while it is irrelevant to model structure and will not affect the efficiency of training. },
  issn={ 1433-3058 },
}

@article{Qian_2023,
  title={ Light-Deeplabv3+: a lightweight real-time semantic segmentation method for complex environment perception },
  volume={ 21 },
  DOI={ doi:10.1007/s11554-023-01380-x },
  journal={ Journal of Real-Time Image Processing },
  author={ Peng DingHuaming Qian },
  year={ 2023 },
  pages={ 1-12 },
  month={ 11 },
  abstract={ Current semantic segmentation methods have high accuracy. However, it has the disadvantage of high computational complexity and time consumption, which makes it difficult to meet the application requirements in complex environments. To achieve fast and accurate semantic segmentation of images, we propose a lightweight semantic segmentation method called Light-Deeplabv3+. First, a MobileNetV2Lite-SE architecture with SE module is proposed as the backbone network of the model, which can reduce the number of model parameters and improve the segmentation speed. Second, we propose an ACsc-ASPP module based on asymmetric dilated convolution block (ADCB) and scSE module to solve the semantic information loss during feature extraction. Our improvements can obtain more semantic features and improve segmentation accuracy. Finally, we propose a DSC-Blaze module to replace the original $$3\times 3$$ standard convolution. It consists of depthwise separable convolution (DSC) and Blaze module, which can improve the model segmentation speed while maintaining the receptive field. The experimental results prove that the Mean Intersection over Union (MIoU) of Light-Deeplabv3+ on the PASCAL VOC2012 dataset is 73.15 $$\%$$ , and the parameter size is only 6.291MB. Its calculation amount is only 20.883G, and the speed on the 3060Ti platform is 37.66 frames per second (FPS). Compared with traditional Deeplabv3+, Light-Deeplabv3+ can achieve efficient and accurate image segmentation results with less computational overhead, and its performance is comparable to state-of-the-art algorithms. },
  issn={ 1861-8219 },
}

@article{Zhu_2024,
  title={ Toward Human-centered XAI in Practice: A survey },
  volume={ 21 },
  DOI={ doi:10.1007/s11633-022-1407-3 },
  journal={ Machine Intelligence Research },
  author={ Xiangwei KongShujie LiuLuhao Zhu },
  year={ 2024 },
  pages={ 740-770 },
  month={ 01 },
  abstract={ Human adoption of artificial intelligence (AI) technique is largely hampered because of the increasing complexity and opacity of AI development. Explainable AI (XAI) techniques with various methods and tools have been developed to bridge this gap between high-performance black-box AI models and human understanding. However, the current adoption of XAI technique still lacks “human-centered” guidance for designing proper solutions to meet different stakeholders’ needs in XAI practice. We first summarize a human-centered demand framework to categorize different stakeholders into five key roles with specific demands by reviewing existing research and then extract six commonly used human-centered XAI evaluation measures which are helpful for validating the effect of XAI. In addition, a taxonomy of XAI methods is developed for visual computing with analysis of method properties. Holding clearer human demands and XAI methods in mind, we take a medical image diagnosis scenario as an example to present an overview of how extant XAI approaches for visual computing fulfil stakeholders’ human-centered demands in practice. And we check the availability of open-source XAI tools for stakeholders’ use. This survey provides further guidance for matching diverse human demands with appropriate XAI methods or tools in specific applications with a summary of main challenges and future work toward human-centered XAI in practice. },
  issn={ 2731-5398 },
}

@article{Moreira_2023,
  title={ A systematic review of integrated information theory: a perspective from artificial intelligence and the cognitive sciences },
  DOI={ doi:10.1007/s00521-023-08328-z },
  journal={ Neural Computing and Applications },
  author={ Luz Enith GuerreroLuis Fernando CastilloJeferson Arango-LópezFernando Moreira },
  year={ 2023 },
  pages={ 1-33 },
  month={ 02 },
  abstract={ The study of consciousness has gained momentum in recent years by the scientific community. In this same sense, the relationship between cognitive sciences and artificial intelligence presents a fundamental theoretical framework in the study of integrated information theory (IIT) as a theory that makes its way into the knowledge of consciousness. However, there are few studies that integrate these topics and a systematic review of the literature is highly pertinent. This paper seeks to identify methods, methodologies or computational solutions using artificial intelligence and cognitive science fundamentals that can provide some kind of solution to the challenges posed by IIT. },
  issn={ 1433-3058 },
}

@article{Tommasino_2023,
  title={ A combined approach for improving humanoid robots autonomous cognitive capabilities },
  volume={ 65 },
  DOI={ doi:10.1007/s10115-023-01844-3 },
  journal={ Knowledge and Information Systems },
  author={ Kurosh MadaniAntonio M. RinaldiCristiano RussoCristian Tommasino },
  year={ 2023 },
  pages={ 3197-3221 },
  month={ 03 },
  abstract={ Recent technologies advancements promise to change our lives dramatically in the near future. A new different living society is progressively emerging, witnessed from the conception of novel digital ecosystems, where humans are expected to share their own spaces and habits with machines. Humanoid robots are more and more being developed and provided with enriched functionalities; however, they are still lacking in many ways. One important goal in this sense is to enrich their cognitive capabilities, to make them more “intelligent” in order to better support humans in both daily and special activities. The goal of this research is to set a step in bridging the gap between symbolic AI and connectionist approaches in the context of knowledge acquisition and conceptualization. Hence, we present a combined approach based on semantics and machine learning techniques for improving robots cognitive capabilities. This is part of a wider framework that covers several aspects of knowledge management, from representation and conceptualization, to acquisition, sharing and interaction with humans. Our focus in this work is in particular on the development and implementation of techniques for knowledge acquisition. Such techniques are discussed and validated through experiments, carried out on a real robotic platform, showing the effectiveness of our approach. The results obtained confirmed that the combination of the approaches gives superior performance with respect to when they are considered individually. },
  issn={ 0219-3116 },
}

@article{Bugarín-Diz_2023,
  title={ Trustworthy artificial intelligence in Alzheimer’s disease: state of the art, opportunities, and challenges },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-023-10415-5 },
  journal={ Artificial Intelligence Review },
  author={ Shaker El-SappaghJose M. Alonso-MoralTamer AbuhmedFarman AliAlberto Bugarín-Diz },
  year={ 2023 },
  pages={ 11149-11296 },
  month={ 03 },
  abstract={ Medical applications of Artificial Intelligence (AI) have consistently shown remarkable performance in providing medical professionals and patients with support for complex tasks. Nevertheless, the use of these applications in sensitive clinical domains where high-stakes decisions are involved could be much more extensive if patients, medical professionals, and regulators were provided with mechanisms for trusting the results provided by AI systems. A key issue for achieving this is endowing AI systems with key dimensions of Trustworthy AI (TAI), such as fairness, transparency, robustness, or accountability, which are not usually considered within this context in a generalized and systematic manner. This paper reviews the recent advances in the TAI domain, including TAI standards and guidelines. We propose several requirements to be addressed in the design, development, and deployment of TAI systems and present a novel machine learning pipeline that contains TAI requirements as embedded components. Moreover, as an example of how current AI systems in medicine consider the TAI perspective, the study extensively reviews the recent literature (2017–2021) on AI systems in a prevalent and high social-impact disease: diagnosis and progression detection of Alzheimer’s Disease (AD). The most relevant AI systems in the AD domain are compared and discussed (such as machine learning, deep learning, ensembles, time series, and multimodal multitask) from the perspective of how they address TAI in their design. Several open challenges are highlighted, which could be claimed as one of the main reasons to justify the rare application of AI systems in real clinical environments. The study provides a roadmap to measure the TAI status of an AI systems and highlights its limitations. In addition, it provides the main guidelines to overcome these limitations and build medically trusted AI-based applications in the medical domain. },
  issn={ 1573-7462 },
}

@article{Hou_2024,
  title={ Dual BiGRU-CNN-based sentiment classification method combining global and local attention },
  volume={ 80 },
  DOI={ doi:10.1007/s11227-023-05558-9 },
  journal={ The Journal of Supercomputing },
  author={ Youwei WangLizhou FengAo LiuWeiqi WangYudong Hou },
  year={ 2024 },
  pages={ 2799-2837 },
  month={ 08 },
  abstract={ Traditional sentiment classification methods often ignore the influence of psychological features on classification results. Moreover, most sentiment classification methods fail to effectively integrate the context semantic information of local structural features and mine the interaction between context semantic features and local structural features. On this basis, we proposed a novel dual BiGRU-CNN-based sentiment classification method combining global and local attention. First, in order to integrate psychological information into sentiment classification, the advantage of the language query and word count model (LIWC) on effectively expressing the users’ psychological features is utilized, and a heterogeneous graph attention networks (HAN) and LIWC-based text representation learning method (called HAN_LIWC) are proposed to improve the representation of each document. Then, we propose a new sentiment classification framework (called BCAT) in which the word features with context semantic information and the local structural features are comprehensively considered to improve the sentiment classification accuracy. Finally, we introduce the global attention layer and the local attention layer. In order to utilize the mutual effect between different features, the global attention layer focuses on mining the interaction between word features, the interaction between local structural features, and the interaction between word features and local structural features. Moreover, the local attention layer focuses on mining the contributions of all features to the final classification results. Experimental results on three datasets show that the proposed algorithm has greatly improved classification accuracy compared to typical traditional machine learning-based methods and deep learning-based methods. },
  issn={ 1573-0484 },
}

@article{Wang_2024,
  title={ A Review of Key Technologies for Emotion Analysis Using Multimodal Information },
  volume={ 16 },
  DOI={ doi:10.1007/s12559-024-10287-z },
  journal={ Cognitive Computation },
  author={ Xianxun ZhuChaopeng GuoHeyang FengYao HuangYichen FengXiangyang WangRui Wang },
  year={ 2024 },
  pages={ 1504-1530 },
  month={ 06 },
  abstract={ Emotion analysis, an integral aspect of human–machine interactions, has witnessed significant advancements in recent years. With the rise of multimodal data sources such as speech, text, and images, there is a profound need for a comprehensive review of pivotal elements within this domain. Our paper delves deep into the realm of emotion analysis, examining multimodal data sources encompassing speech, text, images, and physiological signals. We provide a curated overview of relevant literature, academic forums, and competitions. Emphasis is laid on dissecting unimodal processing methods, including preprocessing, feature extraction, and tools across speech, text, images, and physiological signals. We further discuss the nuances of multimodal data fusion techniques, spotlighting early, late, model, and hybrid fusion strategies. Key findings indicate the essentiality of analyzing emotions across multiple modalities. Detailed discussions on emotion elicitation, expression, and representation models are presented. Moreover, we uncover challenges such as dataset creation, modality synchronization, model efficiency, limited data scenarios, cross-domain applicability, and the handling of missing modalities. Practical solutions and suggestions are provided to address these challenges. The realm of multimodal emotion analysis is vast, with numerous applications ranging from driver sentiment detection to medical evaluations. Our comprehensive review serves as a valuable resource for both scholars and industry professionals. It not only sheds light on the current state of research but also highlights potential directions for future innovations. The insights garnered from this paper are expected to pave the way for subsequent advancements in deep multimodal emotion analysis tailored for real-world deployments. },
  issn={ 1866-9964 },
}

@article{Zheng_2024,
  title={ Handling noisy labels via one-step abductive multi-target learning and its application to helicobacter pylori segmentation },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-17743-2 },
  journal={ Multimedia Tools and Applications },
  author={ Yongquan YangYiming YangJie ChenJiayi ZhengZhongxi Zheng },
  year={ 2024 },
  pages={ 65099-65147 },
  month={ 01 },
  abstract={ Learning from noisy labels is an important concern in plenty of real-world scenarios. Various approaches for this concern first make corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. For the problem 1), we present one-step abductive multi-target learning (OSAMTL) that imposes a one-step logical reasoning upon machine learning via a multi-target learning procedure to constrain the predictions of the learning model to be subject to our prior knowledge about the true target. For the problem 2), we propose a logical assessment formula (LAF) that evaluates the logical rationality of the outputs of an approach by estimating the consistencies between the predictions of the learning model and the logical facts narrated from the results of the one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H. pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine learning model achieving logically more rational predictions, which is beyond various state-of-the-art approaches in handling complex noisy labels. },
  issn={ 1573-7721 },
}

@article{Wang_2023,
  title={ Cross-status communication and project outcomes in OSS development },
  volume={ 28 },
  DOI={ doi:10.1007/s10664-023-10298-8 },
  journal={ Empirical Software Engineering },
  author={ Yisi HanZhendong WangYang FengZhihong ZhaoYi Wang },
  year={ 2023 },
  pages={ 1-36 },
  month={ 05 },
  abstract={ The success of an open source software (OSS) project requires effective communication among its members. Given that OSS projects often have established social status systems, such communication may happen between individuals of different statuses, particularly, elite developers with project management privileges and ordinary project contributors. They communicate with each other on many essential activities, e.g., bug fixing, code review, etc., thus having profound influences on project outcomes. We seek to develop an understanding of cross-status communication from a perspective of language style matching among developers of different status, and its relationships with an OSS project’s outcomes in terms of productivity and quality. We approach the above research objectives with the language style matching (LSM) tool, which measures the similarities of cross-status communication in multiple language style features. We first dynamically identify elite developers having project administration privileges for each sampled project. Then, we capture the cross-status communication between elite and non-elite developers; and calculate the LSM features of these two groups of individuals. The LSM variables, together with project outcomes, were used to fit regression models to analyze potential relationships between cross-status communication’s language matching and project outcomes. Using over 275,000 collected conversations, our analyses yield rich insights into cross-status communication in open source development. First, our results reveal that the elite and non-elite developers exhibit quite similar linguistic patterns in using certain categories of words. Second, we explore the relationships between linguistic similarity in cross-status communication and project outcomes. The regression results are generally negative, indicating there might be very limited significant relationships between cross-status communication’s language matching and project outcomes, with a few exceptions. The study has several limitations. First, it considers projects hosted on GitHub only. Second, to ensure data availability, our sample is drawn from top projects, thus not representing all projects. Third, we only consider a limited number of linguistic features, and indicators for project outcomes. This study is developed from the registered report available at: https://arxiv.org/abs/2104.05538 . This registered report was accepted at the MSR 2021 Registered Reports Track. },
  issn={ 1573-7616 },
}

@article{Saxena_2024,
  title={ Mediating effects of NLP-based parameters on the readability of crowdsourced wikipedia articles },
  volume={ 54 },
  DOI={ doi:10.1007/s10489-024-05399-w },
  journal={ Applied Intelligence },
  author={ Simran SetiaAnamika ChhabraAmit Arjun VermaAkrati Saxena },
  year={ 2024 },
  pages={ 4370-4391 },
  month={ 03 },
  abstract={ In this era of information and communication technology, a large population relies on the Internet to gather information. One of the most popular information sources on the Internet is Wikipedia. Wikipedia is a free encyclopedia that provides a wide range of information to its users. However, there have been concerns about the readability of information on Wikipedia time and again. The readability of the text is defined as the ease of understanding the underlying text. Past studies have analyzed the readability of Wikipedia articles with the help of conventional readability metrics, such as the Flesch-Kincaid readability score and the Automatic Readability Index (ARI). Such metrics only consider the surface-level parameters, such as the number of words, sentences, and paragraphs in the text, to quantify the readability. However, the readability of the text must also take into account the quality of the text. In this study, we consider many new NLP-based parameters capturing the quality of the text, such as lexical diversity, semantic diversity, lexical complexity, and semantic complexity and analyze their impact on the readability of Wikipedia articles using artificial neural networks. Besides NLP parameters, the crowdsourced parameters also affect the readability, and therefore, we also analyze the impact of crowdsourced parameters and observe that the crowdsourced parameters not only influence the readability scores but also affect the NLP parameters of the text. Additionally, we investigate the mediating effect of NLP parameters that connect the crowdsourced parameters to the readability of the text. The results show that the impact of crowdsourced parameters on readability is partially due to the profound effect of NLP-based parameters. },
  issn={ 1573-7497 },
}

@article{Ferri-Molla_2024,
  title={ Environment awareness, multimodal interaction, and intelligent assistance in industrial augmented reality solutions with deep learning },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-17516-x },
  journal={ Multimedia Tools and Applications },
  author={ Juan Izquierdo-DomenechJordi Linares-PellicerIsabel Ferri-Molla },
  year={ 2024 },
  pages={ 49567-49594 },
  month={ 11 },
  abstract={ Augmented reality is increasingly used in various fields, especially industrial applications. Although augmented reality devices’ characteristics and technological benefits are still evolving, augmented reality’s clear advantages in facilitating mechanical tasks and improving operator performance have made it popular. In industrial settings, the human factor remains irreplaceable, but the evolution of artificial intelligence has allowed any activity on the shop floor to be given new semantic possibilities. Through a semantic layer, it is possible to interpret and validate the environment, provide multimodal interaction, and analyze and evaluate information to detect anomalies or risky situations. Deep learning has opened up new possibilities for existing augmented reality solutions, such as visual interpretation of the environment, natural language understanding for problem-solving, or automatic anomaly detection. This new intelligent layer minimizes unnecessary interactions with the environment, validates the operator’s actions, and increases comfort, safety, and focus, making them more efficient in high cognitive level tasks. This work presents a general architecture based on a Semantic layer that relies on augmented reality systems and validates its advantages in a real industrial setting. Overall, integrating artificial intelligence and augmented reality solutions in industrial settings offers significant potential for improving productivity, safety, and worker satisfaction. },
  issn={ 1573-7721 },
}

@article{Shereen_2024,
  title={ Brain Activity is Influenced by How High Dimensional Data are Represented: An EEG Study of Scatterplot Diagnostic (Scagnostics) Measures },
  volume={ 8 },
  DOI={ doi:10.1007/s41666-023-00145-2 },
  journal={ Journal of Healthcare Informatics Research },
  author={ Ronak EtemadpourSonali ShintreeA. Duke Shereen },
  year={ 2024 },
  pages={ 19-49 },
  month={ 12 },
  abstract={ Visualization and visual analytic tools amplify one’s perception of data, facilitating deeper and faster insights that can improve decision making. For multidimensional data sets, one of the most common approaches of visualization methods is to map the data into lower dimensions. Scatterplot matrices (SPLOM) are often used to visualize bivariate relationships between combinations of variables in a multidimensional dataset. However, the number of scatterplots increases quadratically with respect to the number of variables. For high dimensional data, the corresponding enormous number of scatterplots makes data exploration overwhelmingly complex, thereby hindering the usefulness of SPLOM in human decision making processes. One approach to address this difficulty utilizes Graph-theoretic Scatterplot Diagnostic (Scagnostics) to automatically extract a subset of scatterplots with salient features and of manageable size with the hope that the data will be sufficient for improving human decisions. In this paper, we use Electroencephalogram (EEG) to observe brain activity while participants make decisions informed by scatterplots created using different visual measures. We focused on 4 categories of Scagnostics measures: Clumpy, Monotonic, Striated, and Stringy. Our findings demonstrate that by adjusting the level of difficulty in discriminating between data sets based on the Scagnostics measures, different parts of the brain are activated: easier visual discrimination choices involve brain activity mostly in visual sensory cortices located in the occipital lobe, while more difficult discrimination choices tend to recruit more parietal and frontal regions as they are known to be involved in resolving ambiguities. Our results imply that patterns of neural activity are predictive markers of which specific Scagnostics measures most assist human decision making based on visual stimuli such as ours. },
  issn={ 2509-498X },
}

@article{Lin_2024,
  title={ Blind Multimodal Quality Assessment of Low-Light Images },
  DOI={ doi:10.1007/s11263-024-02239-9 },
  journal={ International Journal of Computer Vision },
  author={ Miaohui WangZhuowei XuMai XuWeisi Lin },
  year={ 2024 },
  pages={ 1-24 },
  month={ 10 },
  abstract={ Blind image quality assessment (BIQA) aims at automatically and accurately forecasting objective scores for visual signals, which has been widely used to monitor product and service quality in low-light applications, covering smartphone photography, video surveillance, autonomous driving, etc. Recent developments in this field are dominated by unimodal solutions inconsistent with human subjective rating patterns, where human visual perception is simultaneously reflected by multiple sensory information. In this article, we present a unique blind multimodal quality assessment (BMQA) of low-light images from subjective evaluation to objective score. To investigate the multimodal mechanism, we first establish a multimodal low-light image quality (MLIQ) database with authentic low-light distortions, containing image-text modality pairs. Further, we specially design the key modules of BMQA, considering multimodal quality representation, latent feature alignment and fusion, and hybrid self-supervised and supervised learning. Extensive experiments show that our BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark database. In particular, we also build an independent single-image modality Dark-4K database, which is used to verify its applicability and generalization performance in mainstream unimodal applications. Qualitative and quantitative results on Dark-4K show that BMQA achieves superior performance to existing BIQA approaches as long as a pre-trained model is provided to generate text descriptions. The proposed framework and two databases as well as the collected BIQA methods and evaluation metrics are made publicly available on https://charwill.github.io/bmqa.html . },
  issn={ 1573-1405 },
}

@article{Zhang_2023,
  title={ The application of neural network for software vulnerability detection: a review },
  volume={ 35 },
  DOI={ doi:10.1007/s00521-022-08046-y },
  journal={ Neural Computing and Applications },
  author={ Yuhui ZhuGuanjun LinLipeng SongJun Zhang },
  year={ 2023 },
  pages={ 1279-1301 },
  month={ 11 },
  abstract={ To date, being benefited from the ability of automated feature extraction and the performance of software vulnerability identification, deep learning techniques have attracted extensive attention in data-driven software vulnerability detection. Many methods based on deep learning have been proposed to speed up and intelligentize the process of vulnerability identification. Although these methods have shown significant advantages over traditional machine learning ones, there is an apparent gap between the deep learning-based detection systems and human experts in understanding potentially vulnerable code semantics. In some real-world vulnerability prediction scenarios, the performance of deep learning-based methods drops by more than 50% compared to these methods’ performance in experimental scenarios. We define this phenomenon as the perception gap by examining and reviewing the early software vulnerability detection approaches. Then, from the perspective of the perception gap, this paper profoundly explores the current software vulnerability detection methods and how existing solutions endeavor to narrow the perception gap and push forward the development of the field of interest. Finally, we summarize the challenges of this new field and discuss the possible future. },
  issn={ 1433-3058 },
}

@article{Fang_2024,
  title={ Open Set Recognition in Real World },
  volume={ 132 },
  DOI={ doi:10.1007/s11263-024-02015-9 },
  journal={ International Journal of Computer Vision },
  author={ Zhen YangJun YuePedram GhamisiShiliang ZhangJiayi MaLeyuan Fang },
  year={ 2024 },
  pages={ 3208-3231 },
  month={ 03 },
  abstract={ Open set recognition (OSR) constitutes a critical endeavor within the domain of computer vision, frequently deployed in applications, such as autonomous driving and medical imaging recognition. Existing OSR methodologies predominantly center on the acquisition of a profound association between image data and corresponding labels, facilitating the extraction of discriminative features instrumental for distinguishing novel categories. Nevertheless, real-world scenarios often introduce not only novel classes (referred to semantic shift) but also intricate environmental modifications that engender alterations in the distribution of established classes (termed as covariate shift). The latter phenomenon has the potential to undermine the robust correlation between images and labels established by conventional statistical correlation modeling approaches, consequently resulting in significant degradation of OSR performance. Causal correlation stands as the fundamental linkage between entities, routinely harnessed by humans to enhance their cognitive capacities for a more profound comprehension of the intricate world. With inspiration drawn from this perspective, our work herein introduces the causal inference-inspired open set recognition (CISOR) approach tailored for real-world OSR (RWOSR). CISOR represents the pioneering initiative to leverage the stability inherent in causal correlation to construct two pivotal modules: the covariate causal independence (CCI) module and the semantic causal uniqueness (SCU) module, both instrumental in addressing the RWOSR problem. The CCI module adeptly confronts the challenge of covariate shift by imposing constraints on the correlations between inter-class causal features. This strategy effectively mitigates the impact of spurious correlations between distinct categories on the generalization capacity of discriminative features. Furthermore, in order to counteract the issue of semantic shift, the SCU module harnesses correlations between causal features within the same class as constraints, thereby facilitating the extraction of resilient causal features endowed with superior discriminative capabilities. Empirical findings substantiate the superior efficacy of the proposed CIOSR method when compared to state-of-the-art approaches across diverse RWOSR benchmark datasets. The source code of this article will be available at https://github.com/yangzhen1252/RWOSR1 . },
  issn={ 1573-1405 },
}

@article{Reeja_2024,
  title={ RETRACTED ARTICLE: Memory-guided visual attention generative adversarial network for colorization of nighttime thermal infrared images },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-17030-0 },
  journal={ Multimedia Tools and Applications },
  author={ B. MaheswariS. R. Reeja },
  year={ 2024 },
  pages={ 17679-17697 },
  month={ 10 },
  abstract={ Color images offer more information and are more detail-specific compared to typical grayscale images. The frequency level of the images captured by the near-infrared (NIR) image sensors is lower than that of visible light. The photographs are of extremely poor quality and are not visually pleasing due to the lack of natural colors. Since improving the visible quality of the images has become a priority due to this problem, image colorization techniques have been developed and applied. However, it will be extremely complicated because keeping the images’ original nature and characteristics is challenging when turning them into color images. As a result, this study introduces a novel image colorization strategy for NTIR images based on the GAN model. The generator module, which includes an encoder for both the DC (daytime color images) and NTIR images, collects the input images and performs a translation on them. The visual attention mechanism is introduced to the generator module to capture the essential semantic data about the images. In order to improve the semantic preservation of small sample categories, the framework also includes a memory-guided sample selection technique with an adaptive visual attention loss function. The proposed framework’s memory unit keeps the pseudo-labels that have been extracted and improved by the online semantic distillation module. In addition, the system includes a structured gradient alignment loss function to maintain edge coherence between the translated and input pictures. To apply the proposed model, the cityscape dataset is used. The proposed model is compared to other existing models in terms of non-linear correlation information entropy, precision, recall, means structural similarity, mutual information, entropy, correlation coefficient, average gradient, standard deviation, and sum of correlation of differences and achieved the best results among them. The precision and recall values of the proposed model are 96% and 95% on the Cityscape dataset. },
  issn={ 1573-7721 },
}

@article{Yu_2024,
  title={ Diagnostic Potential of Eye Movements in Alzheimer’s Disease via a Multiclass Machine Learning Model },
  volume={ 16 },
  DOI={ doi:10.1007/s12559-024-10346-5 },
  journal={ Cognitive Computation },
  author={ Jiaqi SongHaodong HuangJiarui LiuJiani WuYingxi ChenLisong WangFuxin ZhongXiaoqin WangZihan LinMengyu YanWenbo ZhangXintong LiuXinyi TangYang LüWeihua Yu },
  year={ 2024 },
  pages={ 3364-3378 },
  month={ 08 },
  abstract={ Early diagnosis plays a crucial role in controlling Alzheimer’s disease (AD) progression and delaying cognitive decline. Traditional diagnostic tools present great challenges to clinical practice due to their invasiveness, high cost, and time-consuming administration. This study was designed to construct a non-invasive and cost-effective classification model based on eye movement parameters to distinguish dementia due to AD (ADD), mild cognitive impairment (MCI), and normal cognition. Eye movement data were collected from 258 subjects, comprising 111 patients with ADD, 81 patients with MCI, and 66 individuals with normal cognition. The fixation, smooth pursuit, prosaccade, and anti-saccade tasks were performed. Machine learning methods were used to screen eye movement parameters and build diagnostic models. Pearson’s correlation analysis was used to assess the correlations between the five most important eye movement indicators in the optimal model and neuropsychological scales. The gradient boosting classifier model demonstrated the best classification performance, achieving 68.2% of accuracy and 66.32% of F1-score in multiclass classification of AD. Moreover, the correlation analysis indicated that the eye movement parameters were associated with various cognitive functions, including general cognitive status, attention, visuospatial ability, episodic memory, short-term memory, and language and instrumental activities of daily life. Eye movement parameters in conjunction with machine learning methods achieve satisfactory overall accuracy, making it an effective and less time-consuming method to assist clinical diagnosis of AD. },
  issn={ 1866-9964 },
}

@article{González-Castaño_2024,
  title={ Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models },
  volume={ 54 },
  DOI={ doi:10.1007/s10489-024-05808-0 },
  journal={ Applied Intelligence },
  author={ Francisco de Arriba-PérezSilvia García-MéndezJavier Otero-MosqueraFrancisco J. González-Castaño },
  year={ 2024 },
  pages={ 12613-12628 },
  month={ 09 },
  abstract={ Cognitive and neurological impairments are very common, but only a small proportion of affected individuals are diagnosed and treated, partly because of the high costs associated with frequent screening. Detecting pre-illness stages and analyzing the progression of neurological disorders through effective and efficient intelligent systems can be beneficial for timely diagnosis and early intervention. We propose using Large Language Models to extract features from free dialogues to detect cognitive decline. These features comprise high-level reasoning content-independent features (such as comprehension, decreased awareness, increased distraction, and memory problems). Our solution comprises (i) preprocessing, (ii) feature engineering via Natural Language Processing techniques and prompt engineering, (iii) feature analysis and selection to optimize performance, and (iv) classification, supported by automatic explainability. We also explore how to improve Chatgpt’s direct cognitive impairment prediction capabilities using the best features in our models. Evaluation metrics obtained endorse the effectiveness of a mixed approach combining feature extraction with Chatgpt and a specialized Machine Learning model to detect cognitive decline within free-form conversational dialogues with older adults. Ultimately, our work may facilitate the development of an inexpensive, non-invasive, and rapid means of detecting and explaining cognitive decline. },
  issn={ 1573-7497 },
}

@article{Leng_2024,
  title={ Top-k approximate selection for typicality query results over spatio-textual data },
  volume={ 66 },
  DOI={ doi:10.1007/s10115-023-02013-2 },
  journal={ Knowledge and Information Systems },
  author={ Xiangfu MengXiaoyan ZhangHongjin HuoQiangkui Leng },
  year={ 2024 },
  pages={ 1425-1468 },
  month={ 11 },
  abstract={ Spatial keyword query is a classical query processing mode for spatio-textual data, which aims to provide users the spatio-textual objects with the highest spatial proximity and textual similarity to the given query. However, the top-k result objects obtained by using the spatial keyword query mode are often similar to each other, while users hope that the system can pick top-k typicality results from the candidate query results in order to make users understand the representative features of the candidate result set. To deal with the problem of typicality analysis and typical object selection of spatio-textual data query results, a typicality evaluation and top-k approximate selection approach is proposed. First, the approach calculates the synthetic distances on dimensions of geographic location, textual semantics, and numeric attributes between all spatio-textual objects. And then, a hybrid index structure that can simultaneously support the location, text, and numeric multi-dimension matching is presented in order to expeditiously obtain the candidate query results. According to the synthetic distances between spatio-textual objects, a Gaussian kernel probability density estimation-based method for measuring the typicality of query results is proposed. To facilitate the query result analysis and top-k typical object selection, the Tournament strategy-based and local neighborhood-based top-k typical object approximate selection algorithms are presented, respectively. The experimental results demonstrated that the text semantic relevancy measuring method for spatio-textual objects is accurate and reasonable, and the local neighborhood-based top-k typicality result approximate selection algorithm achieved both the low error rate and high execution efficiency. The source code and datasets used in this paper are available to be accessed from https://github.com/JiaShengS/Typicality_analysis/ . },
  issn={ 0219-3116 },
}

@article{Zhang_2024,
  title={ ROS package search for robot software development: a knowledge graph-based approach },
  volume={ 19 },
  DOI={ doi:10.1007/s11704-024-3660-9 },
  journal={ Frontiers of Computer Science },
  author={ Shuo WangXinjun MaoShuo YangMenghan WuZhang Zhang },
  year={ 2024 },
  pages={ 1-16 },
  month={ 12 },
  abstract={ In recent years, ROS (Robot Operating System) packages have become increasingly popular as a type of software artifact that can be effectively reused in robotic software development. Indeed, finding suitable ROS packages that closely match the software’s functional requirements from the vast number of available packages is a nontrivial task using current search methods. The traditional search methods for ROS packages often involve inputting keywords related to robotic tasks into general-purpose search engines (e.g., Google) or code hosting platforms (e.g., GitHub) to obtain approximate results of all potentially suitable ROS packages. However, the accuracy of these search methods remains relatively low because the task-related keywords may not precisely match the functionalities offered by the ROS packages. To improve the search accuracy of ROS packages, this paper presents a novel semantic-based search approach that relies on the semantic-level ROS Package Knowledge Graph (RPKG) to automatically retrieve the most suitable ROS packages. Firstly, to construct the RPKG, we employ multi-dimensional feature extraction techniques to extract semantic concepts, including code file name, category, hardware device, characteristics, and function, from the dataset of ROS package text descriptions. The semantic features extracted from this process result in a substantial number of entities (32,294) and relationships (54,698). Subsequently, we create a robot domain-specific small corpus and further fine-tune a pre-trained language model, BERT-ROS, to generate embeddings that effectively represent the semantics of the extracted features. These embeddings play a crucial role in facilitating semantic-level understanding and comparisons during the ROS package search process within the RPKG. Secondly, we introduce a novel semantic matching-based search algorithm that incorporates the weighted similarities of multiple features from user search queries, which searches out more accurate ROS packages than the traditional keyword search method. To validate the enhanced accuracy of ROS package searching, we conduct comparative case studies between our semantic-based search approach and four baseline search approaches: ROS Index, GitHub, Google, and ChatGPT. The experiment results demonstrate that our approach achieves higher accuracy in terms of ROS package searching, outperforming the other approaches by at least 21% from 5 levels, including top1, top5, top10, top15, and top20. },
  issn={ 2095-2236 },
}

@article{Yaseen_2024,
  title={ The role of large language models in agriculture: harvesting the future with LLM intelligence },
  DOI={ doi:10.1007/s13748-024-00359-4 },
  journal={ Progress in Artificial Intelligence },
  author={ Tawseef Ayoub ShaikhTabasum RasoolK. VeningstonSyed Mufassir Yaseen },
  year={ 2024 },
  pages={ 1-48 },
  month={ 12 },
  abstract={ Significant accomplishments in many agricultural applications during the past decade attest to the fast progress and use of deep learning and machine learning methods in agricultural systems. However, these conventional models have a few drawbacks: They are not generalizable since they are trained on large, costly labeled datasets, require expert expertise to create and maintain, and are often built for specific applications. Significant accomplishments in language, vision, and decision-making tasks across several domains have been shown recently by massive pre-trained models, also known as large models (LMs). Recent years have seen large language models (LLMs) demonstrate remarkable competence in a variety of fields, including natural language processing (NLP), by encompassing different advancements in terms of architecture, training methods, context duration, fine-tuning, multi-modality, datasets, efficiency, benchmarking, and many other. The massive amounts of data used to train these models span many domains and modalities. After training, they can handle a wide range of tasks with less tweaking and less task-specific labeled data. Despite its effectiveness and promising future, agricultural artificial intelligence (AAI) has received less attention than other applications of LLMs. To better understand the problem area and open up new research pathways in this sector, this work aims to examine the possibilities of LLMs in smart agriculture by offering conceptual tools and a technical base. Herein, we delve into the potential applications of large models in agriculture, primarily categorizing them into four categories: Agricultural applications of large language models (LLMs), large vision models (LVMs) for precise agricultural applications, multimodal large language models (MLLMs) and model assessment, and intelligent and precise agriculture using reinforcement learning large models (RLLMs). Further, we review some of the most prominent LLMs, including three famous LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions, and limitations. Next, we evaluate famous LLM evaluation metrics and look at datasets for training, fine-tuning, and evaluation. Finally, we focus our discussion on issues and possible future research directions of LLMs in the agricultural sector. This review article aims to provide academics and practitioners with a panoramic perspective of the field and a quick reference to help them draw out relevant ideas from the extensive summaries of prior publications to broaden their LLM research. },
  issn={ 2192-6360 },
}

@article{Dimitri_2024,
  title={ A novel solution for the development of a sentimental analysis chatbot integrating ChatGPT },
  volume={ 28 },
  DOI={ doi:10.1007/s00779-024-01824-6 },
  journal={ Personal and Ubiquitous Computing },
  author={ Filippo FlorindiPasquale FedeleGiovanna Maria Dimitri },
  year={ 2024 },
  pages={ 947-960 },
  month={ 07 },
  abstract={ In today’s business landscape, Chatbots play a pivotal role in innovation and process optimization. In this paper, we introduced a novel advanced Emotional Chatbot AI, introducing sentiment analysis for human chatbot conversations. Adding an emotional component within the human-computer interaction, can in fact dramatically improve the quality of the final conversation between Chatbots and humans. More specifically, in our paper, we provided a practical evaluation of the EmoROBERTA software, introducing it into a novel implementation of an Emotional Chatbot. The pipeline we present is novel, and we developed it within a business context in which the use of sentimental and emotional responses can act in a significant and fundamental way toward the final success and use of the Chatbot itself. The architecture enriches user experience with real-time updates on the topic of interest, maintaining a user-centric design, toward an affective-response enhancement of the interaction established between the Chatbot and the user. The source code is fully available on GitHub: https://github.com/filippoflorindi/F-One . },
  issn={ 1617-4917 },
}

@article{Peng_2023,
  title={ A novel attribute reduction method based on intuitionistic fuzzy three-way cognitive clustering },
  volume={ 53 },
  DOI={ doi:10.1007/s10489-022-03496-2 },
  journal={ Applied Intelligence },
  author={ Xian-wei XinChun-lei ShiJing-bo SunZhan-ao XueJi-hua SongWei-ming Peng },
  year={ 2023 },
  pages={ 1744-1758 },
  month={ 05 },
  abstract={ Attribute reduction plays a critical role in the Pawlak rough set, which aims to improve the computational efficiency and accuracy of a system by removing redundant attributes. Existing attribute reduction algorithms focus on attribute importance, information entropy and discernibility matrices while ignoring the classification differences of attributes and loss cost in the reduction process. More importantly, people are not “all-around experts”, and their cognition level of domain knowledge (CLDK) may vary, which leads to multiple reduction results based on the same attribute set. In view of this, we integrate learners’ CLDK and present an attribute reduction model based on intuitionistic fuzzy three-way cognitive clustering (IF3WCC). In this scenario, we first present the concept, calculation methods and semantic interpretation of intuitionistic fuzzy cognitive entropy (IFCE) and prove its related properties. Intuitionistic fuzzy cognition similarity (IFCS) is then proposed and utilized to implement IF3WCC. We then introduce the three-way decision (3WD) model to calculate the reduction cost of attributes in various clusters and divide them into irreducible, reducible, and determined reduction sets. Next, we develop a secondary reduction strategy for uncertain reduction attributes and provide a corresponding algorithm. Finally, the rationality and effectiveness of the proposed model are verified by comparing it with existing attribute reduction methods. },
  issn={ 1573-7497 },
}

@article{Partsinevelos_2024,
  title={ Lunar ground segmentation using a modified U-net neural network },
  volume={ 35 },
  DOI={ doi:10.1007/s00138-024-01533-3 },
  journal={ Machine Vision and Applications },
  author={ Georgios PetrakisPanagiotis Partsinevelos },
  year={ 2024 },
  pages={ 1-15 },
  month={ 04 },
  abstract={ Semantic segmentation plays a significant role in unstructured and planetary scene understanding, offering to a robotic system or a planetary rover valuable knowledge about its surroundings. Several studies investigate rover-based scene recognition planetary-like environments but there is a lack of a semantic segmentation architecture, focused on computing systems with low resources and tested on the lunar surface. In this study, a lightweight encoder-decoder neural network (NN) architecture is proposed for rover-based ground segmentation on the lunar surface. The proposed architecture is composed by a modified MobilenetV2 as encoder and a lightweight U-net decoder while the training and evaluation process were conducted using a publicly available synthetic dataset with lunar landscape images. The proposed model provides robust segmentation results, allowing the lunar scene understanding focused on rocks and boulders. It achieves similar accuracy, compared with original U-net and U-net-based architectures which are 110–140 times larger than the proposed architecture. This study, aims to contribute in lunar landscape segmentation utilizing deep learning techniques, while it proves a great potential in autonomous lunar navigation ensuring a safer and smoother navigation on the moon. To the best of our knowledge, this is the first study which propose a lightweight semantic segmentation architecture for the lunar surface, aiming to reinforce the autonomous rover navigation. },
  issn={ 1432-1769 },
}

@article{Ye_2024,
  title={ Application of artificial intelligence technology in the field of orthopedics: a narrative review },
  volume={ 57 },
  DOI={ doi:10.1007/s10462-023-10638-6 },
  journal={ Artificial Intelligence Review },
  author={ Pengran LiuJiayao ZhangSongxiang LiuTongtong HuoJiajun HeMingdi XueYing FangHonglin WangYi XieMao XieDan ZhangZhewei Ye },
  year={ 2024 },
  pages={ 1-52 },
  month={ 01 },
  abstract={ Artificial intelligence (AI) was a new interdiscipline of computer technology, mathematic, cybernetics and determinism. These years, AI had obtained a significant development by the improvement of core technology Machine Learning and Deep Learning. With the assistance of AI, profound changes had been brought into the traditional orthopedics. In this paper, we narratively reviewed the latest applications of AI in orthopedic diseases, including the severity evaluation, triage, diagnosis, treatment and rehabilitation. The research point, relevant advantages and disadvantages of the orthopedic AI was also discussed combined with our own research experiences. We aimed to summarize the past achievements and appeal for more attentions and effective applications of AI in the field of orthopedics. },
  issn={ 1573-7462 },
}

@article{Parveen_2024,
  title={ An efficient resume skill extraction using deep feature-based AGT optimized K means clustering },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-024-18220-0 },
  journal={ Multimedia Tools and Applications },
  author={ J. Himabindu PriyankaNikhat Parveen },
  year={ 2024 },
  pages={ 65967-65986 },
  month={ 01 },
  abstract={ When developing a job recommender system, skill extraction is crucial. It may also be used to create talent profiles and knowledge bases for companies. To address this issue, several approaches have been offered. To create a unique skill prediction system based on the job postings for predicting skills from multi-level resumes. To improve the clustering of multi-level resumes and reduce complexity. In the proposed model, initially, CVs from job portals are pre-processed using punctuation removal, tokenization, stop words removal, and stemming. The words in the resumes are then vectorized using TF-IDF-glove vectorization. The word embedding features are given into a convolutional Gated Recurrent Unit Network (CGRUN) to generate the higher-level deep feature representation. Finally, the deep feature representation is supplied into the k means clustering method for CV classification in the appropriate candidate section. The clustering algorithm's local optima trap was reduced using the Artificial Gorilla Troops Optimizer (AGTO) model. The implementation compares to some models developed with alternative resume encoding methods and logistic regression as a classifier: word2vec + CNN, doc2vec + LR (document to vector + logistic regression) and one-hot + LR (one-hot + logistic regression). The experimental findings suggest that the resume skill extraction has an accuracy level of 95.99%. },
  issn={ 1573-7721 },
}

@article{Cambria_2023,
  title={ A brief survey on recent advances in coreference resolution },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-023-10506-3 },
  journal={ Artificial Intelligence Review },
  author={ Ruicheng LiuRui MaoAnh Tuan LuuErik Cambria },
  year={ 2023 },
  pages={ 14439-14481 },
  month={ 05 },
  abstract={ The task of resolving repeated objects in natural languages is known as coreference resolution, and it is an important part of modern natural language processing. It is classified into two categories depending on the resolved objects, namely entity coreference resolution and event coreference resolution. Predicting coreference connections and identifying mentions/triggers are the major challenges in coreference resolution, because these implicit relationships are particularly difficult in natural language understanding in downstream tasks. Coreference resolution techniques have experienced considerable advances in recent years, encouraging us to review this task in the following aspects: current employed evaluation metrics, datasets, and methods. We investigate 10 widely used metrics, 18 datasets and 4 main technical trends in this survey. We believe that this work is a comprehensive roadmap for understanding the past and the future of coreference resolution. },
  issn={ 1573-7462 },
}

@article{Wang_2023a,
  title={ Emotion spreading carried by emoji in social network },
  volume={ 13 },
  DOI={ doi:10.1007/s13278-023-01144-2 },
  journal={ Social Network Analysis and Mining },
  author={ Fuzhong NianXiaochen YangZheming Wang },
  year={ 2023 },
  pages={ 1-12 },
  month={ 11 },
  abstract={ This paper focuses on the spreading characteristics of emoji in social networks, especially by constructing the S3I emotion spreading model to study the evolution laws and characteristics of positive, neutral, and negative emotions on social networks, respectively. The results show that negative emotions spread faster and wider in social networks, validating the social phenomenon that “Good news never goes beyond the gate, while bad news has wings.” Also illustrates in social communication, the emoji has a special position that cannot be described by words in some special situations. The mathematical analysis and simulations were implemented to verify the proposal model and finally compared with the real case to&nbsp;show that the model is effective. },
  issn={ 1869-5469 },
}

@article{Wen_2024,
  title={ A survey on large language model based autonomous agents },
  volume={ 18 },
  DOI={ doi:10.1007/s11704-024-40231-1 },
  journal={ Frontiers of Computer Science },
  author={ Lei WangChen MaXueyang FengZeyu ZhangHao YangJingsen ZhangZhiyuan ChenJiakai TangXu ChenYankai LinWayne Xin ZhaoZhewei WeiJirong Wen },
  year={ 2024 },
  pages={ 1-26 },
  month={ 03 },
  abstract={ Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. },
  issn={ 2095-2236 },
}

@article{张学典_2023,
  title={ Attention-based efficient robot grasp detection network },
  volume={ 24 },
  DOI={ doi:10.1631/FITEE.2200502 },
  journal={ Frontiers of Information Technology & Electronic Engineering },
  author={ Xiaofei Qin 秦晓飞Wenkai Hu 胡文凯Chen Xiao 肖晨Changxiang He 何常香Songwen Pei 裴颂文Xuedian Zhang 张学典 },
  year={ 2023 },
  pages={ 1430-1444 },
  month={ 11 },
  abstract={ To balance the inference speed and detection accuracy of a grasp detection algorithm, which are both important for robot grasping tasks, we propose an encoder–decoder structured pixel-level grasp detection neural network named the attention-based efficient robot grasp detection network (AE-GDN). Three spatial attention modules are introduced in the encoder stages to enhance the detailed information, and three channel attention modules are introduced in the decoder stages to extract more semantic information. Several lightweight and efficient DenseBlocks are used to connect the encoder and decoder paths to improve the feature modeling capability of AE-GDN. A high intersection over union (IoU) value between the predicted grasp rectangle and the ground truth does not necessarily mean a high-quality grasp configuration, but might cause a collision. This is because traditional IoU loss calculation methods treat the center part of the predicted rectangle as having the same importance as the area around the grippers. We design a new IoU loss calculation method based on an hourglass box matching mechanism, which will create good correspondence between high IoUs and high-quality grasp configurations. AEGDN achieves the accuracy of 98.9% and 96.6% on the Cornell and Jacquard datasets, respectively. The inference speed reaches 43.5 frames per second with only about 1.2 × 106 parameters. The proposed AE-GDN has also been deployed on a practical robotic arm grasping system and performs grasping well. Codes are available at https://github.com/robvincen/robot_gradet . },
  issn={ 2095-9230 },
}

@article{Gurcan_2024,
  title={ Analyzing 40 years of educational technology trends: insights from advanced text mining and machine learning },
  DOI={ doi:10.1007/s11042-024-20294-9 },
  journal={ Multimedia Tools and Applications },
  author={ Fatih Gurcan },
  year={ 2024 },
  pages={ 1-21 },
  month={ 09 },
  abstract={ The interactive learning settings provided by educational technology and multimedia have led to significant advancements in modern education systems and enhanced the effectiveness, interest, and interactivity of the learning process. Every day, more research is conducted in an effort to enhance educational environments through the use of advanced multimedia tools and immersion technologies. As a result, the number of studies on educational technologies has risen considerably in recent years. However, there is a lack of studies that use automated methodologies to analyze domain-specific research from its inception to the present. Considering this research gap, this study aims to identify the research interests, issues, and insights, and application areas that describe the research landscape of educational technology, as well as their historical and contemporary developments. From this perspective, this study investigated an empirical corpus totaling 24,340 educational technology articles published over the past four decades in the top 20 publication sources. The methodology of the study employed text mining procedures for preprocessing of the corpus, and implemented latent Dirichlet allocation, an unsupervised machine learning-based topic modeling approach, for the detection of semantic topics. As a result of the analysis, 34 topics were discovered. Topics with rising popularity, such as “Online Learning”, “Mobile Learning”, “Virtual Reality”, “Video Lecturing”, “Social Networks”, “Game-Based Learning”, and “Intelligent Tutoring”, highlight the significance of interactive learning assisted by multimedia tools in the near future. Our research intends to help scholars and practitioners understand current research gaps and potential by giving thorough insights and implications about this field. },
  issn={ 1573-7721 },
}

@article{Czubenko_2023,
  title={ Cognitive motivations and foundations for building intelligent decision-making systems },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-022-10255-9 },
  journal={ Artificial Intelligence Review },
  author={ Zdzisław KowalczukMichał Czubenko },
  year={ 2023 },
  pages={ 3445-3472 },
  month={ 09 },
  abstract={ Concepts based on psychology fit well with current research trends related to robotics and artificial intelligence. Biology-inspired cognitive architectures are extremely useful in building agents and robots, and this is one of the most important challenges of modern science. Therefore, the widely viewed and far-reaching goal of systems research and engineering is virtual agents and autonomous robots that mimic human behavior in solving known and unknown problems. The article proposes, at a high level of generality, an operational cybernetic model of the human mind, developed with the use of carefully selected ideas taken from psychological knowledge. In particular, the work combines extensive knowledge drawn from both the theory of developmental cognitive psychology and the theory of motivation. The proposed mathematically developed operating blocks create a coherent and functional decision-making system containing all the elements necessary in autonomous robotics. The ISD system is under development. There is still a long way to go to full validation. However, as shown in several articles, the basic subsystems of the ISD system, i.e. motivational and emotional, have already been positively verified in operation. The overall purpose of this article is to show a blueprint of the overall concept of the entire ISD. },
  issn={ 1573-7462 },
}

@article{Pal_2024,
  title={ Lexeme connexion measure of cohesive lexical ambiguity revealing factor: a robust approach for word sense disambiguation of Bengali text },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-14676-8 },
  journal={ Multimedia Tools and Applications },
  author={ Debapratim Das DawnAbhinandan KhanSoharab Hossain ShaikhRajat Kumar Pal },
  year={ 2024 },
  pages={ 12939-12983 },
  month={ 07 },
  abstract={ Word sense disambiguation (WSD) is the process of finding out the appropriate meaning of a polysemous word based on any given context. The Bengali language inherently comprises a large number of polysemous words. Recently, researchers in the domain of linguistics have been attracted to the problem of WSD in Bengali text due to its numerous interesting applications, viz. machine translation, opinion polarity identification, question-answering systems, etc. In this paper, lexeme connexion measure of cohesive lexical ambiguity revealing factor has been proposed that takes a decision on the disambiguation of senses of a Bengali polysemous word. All the polysemous words have been treated as target words, and a context window of three different sizes, viz. five, seven, and ten are considered based on these target words. This paper has generated lexeme harmony measure for quantifying heuristically of syntactic belongings of a collection of lexemes in Bengali text. The proposed methodology has been extracted a feature vector by considering the cohesive lexical ambiguity revealing factor or CLARF, depending on frame lexeme harmony (FLH), sense lexeme harmony (SLH), polysemy singularity coherence (PSC), polysemy distribution factor (PDF), and relative polysemy singularity coherence (RPSC) factor of a lexeme. This Bengali WSD technique has been applied max-rule of integrated lexeme connexion measure (LCM) of each lexeme of both the testing and training cases score for sense recognition. The proposed algorithm has succeeded in eliminating the drawback of the Bengali WSD approaches, as it can focus on both the lexical and semantic relationships between words. The performance of this algorithm has been evaluated on a dataset that consists of 100 polysemous words of three/four senses. Various evaluation metrics have been used to analyse the results obtained by the proposed algorithm. The obtained results indicate the robustness of the proposed algorithm. },
  issn={ 1573-7721 },
}

@article{Yu_2024a,
  title={ Research on the influence mechanism of key halo effect and Matthew effect on product online word-of-mouth: considering the moderating role of online store service quality },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-16124-z },
  journal={ Multimedia Tools and Applications },
  author={ Shugang LiYanfang WeiZhaoxu Yu },
  year={ 2024 },
  pages={ 13045-13072 },
  month={ 07 },
  abstract={ The company benefits greatly from online word-of-mouth (OWM) for its products through e-commerce platforms. However, previous research neglects the critical influence mechanism of OWM for products sold in online stores, especially the relationships between consumers’ past subjective impressions and OWM. This study innovatively explores the influence mechanism of key halo effect and Matthew effect on product OWM considering the moderating role of online store PSQ namely store trust. We propose a research model and influencing factors for the halo effect and Matthew effect in the e-commerce platform scenario. Collecting about 30,000 online reviews and 160,000 historical consumer reviews from Amazon, we use a semantic similarity model to match online reviews with a standard scale for the relevant variables, combining text mining and econometric analysis. Our results show that the selected hierarchical regression model has better fit than existing common models. And the halo effect and Matthew effect are indeed beneficial to increase product OWM including the indicators of consumer satisfaction and future consumption intention. And there is a reverse effect with low perceived service quality and star rating Matthew effect on product OWM is not supported. These findings help us understand consumers’ online comment behaviors, and have deep implications for e-commerce platforms and related companies. },
  issn={ 1573-7721 },
}

@article{Srinivasan_2023,
  title={ DeepThink IoT: The Strength of Deep Learning in Internet of Things },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-023-10513-4 },
  journal={ Artificial Intelligence Review },
  author={ Divyansh ThakurJaspal Kaur SainiSrikant Srinivasan },
  year={ 2023 },
  pages={ 14663-14730 },
  month={ 06 },
  abstract={ The integration of Deep Learning (DL) and the Internet of Things (IoT) has revolutionized technology in the twenty-first century, enabling humans and machines to perform tasks more efficiently. The combination of DL and the IoT has resulted in significant advancements in technology by improving the efficiency, security, and user experience of IoT devices and systems. The integration of DL and IoT offers several benefits, including improved data processing and analysis capabilities, the ability for IoT devices to learn from data and adapt to changing conditions, and the early detection of system malfunctions and potential security breaches. This survey paper provides a comprehensive overview of the impact of DL on IoT, including an analysis of sensor data to detect patterns and make predictions, and the implications for various industries such as healthcare, manufacturing, agriculture, and smart cities. The survey paper covers topics such as DL models, frameworks, IoT connectivity terminologies, IoT components, IoT service-oriented architecture, IoT applications, the role of DL in IoT, and challenges faced by DL in IoT. The study also presents quantitative achievements that highlight the potential impact of IoT and DL in environmental contexts such as precision farming and energy consumption. Overall, the survey paper provides an excellent resource for researchers interested in exploring the potential of IoT and DL in their field. },
  issn={ 1573-7462 },
}

@article{Singh_2023,
  title={ Emotion quantification techniques for cognitive reappraisal: a systematic review and scientometric analysis },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-023-10606-0 },
  journal={ Artificial Intelligence Review },
  author={ Mir Aamir HamidJaiteg Singh },
  year={ 2023 },
  pages={ 3363-3416 },
  month={ 10 },
  abstract={ Cognitive reappraisal intends to study the significance of an event concerning any emotional reaction. Understanding the efficacy of cognitive reappraisal in emotion regulation requires reliable and valid techniques for quantifying emotions. Sentiments and emotions are complex mental states that play a significant role in every aspect of human life. The analysis of relevant literature published from 2012 till June 2023 indicated the limited investigation into predicting quantifiable emotional states. The existing emotion recognition techniques use static labels for emotion quantization. This study investigates prominent techniques for dynamic emotion modelling along with associated challenges, shortcomings, and research gaps. Subsequently, a generic framework is proposed that may help researchers explore dynamic emotion modelling techniques and future directions. },
  issn={ 1573-7462 },
}

@article{Wu_2024,
  title={ SAPFIS: a parallel fuzzy inference system for air combat situation assessment },
  volume={ 81 },
  DOI={ doi:10.1007/s11227-024-06521-y },
  journal={ The Journal of Supercomputing },
  author={ Lei GaoJingfei JiangJinwei XuWeijia WangPengbo Wu },
  year={ 2024 },
  pages={ 1-26 },
  month={ 10 },
  abstract={ Situation assessment is an important basis for achieving autonomous decision-making in air combat. The ever-increasing multi-source fusion information perceived by situation assessment system poses a computational challenge to current airborne equipment. Fuzzy inference method introduced in situation assessment could effectively adapt to the incompleteness and uncertainty of situational information, but still struggling to meet the high-performance requirements under limited hardware resources on airborne equipment. Leveraging hardware accelerators (GPUs, FPGAs, etc.) to accelerate intensive computation like situation factor evaluation has become paramount. In this work, we present a novel air combat situation assessment architecture with multi-level Parallel fuzzy inference system (SAPFIS), which designs the first-ever fuzzy inference accelerator directed by our proposed situation assessment model to accelerate inference computation. Experimental results show that our fuzzy inference accelerator implemented on FPGA achieves 230.48 times of performance speedup and 2053.76 times of inference efficiency ratio improvement over the software solutions on four general computing platforms(e.g., Intel and Phytium platforms) with multi-dimensional test datasets. Moreover, SAPFIS with 128-core accelerator delivers up to 17.45 times performance improvement in the simulation of air combat situation assessment application. },
  issn={ 1573-0484 },
}

@article{Ezeamuzie_2023,
  title={ Project-first approach to programming in K–12: Tracking the development of novice programmers in technology-deprived environments },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-022-11180-8 },
  journal={ Education and Information Technologies },
  author={ Ndudi O. Ezeamuzie },
  year={ 2023 },
  pages={ 407-437 },
  month={ 06 },
  abstract={ Several instructional approaches have been advanced for learning programming. However, effective ways of engaging beginners in programming in K–12 are still unclear, especially among low socioeconomic status learners in technology-deprived learning environments. Understanding the learning path of novice programmers will bridge this gap and explain what constitutes an effective learning path for novice. Thirty-eight students from technology-deprived schools participated in a 10-h project-first constructionist learning. Using the Friedman test of repeated measures and Spearman’s rank correlation, trends in the students’ programming ability were evaluated. The findings showed that the students’ programming ability increased on the first day, remained stable throughout the intervention, and were not affected by either semantics or syntax of the Python programming language. However, the features of a program were inconclusive determinants of programming skills. The irregular patterns of programming concepts within and between the learners’ programming solutions suggest focusing on pedagogies that encourage project-first learning. A constructionist model of learning and the challenges educators may encounter amongst novice learners with low socioeconomic status are highlighted. },
  issn={ 1573-7608 },
}

@article{Leite_2024,
  title={ Pull-the-strings },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-021-11876-y },
  journal={ Multimedia Tools and Applications },
  author={ Luis Leite },
  year={ 2024 },
  pages={ 46731-46756 },
  month={ 01 },
  abstract={ Pull-the-Strings presents a mapping model for digital puppetry based on a transparent framework to support generic device controllers and generic tools. Digital puppetry requires a creative interaction design, in particular in the way designers map the puppet to the puppeteer using specific devices. This process depends on a constantly changing interface technology, which limits the reuse of devices and mappings. This paper proposes a methodology and a set of tools that facilitate the mapping process, and promote the recycling of technologies. A flexible and generic environment independent from device specifications. By abstracting the hardware layer, the artist is motivated to think in terms of signal flow, establishing relations through meaningful mappings instead of handling the diverse specifications of each device and application. Pull-the-Strings is a data-flow ecosystem that focus on the functional usage of control signals. It provides a scalable environment for building semantic blocks that connect, transform and generate signals for the manipulation of virtual objects. Its goal is to make technology as transparent as possible, facilitating connections and reducing the obstacles between the performer and the performing object. On the other hand, it proposes an interaction design space that takes into account the manipulation and perception distance, responding to the specifications of the digital puppetry medium. This model was evaluated comparing a set of tools and methods with experienced and non-experienced users. },
  issn={ 1573-7721 },
}

@article{Khan_2024,
  title={ An optimized deep focused U-Net model for image segmentation },
  DOI={ doi:10.1007/s00521-024-10417-6 },
  journal={ Neural Computing and Applications },
  author={ Haroon Haider KhanMajid Iqbal Khan },
  year={ 2024 },
  pages={ 1-27 },
  month={ 09 },
  abstract={ Neural network-based segmentation methods are an important advancement in medical image analysis. Issues with class imbalance pose a significant challenge in medical segmentation, with lesions often occupying a considerably smaller volume (hard objects) relative to the background (easy objects). Medical image segmentation tasks require the model to pay attention on specific parts of the image rather than the entire image. Models based on CNN and its variants like U-Net have shown promising results in this regard. However, they often suffer from unstable gradient during the training process. We propose an optimized deep focused U-Net (DF U-Net) model along with a novel learnable optimized focal loss (LOF) function. The DF U-Net is integrated with a novel block that comprises of five subblocks: efficient channel attention, squeeze and excitation, stochastic depth, residual block and dense block, collectively known as (ESSRD) in the decoder layers which helps to address the unstable gradient and overfitting issues. The proposed LOF loss optimizes itself during the training of the model and converges to optimal values within few epochs. We evaluated DF U-Net using four medical imaging data sets: DRIVE, BUS2017, CVC-Clinic and Kvasir-SEG. During the training process, DF U-Net significantly reduced the convergence time by sharply minimizing the loss to 0.001% and achieved an accuracy of 99.5%. The experimental results show that DF U-Net helps to mitigate the unstable gradient issue while focusing on both easy and hard objects equally. },
  issn={ 1433-3058 },
}

@article{Kirschner_2024,
  title={ Short-Answer Grading for German: Addressing the Challenges },
  volume={ 34 },
  DOI={ doi:10.1007/s40593-023-00383-w },
  journal={ International Journal of Artificial Intelligence in Education },
  author={ Ulrike PadóYunus EryilmazLarissa Kirschner },
  year={ 2024 },
  pages={ 1321-1352 },
  month={ 12 },
  abstract={ Short-Answer Grading (SAG) is a time-consuming task for teachers that automated SAG models have long promised to make easier. However, there are three challenges for their broad-scale adoption: A technical challenge regarding the need for high-quality models, which is exacerbated for languages with fewer resources than English; a usability challenge in adapting high-quality research prototypes to the needs of non-expert users, and a trust challenge in communicating the abilities and limitations of the tools. We propose to meet the technical challenge for German with a robust Transformer-based SAG model. We address the usability challenge with an easy-to-use graphical user interface for the SAG model, and the trust challenge with a workflow that allows teachers to evaluate the model on their own data, to choose on the basis of this evaluation which model predictions to trust, and in consequence to stay in control of grading their students while saving grading effort. },
  issn={ 1560-4306 },
}

@article{Ahrabi_2024,
  title={ Multi-resolution Twinned Residual Auto-Encoders (MR-TRAE)—A Novel DL Model for Image Multi-resolution },
  volume={ 16 },
  DOI={ doi:10.1007/s12559-024-10293-1 },
  journal={ Cognitive Computation },
  author={ Alireza MomenzadehEnzo BaccarelliMichele ScarpinitiSima Sarv Ahrabi },
  year={ 2024 },
  pages={ 1447-1469 },
  month={ 05 },
  abstract={ In this paper, we design and evaluate the performance of the Multi-resolution Twinned Residual Auto-Encoders (MR-TRAE) model, a deep learning (DL)-based architecture specifically designed for achieving multi-resolution super-resolved images from low-resolution (LR) inputs at various scaling factors. For this purpose, we expand on the recently introduced Twinned Residual Auto-Encoders (TRAE) paradigm for single-image super-resolution (SISR) to extend it to the multi-resolution (MR) domain. The main contributions of this work include (i) the architecture of the MR-TRAE model, which utilizes cascaded trainable up-sampling modules for progressively increasing the spatial resolution of low-resolution (LR) input images at multiple scaling factors; (ii) a novel loss function designed for the joint and semi-blind training of all MR-TRAE model components; and (iii) a comprehensive analysis of the MR-TRAE trade-off between model complexity and performance. Furthermore, we thoroughly explore the connections between the MR-TRAE architecture and broader cognitive paradigms, including knowledge distillation, the teacher-student learning model, and hierarchical cognition. Performance evaluations of the MR-TRAE benchmarked against state-of-the-art models (such as U-Net, generative adversarial network (GAN)-based, and single-resolution baselines) were conducted using publicly available datasets. These datasets consist of LR computer tomography (CT) scans from patients with COVID-19. Our tests, which explored multi-resolutions at scaling factors $$\times (2,4,8)$$ , showed a significant finding: the MR-TRAE model can reduce training times by up to $$60\%$$ compared to those of the baselines, without a noticeable impact on achieved performance. },
  issn={ 1866-9964 },
}

@article{Sahoo_2024,
  title={ From Pixels to Prepositions: Linking Visual Perception with Spatial Prepositions Far and Near },
  volume={ 16 },
  DOI={ doi:10.1007/s12559-024-10329-6 },
  journal={ Cognitive Computation },
  author={ Krishna Raj S RSrinivasa Chakravarthy VAnindita Sahoo },
  year={ 2024 },
  pages={ 3319-3343 },
  month={ 08 },
  abstract={ Human language is influenced by sensory-motor experiences. Sensory experiences gathered in a spatiotemporal world are used as raw material to create more abstract concepts. In language, one way to encode spatial relationships is through spatial prepositions. Spatial prepositions that specify the proximity of objects in space, like far and near or their variants, are found in most languages. The mechanism for determining the proximity of another entity to itself is a useful evolutionary trait. From the taxic behavior in unicellular organisms like bacteria to the tropism in the plant kingdom, this behavior can be found in almost all organisms. In humans, vision plays a critical role in spatial localization and navigation. This computational study analyzes the relationship between vision and spatial prepositions using an artificial neural network. For this study, a synthetic image dataset was created, with each image featuring a 2D projection of an object placed in 3D space. The objects can be of various shapes, sizes, and colors. A convolutional neural network is trained to classify the object in the images as far or near based on a set threshold. The study mainly explores two visual scenarios: objects confined to a plane (grounded) and objects not confined to a plane (ungrounded), while also analyzing the influence of camera placement. The classification performance is high for the grounded case, demonstrating that the problem of far/near classification is well-defined for grounded objects, given that the camera is at a sufficient height. The network performance showed that depth can be determined in grounded cases only from monocular cues with high accuracy, given the camera is at an adequate height. The difference in the network’s performance between grounded and ungrounded cases can be explained using the physical properties of the retinal imaging system. The task of determining the distance of an object from individual images in the dataset is challenging as they lack any background cues. Still, the network performance shows the influence of spatial constraints placed on the image generation process in determining depth. The results show that monocular cues significantly contribute to depth perception when all the objects are confined to a single plane. A set of sensory inputs (images) and a specific task (far/near classification) allowed us to obtain the aforementioned results. The visual task, along with reaching and motion, may enable humans to carve the space into various spatial prepositional categories like far and near. The network’s performance and how it learns to classify between far and near provided insights into certain visual illusions that involve size constancy. },
  issn={ 1866-9964 },
}

@article{Nixon_2024,
  title={ Transitioning to Online Instructions and COVID-19 Response: A View from Mining Emergent College Students Discourse in Online Discussion Forum },
  volume={ 34 },
  DOI={ doi:10.1007/s40593-024-00411-3 },
  journal={ International Journal of Artificial Intelligence in Education },
  author={ Yiwen LinNia Nixon },
  year={ 2024 },
  pages={ 706-731 },
  month={ 06 },
  abstract={ The COVID-19 pandemic disrupted teaching and learning activities in higher education around the world. As universities shifted to remote instruction in response to the pandemic, it is important to learn&nbsp;how students engaged in learning during this challenging period. In this paper, we examined the changes in learners’ social and cognitive presence in online discussion forums&nbsp;prior and after remote instruction.&nbsp;We also extracted&nbsp;emergent topics during the pandemic as an attempt to explore&nbsp;what students talked about and how they interacted with their&nbsp;peers. We extracted discussion forum posts between 2019 and 2020 from courses that have been offered repeatedly each term at a four-year university in the U.S. Our findings suggest that students exhibited higher social presence through increased social and affective language during remote instructions. We&nbsp;also&nbsp;identified emergent COVID-19 related discourse,&nbsp;which involved sharing personal&nbsp;experience&nbsp;with positive sentiments and expressing opinions on contemporary events. Our&nbsp;qualitative&nbsp;analysis further revealed that students showed rapport and empathy towrads others, and engaged in active sense making of the pandemic&nbsp;through engaging in critical discourse.&nbsp; Our study sheds lights on leveraging discussion forum to facilitate&nbsp;learner experiences&nbsp;and building classroom community in&nbsp;online courses. We further discussed the potential for conducting large-scale computational linguistic modeling on learner discourse and the role of artificial intelligence in deriving insights on learning behavior at scale to support remote teaching and learning. },
  issn={ 1560-4306 },
}

@article{Majumder_2024,
  title={ Near-term advances in quantum natural language processing },
  volume={ 92 },
  DOI={ doi:10.1007/s10472-024-09940-y },
  journal={ Annals of Mathematics and Artificial Intelligence },
  author={ Dominic WiddowsAaranya AlexanderDaiwei ZhuChase ZimmermanArunava Majumder },
  year={ 2024 },
  pages={ 1249-1272 },
  month={ 04 },
  abstract={ This paper describes experiments showing that some tasks in natural language processing (NLP) can already be performed using quantum computers, though so far only with small datasets. We demonstrate various approaches to topic classification. The first uses an explicit word-based approach, in which word-topic weights are implemented as fractional rotations of individual qubits, and a phrase is classified based on the accumulation of these weights onto a scoring qubit, using entangling quantum gates. This is compared with more scalable quantum encodings of word embedding vectors, which are used to compute kernel values in a quantum support vector machine: this approach achieved an average of 62% accuracy on classification tasks involving over 10000 words, which is the largest such quantum computing experiment to date. We describe a quantum probability approach to bigram modeling that can be applied to understand sequences of words and formal concepts, investigate a generative approximation to these distributions using a quantum circuit Born machine, and introduce an approach to ambiguity resolution in verb-noun composition using single-qubit rotations for simple nouns and 2-qubit entangling gates for simple verbs. The smaller systems presented have been run successfully on physical quantum computers, and the larger ones have been simulated. We show that statistically meaningful results can be obtained, but the quality of individual results varies much more using real datasets than using artificial language examples from previous quantum NLP research. Related NLP research is compared, partly with respect to contemporary challenges including informal language, fluency, and truthfulness. },
  issn={ 1573-7470 },
}

@article{Puente_2024,
  title={ Using machine learning to predict artistic styles: an analysis of trends and the research agenda },
  volume={ 57 },
  DOI={ doi:10.1007/s10462-024-10727-0 },
  journal={ Artificial Intelligence Review },
  author={ Jackeline ValenciaGeraldine García PinedaVanessa García PinedaAlejandro Valencia-AriasJuan Arcila-DiazRenata Teodori de la Puente },
  year={ 2024 },
  pages={ 1-66 },
  month={ 04 },
  abstract={ In the field of art, machine learning models have been used to predict artistic styles in paintings. The foregoing is somewhat advantageous for analysts, as these tools can provide more valuable results and help reduce bias in the results and conclusions provided. Therefore, the objective of this research was to examine research trends in the use of machine learning to predict artistic styles from a bibliometric review based on the PRISMA methodology. From the search equations, 268 documents were found, out of which, following the application of inclusion and exclusion criteria, 128 documents were analyzed. Through quantitative analysis, a growing research interest in the subject is evident, progressing from user perception approaches to the utilization of tools like deep learning for art studies. Among the main results, it is possible to identify that one of the most used techniques in the field has been neural networks for pattern recognition. Also, a large part of the research focuses on the use of design software for image creation and manipulation. Finally, it is found that the number of studies focused on contemporary modern art is still limited, this is due to the fact that a large part of the investigations has focused on historical artistic styles. },
  issn={ 1573-7462 },
}

@article{Foumani_2023,
  title={ Predicting product advertisement links using hybrid learning within social networks },
  volume={ 79 },
  DOI={ doi:10.1007/s11227-023-05213-3 },
  journal={ The Journal of Supercomputing },
  author={ Seyed Mohsen Ebadi JokandanPeyman BayatMehdi Farrokhbakht Foumani },
  year={ 2023 },
  pages={ 15023-15050 },
  month={ 04 },
  abstract={ Advertising has been one of the most effective and valuable marketing tools for many years. Since companies spend vast amounts of money on product advertising, they put their best effort into making the advertising effective, consequently increasing product sales. Advertising recommender systems can significantly help make posted ads more effective. Due to the incredible popularity of social networks in the last two decades and the massive amount of user's activity on these networks, a great opportunity has been provided to elicit user data and post product promotions. Recommender systems for social networks can use various data to make recommendations, including content, post features, individuals, and even context-based information (e.g., activity, location, time, individual, and relational data). One of the upcoming challenges is the study of feature interactions and their valuation. The critical question is: “Can this data, especially contextual data, lead to improved advertising recommendations?” Considering the instantaneous growth of social network data, it is also imperative to verify scalability based on time criteria. Since earlier studies have investigated the influence of a limited number of contextual features on the recommender system, it is necessary to conduct a study that comprehensively utilizes the diverse contextual data available in social networks. Therefore, we use a hybrid context-based SOM-SPM approach to meet the challenges raised. Our method improves the accuracy of offered product’s advertisements for target users. The proposed method improves the performance of the advertising recommender system by constituting a post-context feature matrix based on context dimensions and SPM algorithms to learn behavioral patterns. We experiment with this method on two datasets and evaluate the results with MSE, RMSE, MAE metrics and Davies-Bouldin Index. The results indicate that our method outperforms algorithms like FM, SBS, AFM and MF-LOD. The experiment results also indicated the time scalability of the proposed approach. We also analyzed the influence of various context combinations on recommendation accuracy and then calculated each context-to-context feature interaction to better understand each context's ad recommendation efficiency. For example, the feature interaction matrix analysis in the Instagram dataset revealed that features related to the broadcast time of posts and users are more critical than other contextual features. Therefore, companies can plan more accurately on the time and user's behavioral attributes to achieve advertising effectiveness and minimize spam ads. },
  issn={ 1573-0484 },
}

@article{Ok_2024,
  title={ Mapping digital literacy in language education: A comparative analysis of national curriculum standards using text as data approach },
  DOI={ doi:10.1007/s10639-024-13056-5 },
  journal={ Education and Information Technologies },
  author={ Deul RohJiseung YooHyounjin Ok },
  year={ 2024 },
  pages={ 1-27 },
  month={ 09 },
  abstract={ The need to integrate digital literacy into curricula as a fundamental skill set for navigating the complexities of the digital age has increased. This study aims to explore how knowledge and skills of digital literacy present in language arts standards and what elements of digital literacy are emphasized. To achieve this, we used text mining analysis and European Union’s digital literacy framework to analyze on the recent revisions of curriculum standards in Ontario, Canada, Australia, and South Korea. The research findings are as follows: Firstly, keyword network analysis revealed that in all three countries, the centrality of keywords related to digital literacy was not high, and there was a weak connection among these keywords. Secondly, when contrasting the results of topic modeling with the EU Framework, it was observed that Ontario, Canada, and South Korea prominently emphasized the "Information and data literacy" and "Communication and collaboration" elements, while Australia emphasized the "Digital content creation" element. Lastly, comparing the results of topic modeling on pre and post-revised curricula of Australia and South Korea, it was evident that societal awareness and discourse regarding digital literacy were well-reflected and revised in the standards. Based on these findings, this study suggests the need to integrate digital literacy-related elements more closely with traditional literacy skills within the curriculum, ensure a balanced approach across both cognitive and socio-emotional domains of digital literacy, and consider incorporating integrating digital literacy-in-use into students' learning experiences. },
  issn={ 1573-7608 },
}

@article{Cheng_2023,
  title={ Exploring contributors, collaborations, and research topics in educational technology: A joint analysis of mainstream conferences },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-022-11209-y },
  journal={ Education and Information Technologies },
  author={ Xieling ChenDi ZouHaoran XieGuanliang ChenJionghao LinGary Cheng },
  year={ 2023 },
  pages={ 1323-1358 },
  month={ 07 },
  abstract={ The diversity and advance of information, communication, and analytical technologies and their increasing adoption to assist instruction and learning give rise to various technology-driven conferences (e.g., artificial intelligence in education) in educational technology. Previous reviews on educational technology commonly focused on journal articles while seldom including mainstream conference papers which also contribute to an important part of scientific output in computer science and emerging disciplines like educational technology and are equally and even more important than articles in knowledge transmission. Hence, conference papers should also be included in bibliometric studies to produce a complete and precise picture of scientific production concerning educational technology. This study, therefore, uses bibliometrics and topic modeling to analyze papers from mainstream conferences, including Artificial Intelligence in Education, Learning Analytics and Knowledge, Educational Data Mining, Intelligent Tutoring System, and Learning at Scale, focusing on contributors, collaborations, and particularly research topics and topic evolutions to inform relevant stakeholders about educational technology’s development and its future. Results indicate promising areas like affective computing and behavior mining for adaptive instruction, recommender systems in personalized learning recommendations, eye-tracking for cognitive process diagnosis, videos for feedback provision, and natural language processing in discourse analysis and language education. },
  issn={ 1573-7608 },
}

@article{Wood_2023,
  title={ Presence and consequences of positive words in scientific abstracts },
  volume={ 128 },
  DOI={ doi:10.1007/s11192-023-04864-6 },
  journal={ Scientometrics },
  author={ Moritz EdlingerFinn BuchrieserGuilherme Wood },
  year={ 2023 },
  pages={ 6633-6657 },
  month={ 11 },
  abstract={ Abstracts are the showcase of scientific studies, crafted to make an&nbsp;impression on the reader within a limited space and to&nbsp;determine the amount of attention each study receives. Systemic conditions in the sciences may change the expressive norm and incentive scientists to hype abstracts to promote their work and career. Previous studies found that terms such as “unprecedented”, “novel” and “unique” have been used increasingly in recent history, to describe one’s own research findings. The present study investigates the use of valence-loaded scientific jargon in the abstracts of scientific articles. Sentiment analysis with dictionaries specifically attuned to detect valence-loaded scientific jargon was employed to analyze more than 2,300,000 MEDLINE abstracts from the fields of psychology, biology, and physics. Results show that over the last four decades, abstracts have contained an increasing amount of valence-loaded scientific jargon, as previously observed in earlier studies. Moreover, our results reveal that the positive emotional content of abstracts is increasing in a way that cannot be accounted for by the increase in text length, which has also been observed in the same time period. There were small differences between scientific disciplines. A detailed analysis of the distribution of valence-loaded scientific jargon within abstracts reveals a strong concentration towards the end of the text. We discuss these results in light of psychological evidence relating positive emotions with the propensity to overestimate the value of information to inform judgment and the increase in the competition for attention due to a pressure to publish. },
  issn={ 1588-2861 },
}

@article{Goujon_2024,
  title={ Galactica’s dis-assemblage: Meta’s beta and the omega of post-human science },
  DOI={ doi:10.1007/s00146-024-02088-7 },
  journal={ AI & SOCIETY },
  author={ Nicolas Chartier-EdwardsEtienne GrenierValentin Goujon },
  year={ 2024 },
  pages={ 1-13 },
  month={ 10 },
  abstract={ Released mid-November 2022, Galactica is a set of six large language models (LLMs) of different sizes (from 125&nbsp;M to 120B parameters) designed by Meta AI to achieve the ultimate ambition of “a single neural network for powering scientific tasks”, according to its accompanying whitepaper. It aims to carry out knowledge-intensive tasks, such as publication summarization, information ordering and protein annotation. However, just a few days after the release, Meta had to pull back the demo due to the strong hallucinatory tendencies or underwhelming performances of the model. This article aims to study, through a critical threefold argument, the potential impacts of LLMs once deployed in the scientific value chain. Our first argument is a technical one. By examining the technicity of Galactica, it is possible to explain the descripancies between its promotional corporate discourse and abysmal outputs. Second, by going back to debates in both computer science and computational philosophy on the automation of abduction, we argue from the epistemological front that LLMs indeed cannot produce strong abductions and, therefore, claims about the automation of hypothesis generation remains chambering. Finally, our third argument is a sociological one. By conceptualizing the scientific field through Nancy Katherine Hayles’ cognitive assemblage theory, we aim to outline the potential steering of science by LLMs, mainly through information ordering. The core of our argument rests on the assertion that excessive control on information risks contravening a certain serendipitous aspect inherent to scientific discoveries. },
  issn={ 1435-5655 },
}

@article{Hua_2023,
  title={ EmoDNN: understanding emotions from short texts through a deep neural network ensemble },
  volume={ 35 },
  DOI={ doi:10.1007/s00521-023-08435-x },
  journal={ Neural Computing and Applications },
  author={ Sara KamranRaziyeh ZallSaeid HosseiniMohammadReza KangavariSana RahmaniWen Hua },
  year={ 2023 },
  pages={ 13565-13582 },
  month={ 03 },
  abstract={ The knowledge obtained from emotions via online communities is substantially valuable in various domains, including social management, resource planning, politics, and market predictions. Affective computing, as a multi-aspect realm, aims to exploit emotion-pertinent details from various contents via connecting artificial intelligence to cognitive science. The hidden personality cues in daily brief contents can reveal the cognitive aspect of authors and uncover both similarities and contrasts between them. However, the main challenge lies in devising a cognition-aware algorithm to trace emotional cues in brief contents. To solve the challenge, we develop a novel framework that infers the cognitive aspect of individuals. We propose a deep ensemble method, supplied with a novel dropout algorithm, that aggregates outcomes from various classifiers to extract emotions from short texts. We employ a new embedding approach to enrich emotion-relevant features, collectively assembled via lexicons and attention actuates, resulting in a preferable set of vectors. The experimental results show that our proposed framework can achieve better accuracy in recognizing emotions versus other trending competitors. We empirically observe that detecting emotion latent cues via relying on personality features can effectively distinguish short text authors. Furthermore, the deep learning models overcome conventional methods, including the SVM, categorization, and heuristic rules. },
  issn={ 1433-3058 },
}

@article{Bhattacharjee_2023,
  title={ Style matching CAPTCHA: match neural transferred styles to thwart intelligent attacks },
  volume={ 29 },
  DOI={ doi:10.1007/s00530-023-01075-0 },
  journal={ Multimedia Systems },
  author={ Palash RayAsish BeraDebasis GiriDebotosh Bhattacharjee },
  year={ 2023 },
  pages={ 1865-1895 },
  month={ 03 },
  abstract={ Completely automated public turing test to tell computers and humans apart (CAPTCHA) is widely used to prevent malicious automated attacks on various online services. Text- and image-CAPTCHAs have shown broader acceptability due to usability and security factors. However, recent progress in deep learning implies that text-CAPTCHAs can easily be exposed to various fraudulent attacks. Thus, image-CAPTCHAs are getting research attention to enhance usability and security. In this work, the neural-style transfer (NST) is adapted for designing an image-CAPTCHA algorithm to enhance security while maintaining human performance. In NST-rendered image-CAPTCHAs, existing methods inquire a user to identify or localize the salient object (e.g., content) which is solvable effortlessly by off-the-shelf intelligent tools. Contrarily, we propose a Style Matching CAPTCHA (SMC) that asks a user to select the style image which is applied in the NST method. A user can solve a random SMC challenge by understanding the semantic correlation between the content and style output as a cue. The performance in solving SMC is evaluated based on the 1368 responses collected from 152 participants through a web-application. The average solving accuracy in three sessions is 95.61%; and the average response time for each challenge per user is 6.52&nbsp;s, respectively. Likewise, a Smartphone Application (SMC-App) is devised using the proposed method. The average solving accuracy through SMC-App is 96.33%, and the average solving time is 5.13&nbsp;s. To evaluate the vulnerability of SMC, deep learning-based attack schemes using Convolutional Neural Networks (CNN), such as ResNet-50 and Inception-v3 are simulated. The average accuracy of attacks considering various studies on SMC using ResNet-50 and Inception-v3 is 37%, which is improved over existing methods. Moreover, in-depth security analysis, experimental insights, and comparative studies imply the suitability of the proposed SMC. },
  issn={ 1432-1882 },
}

@article{Cao_2023,
  title={ Reverse-engineering information presentations: recovering hierarchical grouping from layouts of visual elements },
  volume={ 1 },
  DOI={ doi:10.1007/s44267-023-00010-1 },
  journal={ Visual Intelligence },
  author={ Danqing ShiWeiwei CuiDanqing HuangHaidong ZhangNan Cao },
  year={ 2023 },
  pages={ 1-14 },
  month={ 06 },
  abstract={ Visual elements in an information presentation are often spatially and semantically grouped hierarchically for effective message delivery. Studying the hierarchical grouping information can help researchers and designers better explore layout structures and understand design demographics. However, recovering hierarchical grouping is challenging due to a large number of possibilities for compositing visual elements into a single-page design. This paper introduces an automatic approach that takes the layout of visual elements as input and returns the hierarchical grouping as output. To understand information presentations, we first contribute a dataset of 23,072 information presentations with diverse layouts to the community. Next, we propose our technique with a Transformer-based model to predict relatedness between visual elements and a bottom-up algorithm to produce the hierarchical grouping. Finally, we evaluate our technique through a technical experiment and a user study with 30 designers. The results show that the proposed technique is promising. },
  issn={ 2731-9008 },
}

@article{Omri_2023,
  title={ A survey of sentiment analysis from film critics based on machine learning, lexicon and hybridization },
  volume={ 35 },
  DOI={ doi:10.1007/s00521-023-08359-6 },
  journal={ Neural Computing and Applications },
  author={ Mustafa Abdalrassual JassimDhafar Hamed AbdMohamed Nazih Omri },
  year={ 2023 },
  pages={ 9437-9461 },
  month={ 03 },
  abstract={ Recent research and developments in the field of Sentiment Analysis (SA) have made it possible to simplify the detection and classification of sentiments from the textual content. This type of analysis classifies the text according to its positive, negative, or neutral polarity. Recently, researchers have focused on film reviews and aim to extract personal information about text reviews that can, for example, be used to determine the listener’s position on a number of different topics. The main contributions, proposed in the literature, focused on three categories of approaches: (i) a first category based on the lexicon, (ii) a second category based on machine learning, and (iii) a third based on a hybridization of the two previous categories. To our knowledge, and until the elaboration of this study, no previous study has examined the approaches and levels of sentiment in the field of film reviews. In this article, we propose to review and analyze the main works in this field. We begin by giving a methodological review of our study. Then, we present a taxonomy on the domain of sentiment analysis and a generic view of the main families of sentiment classification techniques. As a next step, we describe the different levels of sentiment analysis considered in the literature, then we expose the process of pre-processing, extracting, and selecting the characteristics necessary for the sentiment analysis. We then propose an analysis and a discussion of the results of the main works studied on sentiment analysis. This presentation will then be followed by a discussion of some research questions and a proposal for a number of future directions in this area that we believe are essential to contribute to solving the problem addressed in this article. },
  issn={ 1433-3058 },
}

@article{Corral_2024,
  title={ An academic recommender system on large citation data based on clustering, graph modeling and deep learning },
  volume={ 66 },
  DOI={ doi:10.1007/s10115-024-02094-7 },
  journal={ Knowledge and Information Systems },
  author={ Vaios StergiopoulosMichael VassilakopoulosEleni TousidouAntonio Corral },
  year={ 2024 },
  pages={ 4463-4496 },
  month={ 04 },
  abstract={ Recommendation (recommender) systems (RS) have played a significant role in both research and industry in recent years. In the area of academia, there is a need to help researchers discover the most appropriate and relevant scientific information through recommendations. Nevertheless, we argue that there is a major gap between academic state-of-the-art RS and real-world problems. In this paper, we present a novel multi-staged RS based on clustering, graph modeling and deep learning that manages to run on a full dataset (scientific digital library) in the magnitude of millions users and items (papers). We run several tests (experiments/evaluation) as a means to find the best approach regarding the tuning of our system; so, we present and compare three versions of our RS regarding recall and NDCG metrics. The results show that a multi-staged RS that utilizes a variety of techniques and algorithms is able to face real-world problems and large academic datasets. In this way, we suggest a way to close or minimize the gap between research and industry value RS. },
  issn={ 0219-3116 },
}

@article{Surer_2023,
  title={ Visualization in virtual reality: a systematic review },
  volume={ 27 },
  DOI={ doi:10.1007/s10055-023-00753-8 },
  journal={ Virtual Reality },
  author={ Elif Hilal KorkutElif Surer },
  year={ 2023 },
  pages={ 1447-1480 },
  month={ 01 },
  abstract={ Rapidly growing virtual reality (VR) technologies and techniques have gained importance over the past few years, and academics and practitioners have been searching for efficient visualizations in VR. To date, the emphasis has been on the employment of game technologies. Despite the growing interest and potential, visualization studies have lacked a common baseline in the transition period of 2D visualizations to immersive ones. To this end, the presented study aims to provide a systematic literature review that explains the state-of-the-art research and future trends in visualization in virtual reality. The research framework is grounded in empirical and theoretical works of visualization. We characterize the reviewed literature based on three dimensions: (a) Connection with visualization background and theory, (b) Evaluation and design considerations for virtual reality visualization, and (c) Empirical studies. The results from this systematic review suggest that: (1) There are only a few studies that focus on creating standard guidelines for virtual reality, and each study individually provides a framework or employs previous studies on traditional 2D visualizations; (2) With the myriad of advantages provided for visualization and virtual reality, most of the studies prefer to use game engines; (3) Although game engines are extensively used, they are not convenient for critical scientific studies; and (4) 3D versions of traditional statistical visualization techniques, such as bar plots and scatter plots, are still commonly used in the data visualization context. This systematic review attempts to add a clear picture of the emerging contexts, different elements, and interdependencies to the literature. },
  issn={ 1434-9957 },
}

@article{Xu_2023,
  title={ Two-way Concept-Cognitive Learning with Multi-source Fuzzy Context },
  volume={ 15 },
  DOI={ doi:10.1007/s12559-023-10107-w },
  journal={ Cognitive Computation },
  author={ Xiaoyan ZhangDoudou GuoWeihua Xu },
  year={ 2023 },
  pages={ 1526-1548 },
  month={ 02 },
  abstract={ Concepts learning is the most fundamental unit in the process of human cognition in philosophy. Granularity is one of the fundamental concepts of human cognition. The combination of granular computing and concept learning is critical in the cognitive process. Meanwhile, efficiently and accurately using the information collected from different sources is the focus of data mining in the contemporary. Hence, how to sufficiently learn concepts under a multi-sources context is an essential concern in the field of cognition. This paper offers a new thought for two-way concept-cognitive learning based on granular computing in multi-source fuzzy decision tables. Firstly, based on the best possible guarantee of the classification ability, original information from different sources is fused by conditional entropy, which is the kind of multi-source fusion method (i.e., CE-fusion). Secondly, we learn concepts from a given object set, attribute set, or pair of object and attribute sets in the fused information table, and these three types of concept learning algorithms are designed. This analysis shows that two-way concept learning based on multi-source information fusion is a suitable method of multi-source concept learning. Some examples are valuable for applying these theories to deal with practical issues. Our work will provide a convenient novel tool for researching concept-cognitive learning methods with multi-source fuzzy context. },
  issn={ 1866-9964 },
}

@article{Li_2024,
  title={ Research on full-cycle sharing method of natural resources big data based on middle platform architecture },
  DOI={ doi:10.1007/s11042-024-20434-1 },
  journal={ Multimedia Tools and Applications },
  author={ Qian Li },
  year={ 2024 },
  pages={ 1-18 },
  month={ 11 },
  abstract={ Natural resources refer to the sum of natural environmental factors that exist naturally, have use value, and can be classified into current and future well-being. In order to realize the sharing of natural resources big data, this paper puts forward a full-cycle sharing method of natural resources big data based on the middle platform architecture. Build an analysis model of spatial distribution, time change and attribute characteristic parameters of natural resource big data response, obtain the characteristic quantity of cross-space coverage of natural resource change and state, establish the middle platform architecture system of resource big data by the method of natural resource collaborative perception, and realize the joint acquisition of natural resource big data by the method of aviation acquisition, ground observation and professional monitoring. Build a natural resources spatio-temporal database with business elements connected, rich thematic information and multi-scale integration, and solve the problems of interconnection, dynamic convergence and effective integration of multi-source business information of natural resources big data. Through high-level spatio-temporal cognition and processing, the spatio-temporal distribution law of natural resources can be obtained, and the full-cycle sharing of resource big data can be realized. The test results show that this method has good time–space coupling and strong conflict coordination and resolution ability for the full-cycle sharing of resource big data, and it has a good application value in the simulation, optimization and regulation of different scenarios of natural resources development, utilization and protection. },
  issn={ 1573-7721 },
}

@article{Bedi_2023,
  title={ Empowering reciprocal recommender system using contextual bandits and argumentation based explanations },
  volume={ 26 },
  DOI={ doi:10.1007/s11280-023-01173-z },
  journal={ World Wide Web },
  author={ Tulika KumariBhavna GuptaRavish SharmaPunam Bedi },
  year={ 2023 },
  pages={ 2969-3000 },
  month={ 05 },
  abstract={ Reciprocal Recommender Systems (RRS) aim to recommend relevant matches to users based on the mutual agreement of their preferences. Explainability of reciprocal recommendations is important for developing a persuasive reciprocal recommender system, since it can improve the effectiveness and credibility of the reciprocal recommendation results. Explainable RRS provide an explanation highlighting why a recommendation would be relevant to the user. Explaining the rationale behind predictions with textual or visual artifacts help in increasing trustworthiness and transparency of the system which is crucial especially for models that are used in critical decision making. In this work, XSiameseBiGRU-UCB, a deep learning contextual bandits framework with post-hoc argumentation based explanations for RRS is proposed. XSiameseBiGRU-UCB is an explainable Siamese neural network-based framework that provides explanations to justify the generated reciprocal recommendations for both the parties involved. In RRS, dilemma between exploitation and exploration requires identifying the best possible recommendation from known information or collecting more information about the environment while generating reciprocal recommendations. To tackle this, we propose to use a contextual bandit policy with upper confidence bound, which adaptively exploits and explores user interests to achieve increased rewards in the long run. Experimental studies conducted with four real-world datasets validate the efficacy of the proposed approach. },
  issn={ 1573-1413 },
}

@article{Al-Adhaileh_2024,
  title={ Machine Learning Algorithms for Predicting and Analyzing Arabic Sentiment },
  volume={ 5 },
  DOI={ doi:10.1007/s42979-024-03494-w },
  journal={ SN Computer Science },
  author={ Amani A. AladeemyTheyazn H.H. AldhyaniAli AlzahraniEidah M. AlzahraniOsamah Ibrahim KhalafSaleh Nagi AlsubariSachin N. DeshmukhMosleh Hmoud Al-Adhaileh },
  year={ 2024 },
  pages={ 1-10 },
  month={ 12 },
  abstract={ Sentiment analysis (SA) is a specialized application within the domain of natural language processing (NLP) used to analyze textual input to identify underlying sentiments, emotions, and attitudes. Recent advancements in machine learning (ML) have demonstrated potential in accurately detecting sentiment in Arabic sentiment analysis (ASA). This paper investigates the efficacy of various feature extraction and ML methods for ASA, emphasizing unique Arabic data preprocessing techniques: removing stopwords, repetitions, punctuation, non-Arabic characters, hashtags, mentions, and URLs; deleting diacritics and tashkeel; stemming; tokenizing; and replacing emojis with text representations. We utilized two benchmark datasets, ASTC and Ar-Twitter, focused on social media content, to evaluate our methodology. The comparison involved two feature extraction approaches and five ML techniques: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (DT), Logistic Regression (LR), and XGBoost. Our experimental investigation assessed datasets from Twitter on various topics, such as politics and arts, to evaluate their suitability for ASA. Feature extraction techniques included Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) with unigrams and bigrams. The aim of this comparison is also to explore the influence of feature extraction techniques in developing ASA systems. The best results were achieved using the RF model with unigram features, yielding an accuracy of 87.7% on the ASTC dataset, while the SVM model attained 90.3% accuracy on the Ar-Twitter dataset. This study highlights the importance of detailed preprocessing and robust feature extraction in improving ASA performance, setting a new benchmark for future research in Arabic NLP. },
  issn={ 2661-8907 },
}

@article{Hassim_2024,
  title={ An efficient two-state GRU based on feature attention mechanism for sentiment analysis },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-022-13339-4 },
  journal={ Multimedia Tools and Applications },
  author={ Muhammad ZulqarnainRozaida GhazaliMuhammad AamirYana Mazwin Mohmad Hassim },
  year={ 2024 },
  pages={ 3085-3110 },
  month={ 06 },
  abstract={ Sentiment analysis is one of the most challenging tasks in natural language processing (NLP). The extensively used application of sentiment analysis is sentiment classification of reviews. The purpose of sentiment classification is to determine the sentiment polarity of user opinion, attitude, and emotions expressed in the form of text into positive, negative and neutral polarities. Many advanced deep learning approaches have been proposed to solve sentiment analysis problem. Recurrent neural network (RNN) is one of the popular deep learning architectures which is widely employed in sentiment analysis. In this paper, we proposed a Two State GRU (TS-GRU) based on feature attention mechanism that concentrates on identifying and categorization of the sentiment polarity using sequential modeling and word-feature seizing. The proposed approach integrates pre-feature attention in TS-GRU to associate the complex connection between words by sentence based sequential modeling and capturing the keywords using attention layer for sentiment polarity. Subsequently, a decoder function has been added in the post-feature attention GRU, in order to extract the predicted features during attention mechanism. The proposed approach has been evaluated on three benchmark datasets including IMDB, MR, and SST2. Experimental results conclude that the proposed TS-GRU model obtained higher sentiment analysis accuracy of 90.85%, 80.72%, and 86.51% on IMDB, MR, and SST2 datasets, respectively. },
  issn={ 1573-7721 },
}

@article{Li_2023,
  title={ Investigating online learners’ knowledge structure patterns by concept maps: A clustering analysis approach },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-023-11633-8 },
  journal={ Education and Information Technologies },
  author={ Xiuling HeJing FangHercy N. H. ChengQibin MenYangyang Li },
  year={ 2023 },
  pages={ 11401-11422 },
  month={ 02 },
  abstract={ A deep understanding of the learning level of online learners is a critical factor in promoting the success of online learning. Using knowledge structures as a way to understand learning can help analyze online students’ learning levels. The study used concept maps and clustering analysis to investigate online learners’ knowledge structures in a flipped classroom’s online learning environment. Concept maps (n = 359) constructed by 36 students during one semester (11 weeks) through the online learning platform were collected as analysis objects of learners’ knowledge structures. Clustering analysis was used to identify online learners’ knowledge structure patterns and learner types, and a non-parametric test was used to analyze the differences in learning achievement among learner types. The results showed that (1) there were three online learners’ knowledge structure patterns of increasing complexity, namely, spoke, small-network, and large-network patterns. Moreover, online learners with novice status mostly had spoke patterns in the context of flipped classrooms’ online learning. (2) Two types of online learners were found to have different distributions of knowledge structure patterns, and the complex knowledge structure type of learners exhibited better learning achievement. The study explored a new way for educators to analyze knowledge structures by data mining automatically. The findings provide evidence in the online learning context for the relationship between complex knowledge structures and better learning achievement while suggesting the existence of inadequate knowledge preparedness for flipped classroom learners without a special instructional design. },
  issn={ 1573-7608 },
}

@article{Patnaik_2024,
  title={ Towards making computers conscious: trends and challenges },
  volume={ 7 },
  DOI={ doi:10.1007/s42044-023-00164-7 },
  journal={ Iran Journal of Computer Science },
  author={ L. M. Patnaik },
  year={ 2024 },
  pages={ 139-153 },
  month={ 11 },
  abstract={ The question of consciousness has puzzledmany great philosophersfrom time immemorial and rightly so because of the fact that our brain is made up of flesh and blood and yet produces such vivid subjective experience s of reality. The topic of consciousness has been confined to the field of philosophy till recently; later, neuroscientists, psychologists and computer scientists have joined this quest to resolve the enigma of consciousness. Amongst many such varying approaches to understanding consciousness, there is a particular perspective called “Machine Consciousness” embraced by many prominent scientists and engineers working on reproducing consciousness in a machine. This paper is an exhaustive review of the advancements in machine consciousness and the underlying theories and cognitive architectures. This paper also covers criticisms and shortfalls of relevant approaches and elaborates on machine consciousness’s relationship with other fields. },
  issn={ 2520-8446 },
}

@article{Lan_2024,
  title={ The key artificial intelligence technologies in early childhood education: a review },
  volume={ 57 },
  DOI={ doi:10.1007/s10462-023-10637-7 },
  journal={ Artificial Intelligence Review },
  author={ Honghu YiTing LiuGongjin Lan },
  year={ 2024 },
  pages={ 1-36 },
  month={ 01 },
  abstract={ Artificial Intelligence (AI) technologies have been applied in various domains, including early childhood education (ECE). Integration of AI educational technology is a recent significant trend in ECE. Currently, there are more and more studies of AI in ECE. To date, there is a lack of survey articles that discuss the studies of AI in ECE. In this paper, we provide an up-to-date and in-depth overview of the key AI technologies in ECE that provides a historical perspective, summarizes the representative works, outlines open questions, discuss the trends and challenges through a detailed bibliometric analysis, and provides insightful recommendations for future research. We mainly discuss the studies that apply AI-based robots and AI technologies to ECE, including improving the social interaction of children with an autism spectrum disorder. This paper significantly contributes to provide an up-to-date and in-depth survey that is suitable as introductory material for beginners to AI in ECE, as well as supplementary material for advanced users. },
  issn={ 1573-7462 },
}

@article{Qing_2023,
  title={ Brain-inspired filtering Network for small infrared target detection },
  volume={ 82 },
  DOI={ doi:10.1007/s11042-023-14762-x },
  journal={ Multimedia Tools and Applications },
  author={ Ju MoranHu Qing },
  year={ 2023 },
  pages={ 28405-28426 },
  month={ 03 },
  abstract={ Small infrared target detection is a challenge in computer vision because small infrared target occupies fewer pixels and the environment around the small infrared targets is complex. Although numerous small infrared target detection methods have been proposed, training an end-to-end deep detection model for small infrared target has not been thoroughly investigated. In this paper, we design a brain-inspired neural network (FilterDet) for small infrared target detection. There are two inter-connected modules in FilterDet, namely the brain-inspired filtering module and the target detection module. Brain-inspired filtering module consists of bottom-up filtering module and top-down filtering module which are modeled by bottom-up filtering mechanism and top-down filtering mechanism of the human brain, aiming to filter out the complex environment and interference. Target detection module takes the filtered infrared images as input and performs small infrared target detection. To train FilterDet in an end-to-end way, the loss function is designed by multi-task loss. Furthermore, we build a synthetic single frame infrared image set by generating synthetic infrared images with small targets. Comparative experiments are conducted on three real infrared image sequences and the single frame infrared image set to demonstrate the detection performance of FilterDet. The results show FilterDet has better performance for small infrared target detection compared with other detectors. },
  issn={ 1573-7721 },
}

@article{P_2023,
  title={ Abstractive text summarization employing ontology-based knowledge-aware multi-focus conditional generative adversarial network (OKAM-CGAN) with hybrid pre-processing methodology },
  volume={ 82 },
  DOI={ doi:10.1007/s11042-022-14155-6 },
  journal={ Multimedia Tools and Applications },
  author={ Nafees Muneera MSriramya P },
  year={ 2023 },
  pages={ 23305-23331 },
  month={ 11 },
  abstract={ Over the past few years, the explainable artificial intelligence (XAI) model receives a broad desire for investigation. The natural language processing (NLP) commune is reaching the fundamental change too – constructing a set of paradigms, which describe the preference on a few chief jobs devoid of influencing the execution. Abstractive Text Summarization (ATS) remains the job of building summary sentences by fusing factualities out of disparate source sentences and compressing them into a smaller portrayal when sustaining data content and comprehensive sense. This remains extremely arduous and long-drawn-out for people to physically summarize huge text documents. This study proffers Ontology-based Knowledge Aware Multi-focus Conditional Generative Adversarial Network (OKAM-CGAN) for novel documents. This could build novel sentences by analyzing many finer pieces than sentences, especially, semantic phrases. The proffered OKAM-CGAN comprises 3 prime portions – ontology aware knowledge-based document representation module, multitask and multi-focus learning unit, and an adversarial network unit. Experiential assessment is performed by correlating with advanced methodologies like RNN-W, CopyNet, GCU, Seq2Seq, and KESG concerning ROUGE scores. Consequently, it is observed that the proffered OKAM-CGAN attains 42.1% of ROUGE-L, 40% of accuracy, 45%of precision, and 53% of recall for the CNN/Daily Mail database and 45% of ROUGE-L, 4% of accuracy, 54% of precision, and 57% of recall for the Edmunds database. },
  issn={ 1573-7721 },
}

@article{Wang_2024a,
  title={ Distributed Deep Reinforcement Learning: A Survey and a Multi-player Multi-agent Learning Toolbox },
  volume={ 21 },
  DOI={ doi:10.1007/s11633-023-1454-4 },
  journal={ Machine Intelligence Research },
  author={ Qiyue YinTongtong YuShengqi ShenJun YangMeijing ZhaoWancheng NiKaiqi HuangBin LiangLiang Wang },
  year={ 2024 },
  pages={ 411-430 },
  month={ 01 },
  abstract={ With the breakthrough of AlphaGo, deep reinforcement learning has become a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning difficult to apply in a wide range of areas. Many methods have been developed for sample efficient deep reinforcement learning, such as environment modelling, experience transfer, and distributed modifications, among which distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analysing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing the usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping that this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning. },
  issn={ 2731-5398 },
}

@article{Zhang_2023a,
  title={ Real-time instance segmentation with assembly parallel task },
  volume={ 39 },
  DOI={ doi:10.1007/s00371-022-02537-8 },
  journal={ The Visual Computer },
  author={ Zhen YangYang WangFan YangZhijian YinTao Zhang },
  year={ 2023 },
  pages={ 3937-3947 },
  month={ 06 },
  abstract={ Although instance segmentation has made significant progress in recent years, it is still a challenge to develop highly accurate algorithms with real-time performance. In this paper, we propose a real-time framework denoted by APTMask for instance segmentation, which builds on the real-time project YOLACT. In APTMask, we use Swin-Transformer Tiny with PA-FPN as the default feature backbone and a base image size of $$ 544\times 544 $$ . We devise a new mask branch, which can more effectively exploit the semantic information of PA-FPN deeper features and the positional information of shallow features for mask representation, compared to the use of implicit parameterized forms. We replace fast NMS with Cluster NMS, which compensates for the performance penalty of fast NMS compiled to standard NMS. CIoU loss is also adopted to fully exploit the scale information of the aspect ratio of the bounding box. Experimental results show that APTMask can achieve 39.7/34.7 box/mask AP on COCO val2017 dataset at 31.8 fps evaluated with a single RTX 2080TI GPU card. Compared to YOLACT, APTMask improves the box AP by about 8.0% and the mask AP by 6.2%, which is encouraging and competitive. Given its simplicity and efficiency, we hope that our APTMask can serve as a simple but strong baseline for a variety of instance-wise prediction tasks. },
  issn={ 1432-2315 },
}

@article{Chu_2023,
  title={ A review of AI teaching and learning from 2000 to 2020 },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-022-11491-w },
  journal={ Education and Information Technologies },
  author={ Davy Tsz Kit NgMin LeeRoy Jun Yi TanXiao HuJ. Stephen DownieSamuel Kai Wah Chu },
  year={ 2023 },
  pages={ 8445-8501 },
  month={ 12 },
  abstract={ In recent years, with the popularity of AI technologies in our everyday life, researchers have begun to discuss an emerging term “AI literacy”. However, there is a lack of review to understand how AI teaching and learning (AITL) research looks like over the past two decades to provide the research basis for AI literacy education. To summarize the empirical findings from the literature, this systematic literature review conducts a thematic and content analysis of 49 publications from 2000 to 2020 to pave the way for recent AI literacy education. The related pedagogical models, teaching tools and challenges identified help set the stage for today’s AI literacy. The results show that AITL focused more on computer science education at the university level before 2021. Teaching AI had not become popular in K-12 classrooms at that time due to a lack of age-appropriate teaching tools for scaffolding support. However, the pedagogies learnt from the review are valuable for educators to reflect how they should develop students’ AI literacy today. Educators have adopted collaborative project-based learning approaches, featuring activities like software development, problem-solving, tinkering with robots, and using game elements. However, most of the activities require programming prerequisites and are not ready to scaffold students’ AI understandings. With suitable teaching tools and pedagogical support in recent years, teaching AI shifts from technology-oriented to interdisciplinary design. Moreover, global initiatives have started to include AI literacy in the latest educational standards and strategic initiatives. These findings provide a research foundation to inform educators and researchers the growth of AI literacy education that can help them to design pedagogical strategies and curricula that use suitable technologies to better prepare students to become responsible educated citizens for today’s growing AI economy. },
  issn={ 1573-7608 },
}

@article{Li_2024a,
  title={ Citation advantage of positive words: predictability, temporal evolution, and universality in varied quality journals },
  volume={ 129 },
  DOI={ doi:10.1007/s11192-024-05074-4 },
  journal={ Scientometrics },
  author={ Dengsheng WuHuidong WuJianping Li },
  year={ 2024 },
  pages={ 4275-4293 },
  month={ 06 },
  abstract={ The number of positive words in scientific papers has exhibited a notable upwards trend over the past few decades. However, there remains a gap in our comprehensive understanding of the relationship between positive words and research impact. In this study, we conduct a multifaceted exploration of the citation advantage associated with positive words based on social cognitive theory, examining its predictability, temporal evolution, and universality across journals of varying quality grades. Drawing from a corpus encompassing 124,144 papers published in the management field between 2001 and 2020, our regression results provide compelling evidence suggesting that positive words can serve as a significant predictor of the citation counts of academic papers, supporting the citation advantage of positive words. However, it is essential to recognize that over time, the citation advantage attributed to positive words is experiencing a conspicuous decline. The universality of the above phenomenon has been further verified in the analysis of journals of different quality. Our findings prompt a discussion regarding the need to pay more attention to the overuse and misuse of positive words, as well as practical considerations for enhancing scientific communication within the academic community. },
  issn={ 1588-2861 },
}

@article{Lee_2024,
  title={ Developing a computer-based tutor utilizing Generative Artificial Intelligence (GAI) and Retrieval-Augmented Generation (RAG) },
  DOI={ doi:10.1007/s10639-024-13129-5 },
  journal={ Education and Information Technologies },
  author={ Youngjin Lee },
  year={ 2024 },
  pages={ 1-22 },
  month={ 11 },
  abstract={ This study investigates the development and evaluation of a Retrieval-Augmented Generation (RAG)-based statistics tutor designed to assist students with quantitative analysis methods. The RAG approach was employed to address the well-documented issue of hallucination in Large Language Models (LLMs). A computer tutor was developed that utilizes ChatGPT to understand student questions in natural language and retrieves relevant information from validated course notes to generate responses. The tutor was evaluated using twenty questions curated from the course notes. Two instructors who taught the course using the course notes assessed the quality of the tutor’s response using a pre-defined rubric. The evaluation results indicate that the RAG-based tutor was able to provide correct and relevant answers to all twenty questions and to generate R code snippets upon request. This study highlights the potential of RAG-based tutors to provide accurate and personalized learning experiences for students while mitigating the risk of providing fabricated information often associated with LLMs. },
  issn={ 1573-7608 },
}

@article{Wang_2023b,
  title={ Text recognition and analysis of network public opinion focus events of a major epidemic: a case study of “COVID-19” in Sina Microblogs },
  volume={ 82 },
  DOI={ doi:10.1007/s11042-023-14916-x },
  journal={ Multimedia Tools and Applications },
  author={ HeLin WeiChenying HaiDonglu ShanBei LyuXiulai Wang },
  year={ 2023 },
  pages={ 25811-25827 },
  month={ 03 },
  abstract={ Identifying and analyzing the public’s opinion of focal events during a major epidemic can help the government grasp the vicissitudes of network public opinion in a timely manner and provide the appropriate responses. Taking the COVID-19 epidemic as an example, this study begins by using Python-selenium to capture the original text and comment data related to COVID-19 from Sina Microblog’s CCTV News from Jan. 19, 2020, to Feb. 20, 2020. The study subsequently uses a manual interpretation method to classify the Weibo content and analyzes the shifting focus phenomena of network public opinion based on the moving average method. Next, the study uses an enhances TF-IDF to extract keywords from the Weibo comment and uses the keywords to construct a word co-occurrence network. The results show that during the epidemic, the network public opinion focus shifted significantly over time. With the progression of the epidemic, the focus of network public opinion diversified, and various categories stabilized. Compared to simple keyword and text classification recognition focus problems, the proposed model, which is highly accurate, identified multiple network public opinion focus problems and described the core contradictions of the different focus problems. },
  issn={ 1573-7721 },
}

@article{Wang_2024b,
  title={ Graph pooling in graph neural networks: methods and their applications in omics studies },
  volume={ 57 },
  DOI={ doi:10.1007/s10462-024-10918-9 },
  journal={ Artificial Intelligence Review },
  author={ Yan WangWenju HouNan ShengZiqi ZhaoJialin LiuLan HuangJuexin Wang },
  year={ 2024 },
  pages={ 1-55 },
  month={ 09 },
  abstract={ Graph neural networks (GNNs) process the graph-structured data using neural networks and have proven successful in various graph processing tasks. Currently, graph pooling operators have emerged as crucial components that bridge the gap between node representation learning and diverse graph-level tasks by transforming node representations into graph representations. Given the rapid growth and widespread adoption of graph pooling, this review aims to summarize the existing graph pooling operators for GNNs and their representative applications in omics. Specifically, we first present a comprehensive taxonomy of existing graph pooling algorithms, expanding the categorization for both global and hierarchical pooling operators, and for the first time reviewing the inverse operation of graph pooling, named unpooling. Next, we describe the general evaluation framework for graph pooling operators, encompassing three fundamental aspects: experimental setup, ablation analysis, and model interpretation. We also discuss open issues that significantly influence the design of graph pooling operators, including complexity, connectivity, adaptability, additional loss, and attention mechanisms. Finally, we summarize bioinformatics applications of graph pooling operators in omics, including graphs of gene interaction, medical images, and protein structures for drug discovery and disease diagnosis. Furthermore, we showcase the impact of graph pooling operators on research in specific real-world domains, with a focus on prediction performance and model interpretability. This review provides methodological insights in machine learning based graph modeling and related omics research, as well as an ongoing resource by gathering related papers and code in a dedicated GitHub repository ( https://github.com/Hou-WJ/Graph-Pooling-Operators-and-Bioinformatics-Applications ). },
  issn={ 1573-7462 },
}

@article{Wang_2023c,
  title={ A meta-analysis of the effectiveness of programming teaching in promoting K-12 students’ computational thinking },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-022-11445-2 },
  journal={ Education and Information Technologies },
  author={ Enwei XuWei WangQingxia Wang },
  year={ 2023 },
  pages={ 6619-6644 },
  month={ 11 },
  abstract={ Computational thinking is considered to be an important competence in the intelligent era, and the incorporation of computational thinking as an integral part of school education beginning in childhood has been proposed. However, the ways in which computational thinking can be taught more effectively the context of in K-12 programming teaching remain unclear. This paper reports the results of a meta-analysis of 28 empirical studies on K-12 programming teaching that were published in international education journals in the 21st century to determine which teaching methods and programming tools are most effective in promoting the computational thinking of K-12 students. The results show that (1) programming teaching can promote the improvement of K-12 students’ computational thinking (ES = 0.72, z = 9.9, P &lt; 0.01), with an overall effect at the upper-middle level (95% CI[0.60,0.83]); (2) scaffolding programming (ES = 1.84, z = 11.9, P &lt; 0.01) and problem-based programming (ES = 1.14, z = 5.57, P &lt; 0.01) are the most effective teaching methods and can significantly promote the development of K-12 students’ computational thinking (chi2 = 40.58, P &lt; 0.01); (3) since differences in the effect of programming tools between groups are not significant (Chi2 = 6.47, P = 0.09), it is impossible to determine which programming tools are most effective; and (4) intervention duration (ES = 0.72, z = 11.9, P &lt; 0.05, 95% CI[0.60, 0.83]) and learning scaffold (ES = 0.83, z = 6.27, P &lt; 0.05, 95% CI[0.57, 1.09]) are both key moderating variables that affect the improvement of computational thinking. Based on these results, suggestions are provided for future research and practice. },
  issn={ 1573-7608 },
}

@article{Mishra_2024,
  title={ Enhancing knowledge discovery and management through intelligent computing methods: a decisive investigation },
  volume={ 66 },
  DOI={ doi:10.1007/s10115-024-02099-2 },
  journal={ Knowledge and Information Systems },
  author={ Rayees AhamadKamta Nath Mishra },
  year={ 2024 },
  pages={ 3719-3771 },
  month={ 04 },
  abstract={ Knowledge Discovery and Management (KDM) encompasses a comprehensive process and approach involving the creation, discovery, capture, organization, refinement, presentation, and provision of data, information, and knowledge with a specific goal in mind. At the core, Knowledge Management and Artificial Intelligence (AI) revolve around knowledge itself. AI serves as the mechanism enabling machines to obtain, acquire, process, and utilize information, thereby executing tasks and uncovering knowledge that can be shared with people to enhance strategic decision-making. While conventional methods play a role in the KDM process, incorporating intelligent approaches can further enhance efficiency in terms of time and accuracy. Intelligent techniques, particularly soft computing approaches, possess the ability to learn in any environment by leveraging logic, reasoning, and other computational capabilities. These techniques can be broadly categorized into Learning algorithms (Supervised, Unsupervised, and Reinforcement), Logic and Rule-Based algorithms (Fuzzy Logic, Bayesian Network, and CBR-RBR), Nature-inspired algorithms (Genetic algorithm, Particle Swarm Optimization, and Ant Colony Optimization), and hybrid approaches that combine these algorithms. The primary objective of these intelligent techniques is to address the day-to-day challenges faced by rural and smart digital societies. In this study, the authors extensively investigated various intelligent computing methods (ICMs) specifically relevant to distinct problems, providing accurate and reasonable knowledge-based solutions. The application of both single ICMs and combined ICMs was explored to solve domain-specific problems, and their effectiveness was analyzed and discussed. The results indicated that combined ICMs exhibited superior efficiency compared to single ICMs. Furthermore, the authors conducted an analysis and comparison of ICMs based on their application domain, parameters, methods/algorithms, efficiency, and acceptable outcomes. Additionally, the authors identified several problem scenarios that can be effectively resolved using intelligent techniques. },
  issn={ 0219-3116 },
}

@article{Chen_2023,
  title={ High spatial resolution remote sensing image segmentation based on the multiclassification model and the binary classification model },
  volume={ 35 },
  DOI={ doi:10.1007/s00521-020-05561-8 },
  journal={ Neural Computing and Applications },
  author={ Xiaoxiong ZhengTao Chen },
  year={ 2023 },
  pages={ 3597-3604 },
  month={ 01 },
  abstract={ Semantic segmentation technology is an important step in the interpretation of remote sensing images. High spatial resolution remote sensing images have clear features. Traditional image segmentation methods cannot fully represent the information in high spatial resolution images and tend to yield unsatisfactory segmentation accuracy. With the rapid development of deep learning, many researchers have tried to use deep learning algorithms for remote sensing image segmentation. This paper uses U-Net for multiclassification and binary classification of Gaofen-2 high spatial resolution remote sensing image data. Six types of features, which were build-up, farmland, water, meadow, forest and others, were labeled in the image. A “neighborhood voting” method was used to determine the category of uncertain pixels based on spatial heterogeneity and homogeneity. Through U-Net neural network multiclassification, the overall accuracy of the training data is 93.83%; the overall accuracy of the test data is 82.27%; and the test accuracy of the binary classification algorithm is 79.75%. The results show that the two models yield high accuracy and credibility in remote sensing image segmentation. },
  issn={ 1433-3058 },
}

@article{Lafifi_2024,
  title={ emoLearnAdapt: A new approach for an emotion-based adaptation in e-learning environments },
  volume={ 29 },
  DOI={ doi:10.1007/s10639-023-12429-6 },
  journal={ Education and Information Technologies },
  author={ Adil BoughidaMohamed Nadjib KouahlaYacine Lafifi },
  year={ 2024 },
  pages={ 15269-15323 },
  month={ 01 },
  abstract={ In e-learning environments, most adaptive systems do not consider the learner's emotional state when recommending activities for learning difficulties, blockages, or demotivation. In this paper, we propose a new approach of emotion-based adaptation in e-learning environments. The system will allow recommendation resources/activities to motivate and support the learner in learning. Our first contribution is modeling the learner's emotion by exploiting the facial expressions generated during the pedagogical activities. For this purpose, a probability-based emotion quantification algorithm has been proposed. To recommend support resources, we presented our adaptation approach that leverages a set of adopted adaptation criteria, where the weighting of these criteria differs from one support resource to another. Five experiments aimed at validating the approach were conducted on two groups of students (test and control groups). The results show our approach's impact on improving the learner's cognitive level, engagement time, and motivation. },
  issn={ 1573-7608 },
}

@article{Hua_2024,
  title={ Cognition2Vocation: meta-learning via ConvNets and continuous transformers },
  volume={ 36 },
  DOI={ doi:10.1007/s00521-024-09749-0 },
  journal={ Neural Computing and Applications },
  author={ Sara KamranSaeid HosseiniSayna EsmailzadehMohammad Reza KangavariWen Hua },
  year={ 2024 },
  pages={ 12935-12950 },
  month={ 04 },
  abstract={ Estimating the suitability of individuals for a vocation via leveraging the knowledge within cognitive factors comes with numerous applications: employment resourcing, occupation counseling, and workload management. Accordingly, the enterprises aim to hire the most suitable person from a massive array of similar applicants, maximizing performance and minimizing the gap between strategic indicators and predefined targets. While cognitive factors signify the best-suited person from similarly skilled workers, inferring pertinent latent cues from noisy and growing social network contents is time-intensive. To tackle the challenges involved, we propose a framework that, on the one hand, extends influential features based on the correlations between cognitive cues and, on the other hand, leverages a novel continuous transformer to mitigate the overlapping and approximation issues in discrete modeling. Rather than relying on discrete patterns that may evolve frequently, we use continuous elements that include not only numerous aggregating components but also sense minor irregular fluctuations. In a hybrid manner, we fuse multiple base models to transfer a higher representation to the meta-learning unit, agglomerating outputs from gradient boosters and the ConvNets. The experimental results show that our proposed framework can outperform trending vocation estimation methods by 1.36% in F1-Score and approximately 1% in accuracy. },
  issn={ 1433-3058 },
}

@article{Shen_2024,
  title={ Research on factors influencing the consumer repurchase intention: Data mining of consumers’ online reviews based on machine learning },
  volume={ 36 },
  DOI={ doi:10.1007/s00521-024-09591-4 },
  journal={ Neural Computing and Applications },
  author={ Jianming ZhangHao ZhengJie LiuWei Shen },
  year={ 2024 },
  pages={ 9837-9848 },
  month={ 03 },
  abstract={ The fierce competition in the market makes it necessary for enterprises to not only consider how to increase consumers’ purchase intention but also study to maintain high customer loyalty for continuous purchases. Taking the smartphone brands on the Jingdong platform (hereafter referred to as JD) as an example, the study collected 60,000 review data and using NLP technology for data mining, factors that may affect consumers’ willingness to repurchase were extracted. Based on Theory of Reasoned Action (TRA), the questionnaire was made for empirical research. The results showed that the four factors, product attributes, service quality, brand image and price significantly affect consumers’ repurchase intention, while service quality had the strongest effect among them, implications of the research are discussed. },
  issn={ 1433-3058 },
}

@article{Jindal_2024,
  title={ Emerging artificial intelligence applications: metaverse, IoT, cybersecurity, healthcare - an overview },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-17890-6 },
  journal={ Multimedia Tools and Applications },
  author={ Neha SharmaNeeru Jindal },
  year={ 2024 },
  pages={ 57317-57345 },
  month={ 12 },
  abstract={ The term "artificial intelligence" (AI) refers to "smart" high-tech that is mindful of and able to learn from its surroundings. It is the most revolutionary technology that humans have ever created. Common AI approaches involving machine learning and deep learning techniques can be effectively applied to resolve today's various cybersecurity issues. Furthermore, the metaverse is all about how people communicate and engage with one another through technology. This survey explores the role of AI with its emerging applications and their various technologies, such as the metaverse, healthcare, IoT, gaming, and many more. To determine the strengths, flaws, opportunities, and risks that are inherent in artificial intelligence technologies, using an extensive literature survey, the SWOT (Strengths, Weaknesses, Opportunities, and Threats) assessments have been undertaken in this survey paper. Finally, the survey paper summarises the current state of knowledge of AI applications and discusses the findings present in recent research to ensure a favourable change in artificial intelligence advances and applications. Some technical AI challenges, like high-speed, high-performance hardware and reducing the amount of training data, etc., are also discussed with future scope. },
  issn={ 1573-7721 },
}

@article{Sarkar_2024,
  title={ Automated quantum software engineering },
  volume={ 31 },
  DOI={ doi:10.1007/s10515-024-00436-x },
  journal={ Automated Software Engineering },
  author={ Aritra Sarkar },
  year={ 2024 },
  pages={ 1-17 },
  month={ 04 },
  abstract={ As bigger quantum processors with hundreds of qubits become increasingly available, the potential for quantum computing to solve problems intractable for classical computers is becoming more tangible. Designing efficient quantum algorithms and software in tandem is key to achieving quantum advantage. Quantum software engineering is challenging due to the unique counterintuitive nature of quantum logic. Moreover, with larger quantum systems, traditional programming using quantum assembly language and qubit-level reasoning is becoming infeasible. Automated Quantum Software Engineering&nbsp;(AQSE) can help to reduce the barrier to entry, speed up development, reduce errors, and improve the efficiency of quantum software. This article elucidates the motivation to research AQSE (why), a precise description of such a framework (what), and reflections on components that are required for implementing it (how). },
  issn={ 1573-7535 },
}

@article{Azam_2024,
  title={ Behavior Based Group Recommendation from Social Media Dataset by Using Deep Learning and Topic Modeling },
  volume={ 5 },
  DOI={ doi:10.1007/s42979-024-03055-1 },
  journal={ SN Computer Science },
  author={ Md. Saddam Hossain MuktaJubaer AhmedMohaimenul Azam Khan RaiaanNur Mohammad FahadMuhammad Nazrul IslamNafiz ImtiazMd. Adnanul IslamMohammed Eunus AliSami Azam },
  year={ 2024 },
  pages={ 1-17 },
  month={ 07 },
  abstract={ In this digital era, users frequently share their thoughts, preferences, and ideas through social media, which reflect their Basic Human Values. Basic Human Values (aka values) are the fundamental aspects of human behavior, which define what we consider important, and worth having and pursue them. Existing studies identify the values of individuals from different social network usages such as Facebook and Reddit. However, discovering the similarity (or diversity) of value priorities among the members in a group is important since we can reveal many interesting insights such as finding a set of target customers, identifying the chain of misdeed groups, searching for similar acquaintances in workplaces, etc. In this paper, a graph dataset is compiled using the strongest correlation among the features and then we apply a graph clustering technique to identify a suitable hedonist group (i.e., one dimension of values) for users’ recommendations. Then, we also propose a behavior based (i.e., value ) group recommendation technique by analyzing users’ contextual and psychological attributes. Finally, we validate those group members in real life by introducing two hypotheses. In particular, we analyze the tweets of a total of 1140 users collected from Twitter. We obtain a substantial intra-cluster correlation coefficient (ICC) and silhouette clustering coefficient (SCC) scores of 65% and 76%, respectively, among the members in our discovered group. },
  issn={ 2661-8907 },
}

@article{Mollevi_2024,
  title={ A SHAP-based controversy analysis through communities on Twitter },
  volume={ 27 },
  DOI={ doi:10.1007/s11280-024-01278-z },
  journal={ World Wide Web },
  author={ Samy BenslimaneThomas PapastergiouJérôme AzéSandra BringayMaximilien ServajeanCaroline Mollevi },
  year={ 2024 },
  pages={ 1-29 },
  month={ 09 },
  abstract={ Controversy encompasses content that draws diverse perspectives, along with positive and negative feedback on a specific event, resulting in the formation of distinct user communities. we explore the explainability of controversy through the lens of SHAP (SHapley Additive exPlanations) method, aiming to provide a fair assessment of the individual contributions of different text features of tweets to controversy detection. We conduct an analysis of topic discussions on Twitter from a community perspective, investigating the role of text in accurately classifying tweets into their respective communities. To achieve this, we introduce a SHAP-based pipeline designed to quantify the influence of impactful text features on the predictions of three tweet classifiers. Text content alone offers interesting controversy detection accuracy. It can contain predictive features for controversy detection. For instance, negative connotations, pejorative tendencies and positive qualifying adjectives tend to impact the controversy model detection. },
  issn={ 1573-1413 },
}

@article{Rawat_2023,
  title={ A smart Alzheimer’s patient monitoring system with IoT-assisted technology through enhanced deep learning approach },
  volume={ 65 },
  DOI={ doi:10.1007/s10115-023-01890-x },
  journal={ Knowledge and Information Systems },
  author={ Rajesh ArunachalamGurram SunithaSurendra Kumar ShuklaSurya Nath pandeyShabana UroojSeema Rawat },
  year={ 2023 },
  pages={ 5561-5599 },
  month={ 08 },
  abstract={ Earlier detection of Alzheimer’s disease is more significant for improving the quality of the patient’s life. This aspect may reduce the fatality rate among the population and also maximize the average life expectancy. Thus, this paper introduces a new Alzheimer's prediction model using IoT and deep structured architectures. A new smart Alzheimer’s patient monitoring system is developed by processing healthcare data using IoT devices. Initially, Alzheimer’s patients are detected from the set of patients using “enhanced deep residual network–long short-term memory (DRN-LSTM).” Here, the detection process is done with the data associated with the patients. The optimal feature selection phase and enhanced deep convolutional network (DCN) and deep residual network (DRN)-based detection are accomplished by parameter-improved horse herd optimization algorithm (PI-HHO). The monitored data involve audio, data, and video from the sensors based on the location and movements of patients. Next, the gathered data are forwarded to the optimal feature selection with the same algorithm and predicted the abnormalities through enhanced DNN + LSTM using PI-HHO. Thirdly, the abnormal patients are alerted to the nearby hospital for appropriate treatment and monitoring. All through the result evaluation, the accuracy and precision rate of the recommended Alzheimer’s patient monitoring system attain 98% and 97%. Thus, this smart patient prediction model ensures the high-quality results in terms of standard performance metrics while evaluating with other algorithms. },
  issn={ 0219-3116 },
}

@article{Askounis_2024,
  title={ Depression detection in social media posts using transformer-based models and auxiliary features },
  volume={ 14 },
  DOI={ doi:10.1007/s13278-024-01360-4 },
  journal={ Social Network Analysis and Mining },
  author={ Marios KerasiotisLoukas IliasDimitris Askounis },
  year={ 2024 },
  pages={ 1-20 },
  month={ 09 },
  abstract={ The detection of depression in social media posts is crucial due to the increasing prevalence of mental health issues. Traditional machine learning algorithms often fail to capture intricate textual patterns, limiting their effectiveness in identifying depression. Existing studies have explored various approaches to this problem but often fall short in terms of accuracy and robustness. To address these limitations, this research proposes a neural network architecture leveraging transformer-based models combined with metadata and linguistic markers. The study employs DistilBERT, extracting information from the last four layers of the transformer, applying learned weights, and averaging them to create a rich representation of the input text. This representation, augmented by metadata and linguistic markers, enhances the model’s comprehension of each post. Dropout layers prevent overfitting, and a Multilayer Perceptron (MLP) is used for final classification. Data augmentation techniques, inspired by the Easy Data Augmentation (EDA) methods, are also employed to improve model performance. Using BERT, random insertion and substitution of phrases generate additional training data, focusing on balancing the dataset by augmenting underrepresented classes. The proposed model achieves weighted Precision, Recall, and F1-scores of 84.26%, 84.18%, and 84.15%, respectively. The augmentation techniques significantly enhance model performance, increasing the weighted F1-score from 72.59% to 84.15%. },
  issn={ 1869-5469 },
}

@article{Buchberger_2023,
  title={ Automated programming, symbolic computation, machine learning: my personal view },
  volume={ 91 },
  DOI={ doi:10.1007/s10472-023-09894-7 },
  journal={ Annals of Mathematics and Artificial Intelligence },
  author={ Bruno Buchberger },
  year={ 2023 },
  pages={ 569-589 },
  month={ 10 },
  abstract={ In this note, I present my personal view on the interaction of the three areas Automated Programming, Symbolic Computation, and Machine Learning. Programming is the activity of finding a (hopefully) correct program (algorithm) for a given problem. Programming is central to automation in all areas and is considered one of the most creative human activities. However, already very early in the history of programming, people started to “jump to the meta-level” of programming, i.e., started to develop procedures that automate, or semi-automate, (various aspects or parts of) the process of programming. This area has various names like “Automated Programming”, “Automated Algorithm Synthesis”, etc. Developing compilers can be considered an early example of a problem in automated programming. Automated reasoners for proving the correctness of programs with respect to a specification is an advanced example of a topic in automated programming. ChatGPT producing (amazingly good) programs from problem specifications in natural language is a recent example of automated programming. Programming tends to become the most important activity as the level of technological sophistication increases. Therefore, automating programming is maybe the most exciting and relevant technological endeavor today. It also will have enormous impact on the global job market in the software industry. Roughly, I see two main approaches to automated programming: In this note, I explain how the two approaches work and that they are fundamentally different because they address two completely different ways of how problems are specified. Together, the two approaches constitute (part of) what some people like to call “artificial intelligence”. In my analysis, both approaches are just part of (algorithmic) mathematics. The approaches, like all non-trivial mathematical methods, need quite some intelligence on the side of the human inventors of the methods. However, applying the methods is just “machine execution” of algorithms. It is misleading to call the application “machine intelligence” or “artificial intelligence”. The analysis of the two approaches to automated programming also suggests that the two approaches, in the future, should be combined to achieve even higher levels of sophistication. At the end of this note, I propose some research questions for this new direction. },
  issn={ 1573-7470 },
}

@article{Pereira_2024,
  title={ Turning manual web accessibility success criteria into automatic: an LLM-based approach },
  DOI={ doi:10.1007/s10209-024-01108-z },
  journal={ Universal Access in the Information Society },
  author={ Juan-Miguel López-GilJuanan Pereira },
  year={ 2024 },
  pages={ 1-16 },
  month={ 03 },
  abstract={ Web accessibility evaluation is a costly process that usually requires manual intervention. Currently, large language model (LLM) based systems have gained popularity and shown promising capabilities to perform tasks that seemed impossible or required programming knowledge specific to a given area or were supposed to be impossible to be performed automatically. Our research explores whether an LLM-based system would be able to evaluate web accessibility success criteria that require manual evaluation. Three specific success criteria of the Web Content Accessibility Guidelines (WCAG) that currently require manual checks were tested: 1.1.1 Non-text Content, 2.4.4 Link Purpose (In Context), and 3.1.2 Language of Parts. LLM-based scripts were developed to evaluate the test cases. Results were compared against current web accessibility evaluators. While automated accessibility evaluators were unable to reliably test the three WCAG criteria, often missing or only warning about issues, the LLM-based scripts successfully identified accessibility issues the tools missed, achieving overall 87.18% detection across the test cases. Conclusion The results demonstrate LLMs can augment automated accessibility testing to catch issues that pure software testing misses today. Further research should expand evaluation across more test cases and types of content. },
  issn={ 1615-5297 },
}

@article{Muhammad_2024,
  title={ Person Re-identification with Spatial Multi-granularity Feature Exploration for Social Risk Situational Assessment },
  volume={ 16 },
  DOI={ doi:10.1007/s12559-024-10249-5 },
  journal={ Cognitive Computation },
  author={ Mingfu XiongHanmei ChenYi WenAbdul Khader Jilani SaudagarJavier Del SerKhan Muhammad },
  year={ 2024 },
  pages={ 2701-2712 },
  month={ 01 },
  abstract={ Recently, the “human-oriented” concept of security development has become a consensus among all countries. This depends mainly on intelligent surveillance systems that can support person re-identification (Re-ID) technology to empower social risk situational assessment applications. However, existing Re-ID methods mainly focus on single and fixed convolutional operations for feature extraction, ignoring the multi-dimensional spatial association of the human body, which limits the performance of Re-ID. Human cognition when identifying people does not solely rely on visual cues of the individual in sight, but also on his/her behavioral and gestural characteristics. To solve this issue and inspired by the aforementioned cognitive mechanism of the human brain, this study developed a spatial multi-granularity feature exploration (SMGFE) model for person Re-ID. The proposed SMGFE model comprises two main steps: (i) a multi-granularity feature exploration strategy and (ii) a human spatial association scheme. The former mainly includes coarse (original person images), medium (multi-regional divided person images), and fine-tuned (keypoints of the human body) level features, which form the multi-granularity feature representation. An undirected graph model was then developed to construct multi-dimensional spatial relations for each person. Finally, the unified optimization strategy was applied to train the framework to achieve promising accuracy. We evaluated the proposed algorithm on frequently used and benchmark person Re-ID datasets (Market-1501 and DukeMTMC-reID). The cumulative match curve (CMC) and mean average precision (mAP), which are the common measuring criteria for most person Re-ID methods reported to date, were used to verify the experimental results. Experiments show that our proposed algorithm achieved unrivaled performance levels. In addition, based on the spatial multi-granularity feature exploration strategy, the time efficiency of the proposed method for detecting specific instances can reach O(n), making it suitable for deployment in low-resource terminals for security risk assessment, including Android/iOS analysis servers, urban safety risk surveillance systems, and warning platforms for situational awareness. },
  issn={ 1866-9964 },
}

@article{Ahmad_2023,
  title={ A framework for real-time dress classification in cluttered background images for robust image retrieval },
  volume={ 25 },
  DOI={ doi:10.1007/s10111-023-00735-5 },
  journal={ Cognition, Technology & Work },
  author={ Mudasir DilawarYasir SaleemIkram SyedTauqir Ahmad },
  year={ 2023 },
  pages={ 373-384 },
  month={ 08 },
  abstract={ The dress of a person can provide information like culture, status, gender, and personality. Dress classification can help us improve e-commerce, image-based search engines for the fashion industry, video surveillance, and social media image categorization. Deep learning is emerging as one of the most powerful classification techniques in many fields like medicine, business, and science. In the fashion industry, Convolutional Neural Networks have played a vital role in dress identification classification, but it is still a difficult task due to cluttered backgrounds, different poses, and lack of fashion datasets with rich classes and annotations. To complement the small-sized datasets, transfer learning is being widely used in training deep learning models. Current research applies transfer learning in two steps on the InceptionV3 model pre-trained on the ImageNet dataset. First, the pre-trained model is fine-tuned on DeepFashion dataset to transfer the domain of learned parameters toward fashion. In the second step, the model is fine-tuned on Pak Dataset a collection of Asian cultural fashion images having cluttered backgrounds. Experiments show the robustness and usefulness of two-step transfer learning in the classification of fashion images having cluttered backgrounds. Dress classification can be used for fashion image retrieval systems and recommendation systems. Dress classification can also be used in video surveillance systems for finding missing persons or crime suspects. },
  issn={ 1435-5566 },
}

@article{周婷婷_2024,
  title={ Iris: a multi-constraint graphic layout generation system },
  volume={ 25 },
  DOI={ doi:10.1631/FITEE.2300312 },
  journal={ Frontiers of Information Technology & Electronic Engineering },
  author={ Liuqing Chen 陈柳青Qianzhi Jing 景千芝Yixin Tsang 曾怡欣Tingting Zhou 周婷婷 },
  year={ 2024 },
  pages={ 968-987 },
  month={ 07 },
  abstract={ In graphic design, layout is a result of the interaction between the design elements in the foreground and background images. However, prevalent research focuses on enhancing the quality of layout generation algorithms, overlooking the interaction and controllability that are essential for designers when applying these methods in real-world situations. This paper proposes a user-centered layout design system, Iris, which provides designers with an interactive environment to expedite the workflow, and this environment encompasses the features of user-constraint specification, layout generation, custom editing, and final rendering. To satisfy the multiple constraints specified by designers, we introduce a novel generation model, multi-constraint LayoutVQ-VAE, for advancing layout generation under intra- and inter-domain constraints. Qualitative and quantitative experiments on our proposed model indicate that it outperforms or is comparable to prevalent state-of-the-art models in multiple aspects. User studies on Iris further demonstrate that the system significantly enhances design efficiency while achieving human-like layout designs. },
  issn={ 2095-9230 },
}

@article{Wang_2023d,
  title={ Superiority of three-way decisions from the perspective of probability },
  volume={ 56 },
  DOI={ doi:10.1007/s10462-022-10203-7 },
  journal={ Artificial Intelligence Review },
  author={ Longjun YinQinghua ZhangFan ZhaoDun LiuGuoyin Wang },
  year={ 2023 },
  pages={ 1263-1295 },
  month={ 05 },
  abstract={ Three-way decisions (3WDs) is a typical method for dealing with uncertain issues. It is essentially an extension of two-way decisions (2WDs). What is the superiority of 3WDs/S3WDs over traditional 2WDs? Only a few studies have analyzed the theoretical superiority of 3WDs over traditional 2WDs. The motivation of this paper is to theoretically analyze the superiority of 3WDs over 2WDs in dealing with classification problems. From the perspective of probability, 3WDs is compared with 2WDs for a thorough discussion and analysis of the superiority of 3WDs in this paper. First, it is proved that increasing information can effectively improve classification precision in terms of the sample mean, misclassification probability, and interval of uncertain classification. Second, a novel 3WDs model/sequential three-way decisions model (3WDM-CLSM/S3WDM-CLSM) based on confidence level and sample mean is proposed, and the corresponding method for calculating the pair of thresholds based on the confidence level is presented. Third, the superiority of 3WDs compared to 2WDs is analyzed in terms of classification precision and cost of information acquisition (CIA). Finally, experiments are completed to show that the 3WDs model can effectively reduce the CIA, and its classification accuracy is close to that of the 2WDs model. },
  issn={ 1573-7462 },
}

@article{Hu_2024,
  title={ Few-shot defect detection using feature enhancement and image generation for manufacturing quality inspection },
  volume={ 54 },
  DOI={ doi:10.1007/s10489-023-05199-8 },
  journal={ Applied Intelligence },
  author={ Yu GongMingzhou LiuXiaoqiao WangConghu LiuJing Hu },
  year={ 2024 },
  pages={ 375-397 },
  month={ 12 },
  abstract={ Visual defect detection, which is pivotal in industrial quality control, often requires extensive datasets for training deep-learning models. However, in industrial environments, the presence of multiple production batches, small lot sizes, and rapidly evolving task requirements make it challenging to acquire sufficient and diverse defect data. To address these challenges, this study introduces an innovative approach to data augmentation and defect detection under few-data conditions. Our strategy employs a two-stage feature-enhancement method complemented by an expert knowledge-based image-generation method. The former ensures robustness in color and richness in feature representation by focusing on local defect irregularities. The latter leverages both the core mechanistic understanding of experts and nonquantitative empirical insights, combined with a variable background domain, to generate rich images with diverse defect features. We evaluated the feasibility and effectiveness of the proposed method using a proprietary dataset curated by an automotive component manufacturing enterprise. The experimental results revealed that, across the five defect categories, training on 2 images achieved an F1-Score of 41.06%, which improved to 67.11% when the training dataset was expanded to 15 images. A comparative analysis with prevailing feature enhancement and image generation methods revealed our solution’s superiority, bridging methodological drawbacks, and markedly eclipsing others in terms of enhancement potency. Furthermore, when benchmarked against additional public industrial datasets, the model exhibited exceptional adaptability and superior performance, thus highlighting its potential as a robust tool for quality control in manufacturing scenarios. },
  issn={ 1573-7497 },
}

@article{Pedreschi_2024,
  title={ Explaining Siamese networks in few-shot learning },
  volume={ 113 },
  DOI={ doi:10.1007/s10994-024-06529-8 },
  journal={ Machine Learning },
  author={ Andrea FedeleRiccardo GuidottiDino Pedreschi },
  year={ 2024 },
  pages={ 7723-7760 },
  month={ 04 },
  abstract={ Machine learning models often struggle to generalize accurately when tested on new class distributions that were not present in their training data. This is a significant challenge for real-world applications that require quick adaptation without the need for retraining. To address this issue, few-shot learning frameworks, which includes models such as Siamese Networks, have been proposed. Siamese Networks learn similarity between pairs of records through a metric that can be easily extended to new, unseen classes. However, these systems lack interpretability, which can hinder their use in certain applications. To address this, we propose a data-agnostic method to explain the outcomes of Siamese Networks in the context of few-shot learning. Our explanation method is based on a post-hoc perturbation-based procedure that evaluates the contribution of individual input features to the final outcome. As such, it falls under the category of post-hoc explanation methods. We present two variants, one that considers each input feature independently, and another that evaluates the interplay between features. Additionally, we propose two perturbation procedures to evaluate feature contributions. Qualitative and quantitative results demonstrate that our method is able to identify highly discriminant intra-class and inter-class characteristics, as well as predictive behaviors that lead to misclassification by relying on incorrect features. },
  issn={ 1573-0565 },
}

@article{Ding_2023,
  title={ The impact of media diversity and cognitive style on learning experience in programming video lecture: A brainwave analysis },
  volume={ 28 },
  DOI={ doi:10.1007/s10639-023-11608-9 },
  journal={ Education and Information Technologies },
  author={ Xuefen LinWei TangWeifeng MaYang LiuFeng Ding },
  year={ 2023 },
  pages={ 10617-10637 },
  month={ 01 },
  abstract={ The use of video lectures has become a core feature of digital learning, but how the media diversity carried in videos affects learning experience has been rarely studied. Adopting a two-factor experimental design, this study used cognitive style questionnaires, brain wave detection, cognitive load scale, and post-test to explore the impacts of three commonly used algorithm-based video lectures on the sustained attention, learning engagement, cognitive load, and learning outcomes of verbal and visual style learners. The results show that cognitive style and video lecture type had a small effect on learners’ sustained attention and learning engagement levels; and visual learners demonstrated significantly higher attention and learning engagement levels in the animation group than in the Tablet drawing and PPT groups. Similarly, with the increase of media diversity, cognitive load also increased, but the increase did not reach a significant level. The study also found that media diversity had a small impact on learning outcomes, but cognitive style not. It further proved that cognitive load caused by moderate media diversity didn’t affect learning outcomes. This research provides a valuable reference for creating effective video lectures and significant support for researching video courses from neuroscience perspective. },
  issn={ 1573-7608 },
}

@article{P._2024,
  title={ NeuroInteract: An Innovative Deep Learning Strategy for Effective Drug Repositioning in Schizophrenia Therapy },
  volume={ 17 },
  DOI={ doi:10.1007/s12559-024-10384-z },
  journal={ Cognitive Computation },
  author={ Sherine Glory J.Durgadevi P.Ezhumalai P. },
  year={ 2024 },
  pages={ 1-24 },
  month={ 12 },
  abstract={ Schizophrenia (SCZ) is a serious physiological and neurological disorder that affects an individual’s perception of factuality. It expresses different symptoms such as thinking, aberrant behavior, delusions, and hallucinations. An efficient approach for inferring potential indications for drugs is through drug repositioning. In this context, drug repositioning imparts a valuable strategy to gain safer, faster, and potentially efficient treatment options to improve schizophrenia therapy. Current treatments are insufficient and existing drug repositioning methods are unsuccessful in solving the drug-disease interactions’ difficulties, including long-term efficacy, drug synergy, and capturing genetic variations. Also, existing methods are restrained because of the incapacity to efficiently integrate heterogeneous biomedical data, which results in suboptimal predictions. This research introduces a NeuroInteract model using deep learning in order to predict candidate drugs for SCZ therapy. The proposed model enhances the accuracy of drug repositioning through the collection of various data sources such as genetic information and drug-disease associations. The novelty of the proposed model is the utilization of the heterogeneous data network that is integrated with the progressive optimization model for the purpose of improving prediction accuracy. The developed method imparts effective learning from various data characteristics through the integration of various types of neural network layers such as fully connected layers, convolutional layers, recurrent layers, and graph convolutional layers. The collected data from DrugBank 5.0 and repoDB undergoes a process of data integration, which aids in generating precise predictions for candidate drugs for repositioning. A data pre-processing technique is employed to improve the data quality. After data pre-processing, the proposed method effectively extracts the meaningful features and finds the spatial dependencies to predict the potential candidate drugs for SCZ treatment. Also, it efficiently handles sequential dependencies and genetic information. The oppositional crossover boosted meerkat optimization (OCMO) algorithm is deployed to optimize the performance of the model. The OCMO optimizes the learning process and enhances the model accuracy by dynamically adjusting its search strategy. Ultimately, comprehensive experimental analyses are conducted using several estimation parameters. The proposed method gains greater effectiveness and outperforms existing methods in drug repositioning. The developed method reaches an accuracy of 98.84% and a hit rate of 98.76%. These experimental findings ascertain the ability of NeuroInteract to find promising drugs for repurposing, furnishing a robust and more cost-effective model for SCZ treatment. },
  issn={ 1866-9964 },
}

@article{Li_2023a,
  title={ Empirically revisiting and enhancing automatic classification of bug and non-bug issues },
  volume={ 18 },
  DOI={ doi:10.1007/s11704-023-2771-z },
  journal={ Frontiers of Computer Science },
  author={ Zhong LiMinxue PanYu PeiTian ZhangLinzhang WangXuandong Li },
  year={ 2023 },
  pages={ 1-20 },
  month={ 12 },
  abstract={ A large body of research effort has been dedicated to automated issue classification for Issue Tracking Systems (ITSs). Although the existing approaches have shown promising performance, the different design choices, including the different textual fields, feature representation methods and machine learning algorithms adopted by existing approaches, have not been comprehensively compared and analyzed. To fill this gap, we perform the first extensive study of automated issue classification on 9 state-of-the-art issue classification approaches. Our experimental results on the widely studied dataset reveal multiple practical guidelines for automated issue classification, including: (1) Training separate models for the issue titles and descriptions and then combining these two models tend to achieve better performance for issue classification; (2) Word embedding with Long Short-Term Memory (LSTM) can better extract features from the textual fields in the issues, and hence, lead to better issue classification models; (3) There exist certain terms in the textual fields that are helpful for building more discriminating classifiers between bug and non-bug issues; (4) The performance of the issue classification model is not sensitive to the choices of ML algorithms. Based on our study outcomes, we further propose an advanced issue classification approach, DeepLabel, which can achieve better performance compared with the existing issue classification approaches. },
  issn={ 2095-2236 },
}

@article{Berkovsky_2023,
  title={ Eye-tracking-based personality prediction with recommendation interfaces },
  volume={ 33 },
  DOI={ doi:10.1007/s11257-022-09336-9 },
  journal={ User Modeling and User-Adapted Interaction },
  author={ Li ChenWanling CaiDongning YanShlomo Berkovsky },
  year={ 2023 },
  pages={ 121-157 },
  month={ 06 },
  abstract={ Recent research in behavioral decision making demonstrates the advantages of using eye-tracking to surface insights into users’ underlying cognitive processes. Personality, according to psychology definition, accounts for individual differences in our enduring emotional, interpersonal, experiential, attitudinal, and motivational styles. In recommender systems (RS), it has been found that user personality is related to their preferences and behavior, which attracted an increasing attention to the ways to leverage personality into the recommendation process. However, accurate acquisition of a user’s personality is still a challenging issue. In this work, we investigate the possibility of automatically detecting personality from users’ eye movements when interacting with a recommendation interface. Specifically, we report an experiment that harnesses two recommendation interfaces to collect eye-movement data in several product domains and then utilize the data to predict the users’ Big-Five personality traits through various machine learning methods. The results show that AdaBoost combined with Gini index score-based feature selector predicts the traits most accurately, and interface- and domain-specific data allow to improve the accuracy of personality trait predictions. Our findings could inform personality-based RS by improving the process of indirect user personality acquisition. },
  issn={ 1573-1391 },
}

@article{Gašpar_2024,
  title={ Twenty-five years of Bayesian knowledge tracing: a systematic review },
  volume={ 34 },
  DOI={ doi:10.1007/s11257-023-09389-4 },
  journal={ User Modeling and User-Adapted Interaction },
  author={ Ines Šarić-GrgićAni GrubišićAngelina Gašpar },
  year={ 2024 },
  pages={ 1127-1173 },
  month={ 01 },
  abstract={ The quality of an artificial intelligence-based tutoring system is its ability to observe and interpret student behaviour to infer the preferences and needs of an individual student. The student model enables a comprehensive representation of student knowledge and affects the quality of the other intelligent tutoring system’s (ITS) components. The Bayesian knowledge tracing model (BKT) is one of the first machine learning-based and widely investigated student models due to its interpretability and ability to infer student knowledge. The past Twenty-five Years have seen increasingly rapid advances in the field, so this systematic review deals with the BKT model enhancements by using the PRISMA guidelines and a unique set of criteria, including 13 aspects of enhancements and computational methods. Also, the study reveals two types of evaluation approaches found in the literature, including the prediction of student answers and the ability to estimate knowledge mastery. Overall, the most frequently investigated enhancements extended the vanilla BKT model by including student characteristics and tutor interventions. The educational context-based enhancements of domain knowledge properties, question difficulty and architectural prior knowledge were also frequently investigated enhancements. The expectation–maximization algorithm practically became the standard in estimating BKT parameters. While the enhanced BKT models generally overperformed the vanilla model in predicting the student answer by using the measures such as RMSE (root mean square error), AUC–ROC (area under curve, receiver operating characteristics curve) and accuracy, only a few studies further investigated the systems’ estimations of knowledge mastery by correlating it to knowledge on post-tests. The most frequently used educational platforms included ITSs, Massive Open Online Courses (MOOCs) and simulated environments. },
  issn={ 1573-1391 },
}

@article{Torres_2024,
  title={ Applying recommendation system for developing programming competencies in children from a non-weird context },
  volume={ 29 },
  DOI={ doi:10.1007/s10639-023-12156-y },
  journal={ Education and Information Technologies },
  author={ Jesennia Cárdenas-CoboCristian Vidal-SilvaLisett ArévaloMagali Torres },
  year={ 2024 },
  pages={ 9355-9386 },
  month={ 09 },
  abstract={ The information society is part of current life, and algorithmic thinking and programming are relevant for everybody regardless of educational background. Today’s world needs professionals with computing competencies from WEIRD (Western, Educated, Industrialized, Rich, and Democratic Societies) and non-WEIRD contexts. Traditional programming languages include syntax barriers that complicate their overall adoption and usefulness for people from a non-WEIRD context. To solve it, block-based programming languages like Scratch permit the development of programming competencies without syntax restrictions in online environments. This article presents empirical evidence of the positive impact of Scratch with the CARAMBA recommendation system for lessons and exercises proposals based on collaborative filtering of personalized learning from students’ experiences. Previous experiences demonstrated that students require assistance in successfully defining sub-competencies and exercises to develop programming competencies by applying Scratch. This work shows the application of Scratch and CARAMBA in a non-WEIRD school context for developing programming competencies. Obtained results show that developing exercises with Scratch and CARAMBA motivated students’ autonomy, and as well, the programming learning application increased exam scores in all the analyzed grades. Those results encourage us to continue using Scratch and CARAMBA for developing programming competencies in similar non-WEIRD contexts. },
  issn={ 1573-7608 },
}

@article{Tetzlaff_2023,
  title={ Organization and Priming of Long-term Memory Representations with Two-phase Plasticity },
  volume={ 15 },
  DOI={ doi:10.1007/s12559-022-10021-7 },
  journal={ Cognitive Computation },
  author={ Jannik LuboeinskiChristian Tetzlaff },
  year={ 2023 },
  pages={ 1211-1230 },
  month={ 06 },
  abstract={ In recurrent neural networks in the brain, memories are represented by so-called Hebbian cell assemblies. Such assemblies are groups of neurons with particularly strong synaptic connections formed by synaptic plasticity and consolidated by synaptic tagging and capture (STC). To link these synaptic mechanisms to long-term memory on the level of cognition and behavior, their functional implications on the level of neural networks have to be understood. We employ a biologically detailed recurrent network of spiking neurons featuring synaptic plasticity and STC to model the learning and consolidation of long-term memory representations. Using this, we investigate the effects of different organizational paradigms, and of priming stimulation, on the functionality of multiple memory representations. We quantify these effects by the spontaneous activation of memory representations driven by background noise. We find that the learning order of the memory representations significantly biases the likelihood of activation towards more recently learned representations, and that hub-like overlap structure counters this effect. We identify long-term depression as the mechanism underlying these findings. Finally, we demonstrate that STC has functional consequences for the interaction of long-term memory representations: 1. intermediate consolidation in between learning the individual representations strongly alters the previously described effects, and 2. STC enables the priming of a long-term memory representation on a timescale of minutes to hours. Our findings show how synaptic and neuronal mechanisms can provide an explanatory basis for known cognitive effects. },
  issn={ 1866-9964 },
}

@article{Penichet_2024,
  title={ The data dance: choreographing seamless partnerships between humans, data, and GenAI },
  DOI={ doi:10.1007/s41060-024-00684-w },
  journal={ International Journal of Data Science and Analytics },
  author={ Iván DurangoJosé A. GalludVictor M. R. Penichet },
  year={ 2024 },
  pages={ 1-28 },
  month={ 11 },
  abstract={ The accelerated development of generative artificial intelligence (GenAI) has initiated a transformative period in human–computer interaction, providing a wealth of unprecedented insights and concomitant complexities. This article examines the potential for the symbiotic integration of GenAI with Human–Data Interaction (HDI) principles, proposing a novel approach to the creation of more transparent, controllable, and user-centric AI systems. This article examines the ways in which the core principles of HDI—namely legibility, agency, and negotiability—can be systematically applied to GenAI systems across a range of domains. A comprehensive literature review and case studies demonstrate how this integration can address critical issues in AI development, including interpretability, user trust, and ethical alignment. This article introduces the Dynamic Neural Architecture for Interpretable and Customizable AI (DYNAMIC) approach, which represents a paradigm shift in the design of GenAI systems. DYNAMIC incorporates adaptive neural topologies, interpretable modules, and meta-learning techniques, thereby creating AI systems that are capable of dynamically adjusting to user needs and contexts while maintaining transparency and ethical awareness. Moreover, we put forth the Creative Cross-Domain Synergy (CDCS) approach, which employs HDI-enhanced GenAI to enable multimodal and cross-domain creative collaborations. The objective of this approach is to facilitate the formation of novel forms of partnership between humans and AI in a range of fields, including artistic creation and scientific research. Our research demonstrates the potential of HDI-GenAI integration to facilitate broader access to advanced AI technologies, mitigate ethical risks, and foster more meaningful human–AI collaborations. In conclusion, we present a research agenda that addresses key challenges in scalability, real-time interactivity, and ethical governance. We call for interdisciplinary efforts to realize the full potential of HDI-enhanced GenAI systems. This work contributes to the growing body of research on human-centred AI, offering a roadmap for developing GenAI systems that not only augment human capabilities but also align closely with human values and societal needs. },
  issn={ 2364-4168 },
}

@article{Yang_2024,
  title={ Employing animation-based TE to activate Chinese children’ cognitive-involvement in L2 feature acquisition },
  volume={ 29 },
  DOI={ doi:10.1007/s10639-023-12209-2 },
  journal={ Education and Information Technologies },
  author={ Xiongjun ZhouYunxia XiaoMengxue ZhengZhijie LiangBo SunJuan Yang },
  year={ 2024 },
  pages={ 9843-9863 },
  month={ 09 },
  abstract={ A substantial amount of literature has shown that the input flooding and enhancement (Textual enhancement, TE) technique helps learners to perceive and acquire target features implicitly. Previous studies explored the efficacy of computer-assisted TE interventions in Second Language Acquisition with diverse techniques, but animation has not yet been sufficiently taken into consideration. In this paper, we developed an animation-based TE intervention to promote the primary school children’ acquisition of the linguistic features of English question sentences. The results suggested that learners in experimental group improved their performances in producing questions and statements with a large effect size, as they were found to spontaneously put extra efforts in exploring the related knowledge of the visually enhanced forms. The study presumed that the animation-based TE intervention would stimulate participants’ awareness and curiosity of the target forms, which consequently led to improved performance in tasks both involving precursor knowledge and newly acquired knowledge. },
  issn={ 1573-7608 },
}

@article{Yang_2023,
  title={ A location-aware siamese network for high-speed visual tracking },
  volume={ 53 },
  DOI={ doi:10.1007/s10489-022-03636-8 },
  journal={ Applied Intelligence },
  author={ Lifang ZhouXiang DingWeisheng LiJiaxu LengBangjun LeiWeibin Yang },
  year={ 2023 },
  pages={ 4431-4447 },
  month={ 06 },
  abstract={ Accurately locating the target position is a challenging task during high-speed visual tracking. Most Siamese trackers based on shallow networks can maintain a fast speed, but they have poor positioning performance. The underlying reason for this is that the appearance features extracted from the shallow network are not effective enough, making it difficult to accurately locate the target from the complex background. Therefore, we present a location-aware Siamese network to address this issue. Specifically, we propose a novel context enhancement module (CEM), which contributes to capturing distinguished object information from both the local and the global levels. At the local level, the features of image local blocks contain more discriminative information that is conductive to locating the target. At the global level, global context information can effectively model long-range dependency, meaning that our tracker can better understand the tracking scene. Then, we construct a well-designed feature fusion network (F-net) to make full use of context information at different scales, where the location block can dynamically adjust to the convolution direction according to the geometry of the target. Finally, Distance-IoU loss (DIoU) is employed to guide the tracker to obtain a more accurate estimation of the target position. Extensive experiments on seven benchmarks including the VOT2016, VOT2018, VOT2019, OTB50, OTB100, UAV123 and LaSOT demonstrate that our tracker achieves competitive results while running at over 200 frames-per-second (FPS). },
  issn={ 1573-7497 },
}

@article{Sun_2024,
  title={ CRViT: Vision transformer advanced by causality and inductive bias for image recognition },
  volume={ 55 },
  DOI={ doi:10.1007/s10489-024-05910-3 },
  journal={ Applied Intelligence },
  author={ Faming LuKunhao JiaXue ZhangLin Sun },
  year={ 2024 },
  pages={ 1-16 },
  month={ 12 },
  abstract={ Vision Transformer (ViT) has shown powerful potential in various vision tasks by exploiting Transformer’s self-attention mechanism and global perception capability. However, to train a large number of network parameters, ViT requires a huge amount of data and number of computational resources, thus performing poorly on small and medium-sized datasets. Compared to ViT, convolutional networks maintain high accuracy despite the small amount of data due to the consideration of the inductive bias (IB). Besides, causal relationships can explore the underlying correlation of data structures, making the deep learning networks more intelligent. In this work, we propose a Causal Relationship Vision Transformer (CRViT), which refines ViT by fusing causal relationships and IB. We propose a random fourier features module that makes feature vectors independent of each other and uses convolution to learn correct correlation between feature vectors and extract causal features to introduce causal relationships in our network. The structure of convolutional downsampling significantly reduces the number of parameters of our model while introducing IB. Experimental validations underscore the data efficiency of CRViT, achieving a Top-1 accuracy of 80.6% on the ImageNet-1k dataset. This surpasses the ViT benchmark by 2.7% while concurrently reducing parameters by 92%. This enhanced performance is also consistent across smaller datasets, including T-ImageNet, CIFAR, and SVHN. We create the counterfactual dataset Colorful MNIST and experimentally demonstrate that causality is truly joined. },
  issn={ 1573-7497 },
}

@article{Nedelkoska_2023,
  title={ Mental health concerns precede quits: shifts in the work discourse during the Covid-19 pandemic and great resignation },
  volume={ 12 },
  DOI={ doi:10.1140/epjds/s13688-023-00417-2 },
  journal={ EPJ Data Science },
  author={ R. Maria del Rio-ChanonaAlejandro Hermida-CarrilloMelody Sepahpour-FardLuning SunRenata TopinkovaLjubica Nedelkoska },
  year={ 2023 },
  pages={ 1-26 },
  month={ 10 },
  abstract={ To study the causes of the 2021 Great Resignation, we use text analysis and investigate the changes in work- and quit-related posts between 2018 and 2021 on Reddit. We find that the Reddit discourse evolution resembles the dynamics of the U.S. quit and layoff rates. Furthermore, when the COVID-19 pandemic started, conversations related to working from home, switching jobs, work-related distress, and mental health increased, while discussions on commuting or moving for a job decreased. We distinguish between general work-related and specific quit-related discourse changes using a difference-in-differences method. Our main finding is that mental health and work-related distress topics disproportionally increased among quit-related posts since the onset of the pandemic, likely contributing to the quits of the Great Resignation. Along with better labor market conditions, some relief came beginning-to-mid-2021 when these concerns decreased. Our study underscores the importance of having access to data from online forums, such as Reddit, to study emerging economic phenomena in real time, providing a valuable supplement to traditional labor market surveys and administrative data. },
  issn={ 2193-1127 },
}

@article{Veidenbaum_2024,
  title={ Hyperdimensional computing: a framework for stochastic computation and symbolic AI },
  volume={ 11 },
  DOI={ doi:10.1186/s40537-024-01010-8 },
  journal={ Journal of Big Data },
  author={ Mike HeddesIgor NunesTony GivargisAlexandru NicolauAlex Veidenbaum },
  year={ 2024 },
  pages={ 1-32 },
  month={ 10 },
  abstract={ Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), is a neuro-inspired computing framework that exploits high-dimensional random vector spaces. HDC uses extremely parallelizable arithmetic to provide computational solutions that balance accuracy, efficiency and robustness. The majority of current HDC research focuses on the learning capabilities of these high-dimensional spaces. However, a tangential research direction investigates the properties of these high-dimensional spaces more generally as a probabilistic model for computation. In this manuscript, we provide an approachable, yet thorough, survey of the components of HDC. To highlight the dual use of HDC, we provide an in-depth analysis of two vastly different applications. The first uses HDC in a learning setting to classify graphs. Graphs are among the most important forms of information representation, and graph learning in IoT and sensor networks introduces challenges because of the limited compute capabilities. Compared to the state-of-the-art Graph Neural Networks, our proposed method achieves comparable accuracy, while training and inference times are on average 14.6× and 2.0× faster, respectively. Secondly, we analyse a dynamic hash table that uses a novel hypervector type called circular-hypervectors to map requests to a dynamic set of resources. The proposed hyperdimensional hashing method has the efficiency to be deployed in large systems. Moreover, our approach remains unaffected by a realistic level of memory errors which causes significant mismatches for existing methods. },
  issn={ 2196-1115 },
}

@article{Pundir_2024,
  title={ Explainable AI approach for early detection of Parkinson’s disease using PPMI Online data },
  volume={ 36 },
  DOI={ doi:10.1007/s00521-024-10127-z },
  journal={ Neural Computing and Applications },
  author={ Nitisha AggarwalGeetika Jain SaxenaSanjeev SinghAmit Pundir },
  year={ 2024 },
  pages={ 19209-19230 },
  month={ 08 },
  abstract={ Accurate and early disease prediction enables patients to plan and improve their quality of life in the future. Early detection of neurodegenerative diseases, such as Parkinson’s disease, is a high priority and a significant challenge in which physicians must act quickly to diagnose and predict the risk of disease severity. Machine learning (ML) models combined with feature selection (FS) techniques can assist physicians in quickly diagnosing a disease. FS technique optimally subsets features to improve model performance and reduce the number of tests required for a patient, thereby speeding up diagnosis. This paper proposes an e-diagnosis approach based on ML-FS algorithms to detect Parkinson’s disease using data obtained from Parkinson’s Progression Markers Initiative (PPMI) Online study. Also, it can be considered patient-oriented research as it uses self-reported online collected data. The results of six FS techniques pre-applied to classification algorithms named logistic regression, random forest, support vector machine, CatBoost, extreme learning machine, and XGBoost are shown in this study. Chi-square, mutual information, and analysis of variance (ANOVA) filter-based FS methods, while sequential feature selection, Boruta, and recursive feature elimination are considered wrapper methods. The outcomes show that random forest when trained on features selected by the recursive feature elimination technique help to build an efficient and effective approach for detecting Parkinson’s disease. },
  issn={ 1433-3058 },
}

@article{Krishnamurthy_2024,
  title={ A fuzzy rough set-based horse herd optimization algorithm for map reduce framework for customer behavior data },
  volume={ 66 },
  DOI={ doi:10.1007/s10115-024-02105-7 },
  journal={ Knowledge and Information Systems },
  author={ D. SudhaM. Krishnamurthy },
  year={ 2024 },
  pages={ 4721-4753 },
  month={ 04 },
  abstract={ A large number of association rules often minimizes the reliability of data mining results; hence, a dimensionality reduction technique is crucial for data analysis. When analyzing massive datasets, existing models take more time to scan the entire database because they discover unnecessary items and transactions that are not necessary for data analysis. For this purpose, the Fuzzy Rough Set-based Horse Herd Optimization (FRS-HHO) algorithm is proposed to be integrated with the Map Reduce algorithm to minimize query retrieval time and improve performance. The HHO algorithm minimizes the number of unnecessary items and transactions with minimal support value from the dataset to maximize fitness based on multiple objectives such as support, confidence, interestingness, and lift to evaluate the quality of association rules. The feature value of each item in the population is obtained by a Map Reduce-based fitness function to generate optimal frequent itemsets with minimum time. The Horse Herd Optimization (HHO) is employed to solve the high-dimensional optimization problems. The proposed FRS-HHO approach takes less time to execute for dimensions and has a space complexity of 38% for a total of 10&nbsp;k transactions. Also, the FRS-HHO approach offers a speedup rate of 17% and a 12% decrease in input–output communication cost when compared to other approaches. The proposed FRS-HHO model enhances performance in terms of execution time, space complexity, and speed. },
  issn={ 0219-3116 },
}

@article{Montoya_2023,
  title={ The ethics of algorithms from the perspective of the cultural history of consciousness: first look },
  volume={ 38 },
  DOI={ doi:10.1007/s00146-022-01475-2 },
  journal={ AI & SOCIETY },
  author={ Carlos Andres Salazar MartinezOlga Lucia Quintero Montoya },
  year={ 2023 },
  pages={ 763-775 },
  month={ 05 },
  abstract={ Theories related to cognitive sciences, Human-in-the-loop Cyber-physical systems, data analysis for decision-making, and computational ethics make clear the need to create transdisciplinary learning, research, and application strategies to bring coherence to the paradigm of a truly human-oriented technology. Autonomous objects assume more responsibilities for individual and collective phenomena, they have gradually filtered into routines and require the incorporation of ethical practice into the professions related to the development, modeling, and design of algorithms. To make this possible, it is pertinent and urgent to bring them closer to the problems and approaches of the humanities. Increasingly transdisciplinary research must be part of the construction of systems that provide developers and scientists with the necessary bases to understand the ethical debate and therefore their commitment to society. This article considers two theories as articulating axes: Blumenberg’s, coming from the field of philosophy, for whom the process of technification and especially the implementation of mathematical models in their algorithmic form leads to an emptying of meaning and therefore makes programmers who implement their functions to be alien to the concerns that gave them origin; Daston’s, belonging to the field of the history of science and according to which the division of labor in the processes of technification of the calculation implies a kind of subordination in which those who implement the inventions of a small group of privileged mathematicians ignore the procedures that put them into operation. Given these two theories, the black box models prevalent in AI development, and the urgency of establishing explanatory frameworks for the development of computational ethics, this article exposes the need to give a voice to the cultural history of consciousness for promoting the discussions around the implementation of mathematical algorithms. The paper takes as a reference the different points of view that have emerged around the study of technological ethics, its applicability, management, and design. It criticizes the current state of studies from a humanistic perspective and explains how the historical perspective allows promoting the training of software engineers, developers and creators so that they assume intuitions and moral values in the development of their work. Specifically, it aims to expose how cultural history, applied to the study of consciousness and its phenomena, makes those involved in this technological revolution aware of the effect that they, through their algorithms, have on society in general and on human beings in particular. },
  issn={ 1435-5655 },
}

@article{Malik_2024,
  title={ Text classification models for personality disorders identification },
  volume={ 14 },
  DOI={ doi:10.1007/s13278-024-01219-8 },
  journal={ Social Network Analysis and Mining },
  author={ Deepti JainSandhya AroraC. K. JhaGarima Malik },
  year={ 2024 },
  pages={ 1-20 },
  month={ 03 },
  abstract={ This research focuses on identifying personality disorders in individuals using their social media text. We developed a unique collection of words (PD-Corpus) and a dataset (PD-TXT), which includes texts marked with different personality disorder traits. Our goal was to classify these texts into six types of personality disorders, using Natural Language Processing (NLP) classification models. The results showed that our transformer-based models, especially the BERT-base-uncased model, were more effective than traditional methods, achieving a 74.7% success rate in correctly classifying these disorders. Also, our models consistently outperform existing literature baseline models on the PD-TXT dataset, showcasing significant enhancements. This study presents a new way to predict personality disorders through linguistic analysis and highlights the potential for further research combining language studies with mental health. },
  issn={ 1869-5469 },
}

@article{Li_2024b,
  title={ UCEMA: Uni-modal and cross-modal encoding network based on multi-head attention for emotion recognition in conversation },
  volume={ 30 },
  DOI={ doi:10.1007/s00530-024-01561-z },
  journal={ Multimedia Systems },
  author={ Hongkun ZhaoSiyuan LiuYang ChenFanmin KongQingtian ZengKang Li },
  year={ 2024 },
  pages={ 1-14 },
  month={ 11 },
  abstract={ Emotion recognition in conversation (ERC) represents a pivotal research domain within affective computing, concentrating on discerning the emotional nuances embedded within individual utterances during conversational exchanges. The majority of current research focuses on modeling situational cues, with relatively little attention paid on affective tendencies inherent in emotional expression. Moreover, ensuring the fair representation of diverse modalities in emotional expression presents a significant challenge in effectively extracting synergies and insights from multi-modal data sources. To tackle these challenges, this study proposes a novel approach termed the Uni-Modal and Cross-Modal Encoding Network based on Multi-Head Attention (UCEMA) for ERC. The framework leverages two distinct encoding techniques, namely Uni-Modal Encoding based on Multi-Head Attention (UEMA) and Cross-Modal Encoding based on Multi-Head Attention (CEMA), to extract distinct emotional features from individual modes and facilitate the fusion of emotional attributes within the multi-modal context. Particular emphasis is placed on textual input as the primary mode of interaction. Additionally, this study employs Context Modeling (CM) to analyze the outcomes of emotion recognition in conversational contexts. A comprehensive comparative analysis of the UCEMA was conducted on two publicly available datasets, IEMOCAP and MELD. The results demonstrated that the UCEMA exhibited superior efficacy. It is noteworthy that the proposed framework effectively balances intra-modal emotional orientation information, inter-modal emotional association information, and context-related cues, thereby demonstrating superior performance in recognition accuracy compared to current state-of-the-art (SOTA) models. },
  issn={ 1432-1882 },
}

@article{Luo_2023,
  title={ TransBLS: transformer combined with broad learning system for facial beauty prediction },
  volume={ 53 },
  DOI={ doi:10.1007/s10489-023-04931-8 },
  journal={ Applied Intelligence },
  author={ Junying GanXiaoshan XieGuohui HeHeng Luo },
  year={ 2023 },
  pages={ 26110-26125 },
  month={ 08 },
  abstract={ Facial beauty prediction (FBP) is a frontier topic in the fields of machine learning and computer vision, focusing on how to enable computers to judge facial beauty like humans. The existing FBP methods are mainly based on deep neural networks (DNNs). However, DNNs lack global characteristics and only build local dependencies, so FBP still suffers from insufficient supervision information, low accuracy and overfitting. A transformer is a self-attention-based architecture that possesses better global characteristics than DNNs and can build long-term dependencies. Transformers have been widely used to solve some computer vision problems in recent years and have produced better results. In this paper, we propose an adaptive transformer with global and local multihead self-attention for FBP, called GLAFormer. However, GLAFormer does not converge and is prone to overfitting when the training samples are insufficient. The broad learning system (BLS) can accelerate the model convergence process and reduce overfitting. Therefore, we further combine GLAFormer with the BLS to form TransBLS, in which a GLAFormer block is designed as a feature extractor, the features extracted by it are transferred to the BLS for further refining and fitting, and the results are output. Experimental results indicate that TransBLS achieves state-of-the-art FBP performance on several datasets with different scales, better solving the low accuracy and overfitting problems encountered in FBP. It can also be widely applied in pattern recognition and object detection tasks. },
  issn={ 1573-7497 },
}

@article{Nielsen_2024,
  title={ A Semi-Automated Solution Approach Recommender for a Given Use Case: a Case Study for AI/ML in Oncology via Scopus and OpenAI },
  volume={ 4 },
  DOI={ doi:10.1007/s44230-024-00070-6 },
  journal={ Human-Centric Intelligent Systems },
  author={ Deniz Kenan KılıçAlex Elkjær VasegaardAurélien DesoeuvresPeter Nielsen },
  year={ 2024 },
  pages={ 447-495 },
  month={ 05 },
  abstract={ Nowadays, literature review is a necessary task when trying to solve a given problem. However, an exhaustive literature review is very time-consuming in today’s vast literature landscape. It can take weeks, even if looking only for abstracts or surveys. Moreover, choosing a method among others, and targeting searches within relevant problem and solution domains, are not easy tasks. These are especially true for young researchers or engineers starting to work in their field. Even if surveys that provide methods used to solve a specific problem already exist, an automatic way to do it for any use case is missing, especially for those who don’t know the existing literature. Our proposed tool, SARBOLD-LLM, allows discovering and choosing among methods related to a given problem, providing additional information about their uses in the literature to derive decision-making insights, in only a few hours. The SARBOLD-LLM comprises three modules: (1: Scopus search) paper selection using a keyword selection scheme to query Scopus API; (2: Scoring and method extraction) relevancy and popularity scores calculation and solution method extraction in papers utilizing OpenAI API (GPT 3.5); (3: Analyzes) sensitivity analysis and post-analyzes which reveals trends, relevant papers and methods. Comparing the SARBOLD-LLM to manual ground truth using precision, recall, and F1-score metrics, the performance results of AI in the oncology case study are 0.68, 0.9, and 0.77, respectively. SARBOLD-LLM demonstrates successful outcomes across various domains, showcasing its robustness and effectiveness. The SARBOLD-LLM addresses engineers more than researchers, as it proposes methods and trends without adding pros and cons. It is a useful tool to select which methods to investigate first and comes as a complement to surveys. This can limit the global search and accumulation of knowledge for the end user. However, it can be used as a director or recommender for future implementation to solve a problem. },
  issn={ 2667-1336 },
}

@article{Li_2024c,
  title={ Enhanced Self-Attention-Based Rapid CNN for Detecting Dense Objects in Varying Illumination },
  volume={ 17 },
  DOI={ doi:10.1007/s12559-024-10376-z },
  journal={ Cognitive Computation },
  author={ Lu ChenLi YangTan JieMa HaoyuanLiu YuFu ShenbingJunkang WangHao WuGun Li },
  year={ 2024 },
  pages={ 1-20 },
  month={ 12 },
  abstract={ This paper addresses the challenge of efficient detection of densely arranged unordered items under varying illumination. Specifically, a novel convolutional neural network-based method is proposed for item vector detection, recognition, and classification, termed Self-Attention and Concatenation-Based Detector (ACDet). In a benchmark pharmaceutical case study, rapid and accurate detection of pharmaceutical package contours is achieved, enabling the automatic and fast verification of both the quantity and types of pharmaceuticals during distribution. At the input stage, a combined image augmentation method is applied to improve the detection model’s ability to learn the appearance features of items from multiple angles. Based on YOLOv8 model, integrating computational module C2F with Attention (C2F-A), multidimensional self-attention reinforcement is applied to the outputs of multiple gradient streams. The designed Weighted Concatenation (WConcat) module self-learns to weight and concatenate multi-level feature maps, enhancing the model’s cognitive capability. Finally, simulation experiments are conducted to determine the optimal timing for utilizing each module. Simulation experiments compared the proposed ACDet with several state-of-the-art YOLO architecture models utilizing the benchmark Comprehensive Pharmaceutical Package Dataset (CPPD). ACDet achieved 81.0% mAP and 79.5% Smooth mAP on the CPPD dataset, outperforming other models by an average of 5.5% to 16.6%. On public datasets, the results were 52.2% and 51.0%, respectively. The impact of utilizing C2F-A at different stages on performance was also tested, concluding that the WConcat module does not necessitate spatial attention. Finally, in zero-shot testing, the verification success rate reached 99.91%. Our work shows that the proposed ACDet can overcome many challenges in complex object detection scenarios, enhancing robustness while maintaining a lightweight design. The proposed model can serve as a new benchmark. },
  issn={ 1866-9964 },
}

@article{Yu_2024b,
  title={ How is public discussion as reflected in WeChat articles different from scholarly research in China? An empirical study of metaverse },
  volume={ 129 },
  DOI={ doi:10.1007/s11192-023-04892-2 },
  journal={ Scientometrics },
  author={ Yang ZhangYinghua XieLongfei LiYian LiangHouqiang Yu },
  year={ 2024 },
  pages={ 473-495 },
  month={ 12 },
  abstract={ Social media platforms do not blur the difference in information preferences between the general public and researchers when faced with the same heated events. This study aims to investigate the consistency between the public focus conveyed by WeChat articles and the scholarly focus reflected by CNKI papers in China, and to reveal the underlying interaction between researchers and the public. Metaverse is used as a case study. Based on articles mentioning metaverse in WeChat and CNKI, the dominant accounts and disciplines, topics discussed and studied, and sentiments related to metaverse are explored. Furthermore, WeChat articles mentioning scholarly outputs are identified to map the interaction between the public and researchers. Empirical analysis reveals that the first articles mentioning metaverse in both datasets predate the rebranding of Facebook. WeChat official accounts from the technology and finance industries post more metaverse-related articles, while researchers from journalism and information management are the main forces in academia. Both the public and academia discuss the impact of metaverse on the economy, politics, and social relations, the public also discusses the infrastructure, while academia ponders metaverse from the philosophical perspective, mass communication, and education. 60% of academic articles are mentioned by WeChat. The operators of WeChat official accounts, the public, and scholars express different sentiments. The theoretical significance lies in combining social media studies of science with bibliometric analysis. Practically, the public can take advantage to clarify the confusion related to metaverse. For policymakers, we provide scientific evidence to look for directions in the metaverse development. },
  issn={ 1588-2861 },
}

@article{Rajakumari_2024,
  title={ Ensemble-of-classifiers-based approach for early Alzheimer’s Disease detection },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-16023-3 },
  journal={ Multimedia Tools and Applications },
  author={ RS RajasreeS Brintha Rajakumari },
  year={ 2024 },
  pages={ 16067-16095 },
  month={ 07 },
  abstract={ Alzheimer’s disease (AD) is a deadly neurological condition. Deep learning approaches (DL) techniques have just been utilized to track the evolution of Alzheimer’s disease. These studies only employed baseline neuro imaging data. Because of the high cost of neuro imaging data, it is constantly restricted or unavailable. As a result, this research developed a novel, four-phase early Alzheimer’s disease detection approach: “(a) pre-processing, (b) feature extraction, (c) feature selection, and (d) classification”. Data cleaning and normalization is used in pre-processing. Consequently, features like “Weighted Geometric Mean Principle Component Analysis (WGM-PCA), Statistical Features, higher-order statistical features, and Weighted modified correlation-based features” are retrieved from the pre-processed data. Employing the Improved Attribute Ranker (IAR), the most relevant characteristics are chosen. Furthermore, the disease classification phase is represented by a deep learning model based on an ensemble of classifiers, containing optimized “Bi-GRU, Multi-Layer Perceptron (MLP), and Quantum Neural Network (QDNN)”, respectively. The ultimate decision is obtained via optimal Bi-GRU, which is trained using MLP and QDNN outcomes. Both the MLP and the QDNN would be trained using the chosen IAR-based features. Interestingly, to improve the network’s detection accuracy, the weight of the QDNN model is adjusted using the recently proposed Enhanced Math Optimizer Accelerated Arithmetic Optimization (EMOAOA) technique. Particularly, the proposed EMOAOA+EC achieved detecting accuracies of 95% at the 60th LR, 95.5% at the 70th LR, 98% at the 80th LR, and 98.7% at the 90th LR. The development of the optimized ensemble classifier is responsible for this improvement. },
  issn={ 1573-7721 },
}

@article{Ukrit_2024,
  title={ Design of inception with deep convolutional neural network based fall detection and classification model },
  volume={ 83 },
  DOI={ doi:10.1007/s11042-023-16476-6 },
  journal={ Multimedia Tools and Applications },
  author={ K. Durga BhavaniM. Ferni Ukrit },
  year={ 2024 },
  pages={ 23799-23817 },
  month={ 08 },
  abstract={ Falling is the most serious health problem for elderly population resulting in serious injuries, if not treated quickly. As the world population gets increased, the number of serious falls and succeeding financial burdens rise accordingly. It is important to detect falls timely to initiate appropriate medical responses to decrease considerable physical, social, and financial losses. Earlier detection of fall events can helps to provide timely medical services and reduce severe injuries. Different techniques have been developed for fall detection process among elderly population. Recently, the development of Internet of Things (IoT) and artificial intelligence (AI) technologies involving deep learning (DL) and machine learning (ML) approaches can be employed in the field of healthcare for automating the diagnosis procedure of diseased and abnormal cases. This study designs a new Inception with deep convolutional neural network-based fall detection and classification (INDCNN-FDC) model. The presented INDCNN-FDC model intends to categorize the events into two class labels namely fall and not fall. To accomplish this, the INDCNN-FDC model carries out two stages of data pre-processing: Gaussian filter (GF) based image sharpening and Guided Filter (GIF) based image smoothing. In addition, the presented INDCNN-FDC model applies deep transfer learning-based Inception v3 model for generating a helpful group of feature vectors. Finally, DCNN approach receives the feature vectors as input and performs fall detection process. The experimental validation of the INDCNN-FDC approach is performed on benchmark dataset. The comparative study reported the supremacy of the INDCNN-FDC model over state-of-the-art. },
  issn={ 1573-7721 },
}

@article{Chen_2024a,
  title={ Enhancing academic performance prediction with temporal graph networks for massive open online courses },
  volume={ 11 },
  DOI={ doi:10.1186/s40537-024-00918-5 },
  journal={ Journal of Big Data },
  author={ Qionghao HuangJili Chen },
  year={ 2024 },
  pages={ 1-26 },
  month={ 04 },
  abstract={ Educational big data significantly impacts education, and Massive Open Online Courses (MOOCs), a crucial learning approach, have evolved to be more intelligent with these technologies. Deep neural networks have significantly advanced the crucial task within MOOCs, predicting student academic performance. However, most deep learning-based methods usually ignore the temporal information and interaction behaviors during the learning activities, which can effectively enhance the model’s predictive accuracy. To tackle this, we formulate the learning processes of e-learning students as dynamic temporal graphs to encode the temporal information and interaction behaviors during their studying. We propose a novel academic performance prediction model (APP-TGN) based on temporal graph neural networks. Specifically, in APP-TGN, a dynamic graph is constructed from online learning activity logs. A temporal graph network with low-high filters learns potential academic performance variations encoded in dynamic graphs. Furthermore, a global sampling module is developed to mitigate the problem of false correlations in deep learning-based models. Finally, multi-head attention is utilized for predicting academic outcomes. Extensive experiments are conducted on a well-known public dataset. The experimental results indicate that APP-TGN significantly surpasses existing methods and demonstrates excellent potential in automated feedback and personalized learning. },
  issn={ 2196-1115 },
}

@article{Memmert_2023,
  title={ Graph representations for the analysis of multi-agent spatiotemporal sports data },
  volume={ 53 },
  DOI={ doi:10.1007/s10489-022-03631-z },
  journal={ Applied Intelligence },
  author={ Dominik RaabeReinhard NabbenDaniel Memmert },
  year={ 2023 },
  pages={ 3783-3803 },
  month={ 06 },
  abstract={ Analyzing tactical patterns in invasion games using multi-agent spatiotemporal data is a challenging task at the intersection of computer and sports science. A fundamental yet understudied problem in this area is finding an optimal data representation for processing athlete trajectories using machine learning algorithms. In the present work, we address this gap by discussing common representations in use and propose Tactical Graphs, an alternative graph-based format capable of producing integrative, contextualized models for machine learning applications. We provide an in-depth, domain-specific motivation of the proposed data representation scheme and show how this approach exploits inherent data traits. We propose Tactical Graph Networks (TGNets), a light-weight, hybrid machine learning architecture sensitive to player interactions. Our method is evaluated with an extensive ablation study and the first comprehensive state of the art comparison between standard feature, state vector, and image-based methods on the same dataset. Experiments were conducted using real-world football data containing short sequences of defensive play labelled according to the outcome of ball winning attempts. The results indicate that TGNets are on par with state-of-the-art deep learning models while exhibiting only a fraction of their complexity. We further demonstrate that selecting the right data representation is crucial as it has a significant influence on model performance. The theoretical findings and the proposed method provide insights and a strong methodological alternative for all classification, prediction or pattern recognition applications in the areas of collective movement analysis, automated match analysis, and performance analysis. },
  issn={ 1573-7497 },
}

@article{Frandsen_2023,
  title={ A systematic review of almost three decades of value sensitive design (VSD): what happened to the technical investigations? },
  volume={ 25 },
  DOI={ doi:10.1007/s10676-023-09700-2 },
  journal={ Ethics and Information Technology },
  author={ Anne GerdesTove Faber Frandsen },
  year={ 2023 },
  pages={ 1-16 },
  month={ 04 },
  abstract={ This article presents a systematic literature review documenting how technical investigations have been adapted in value sensitive design (VSD) studies from 1996 to 2023. We present a systematic review, including theoretical and applied studies that either discuss or conduct technical investigations in VSD. This systematic review contributes to the VSD community when seeking to further refine the methodological framework for carrying out technical investigations in VSD. },
  issn={ 1572-8439 },
}

@article{Maggini_2024,
  title={ Beyond phase-in: assessing impacts on disinformation of the EU Digital Services Act },
  DOI={ doi:10.1007/s43681-024-00467-w },
  journal={ AI and Ethics },
  author={ Luca NanniniEleonora BonelDavide BassiMichele Joshua Maggini },
  year={ 2024 },
  pages={ 1-29 },
  month={ 04 },
  abstract={ This work proposes a comprehensive research agenda to empirically evaluate the real-world impacts of the European Union’s Digital Services Act (DSA) on combating online disinformation. It provides background on the DSA’s context, mechanisms, timeline, and expected effects on platforms to situate the need for rigorous impact assessment. A detailed legal, technical, psychological, behavioral and ethical critique reveals meaningful gaps in the DSA requiring ongoing regulatory refinement and oversight. Most critically, the paper puts forth an encompassing framework spanning computational analytics, interviews, ethnography, surveys, discourse analysis and mixed methods to rigorously assess the DSA’s multi-dimensional effects on complex factors enabling disinformation proliferation. Priorities include evaluating notice-and-takedown efficacy, advertising transparency improvements, risk assessment outcomes, oversight integration, and procedural shifts in platform governance. Coordinated efforts between researchers, regulators and platforms are needed to address methodological challenges around isolating DSA impacts amidst an evolving EU regulatory landscape, constrained data access from platforms, and difficulties generalizing findings across the sociotechnical diversity of platforms and national contexts in EU Member States. },
  issn={ 2730-5961 },
}

@article{Yang_2025,
  title={ Efficient encrypted semantic search method toward internet of vehicles },
  volume={ 81 },
  DOI={ doi:10.1007/s11227-025-06981-w },
  journal={ The Journal of Supercomputing },
  author={ Ruyan WangZhen ZhangPuning ZhangChao WuZhigang Yang },
  year={ 2025 },
  pages={ 1-35 },
  month={ 02 },
  abstract={ The internet of vehicles (IoV) connects vehicles, infrastructure, and users, generating a large amount of sensitive data that requires strong privacy protection. To address the limitations of traditional keyword-based exact search methods, this paper proposes an efficient encrypted semantic search method for IoV. The method introduces an encrypted search architecture that combines edge and cloud computing to achieve low-latency processing and high-capacity storage. An encrypted knowledge graph-based semantic search (EKGSS) method is developed to combine symmetric encryption with knowledge graphs to achieve privacy-preserving and semantically relevant content retrieval. Experiment results show that EKGSS improves precision by 3.2% and recall by 36% while ensuring data privacy. },
  issn={ 1573-0484 },
}

@article{Koulouriotis_2025,
  title={ Text mining technologies applied to free-text answers of students in e-assessment },
  volume={ 28 },
  DOI={ doi:10.1007/s10791-024-09496-9 },
  journal={ Discover Computing },
  author={ Angelos CharitopoulosMaria RangoussiDimitris MetafasDimitrios Koulouriotis },
  year={ 2025 },
  pages={ 1-33 },
  month={ 01 },
  abstract={ Educational Text Mining is a rapidly growing field, thanks to the adoption of modern probabilistic and machine learning algorithms. The current study focuses on student e-assessment by means of open-ended questions that require free-text answers (i.e., student essays), whose analysis and evaluation are resource-demanding tasks for the instructor, even when supported by modern e-learning platforms. Topic modeling through the Latent Dirichlet Allocation algorithm is employed in an experimental setup, aiming to (a) extract meaningful topics from the body of pooled student answers (interpretable in the educational context of the course), (b) align the extracted topics with the ‘native’ internal structure of the body of texts, and (c) offer recommendations for the teacher in the form of alternative (meaningful) restructurings of the e-assessment units and consequently of the course content units. Quantitative and qualitative evaluation of the extracted topic models have yielded positive results regarding the first two aims, and as far as the third aim is concerned, the extracted topic models expilicitly suggest that the teacher should proceed with relevant restructurings of the course content. These recommendations are of practical use for the teacher, especially when the teacher seeks to restructure the content of the course towards either fewer or more internal units. In conclusion, topic modeling provides a spectrum of possibilities to the teacher who is interested in exploring ways to improve the structure and organization of a course. },
  issn={ 2948-2992 },
}

@article{Zhang_2025,
  title={ A review on the novelty measurements of academic papers },
  volume={ 130 },
  DOI={ doi:10.1007/s11192-025-05234-0 },
  journal={ Scientometrics },
  author={ Yi ZhaoChengzhi Zhang },
  year={ 2025 },
  pages={ 727-753 },
  month={ 01 },
  abstract={ Novelty evaluation is vital for the promotion and management of innovation. With the advancement of information techniques and the open data movement, some progress has been made in novelty measurements. Tracking and reviewing novelty measures provides a data-driven way to assess contributions, progress, and emerging directions in the science field. As academic papers serve as the primary medium for the dissemination, validation, and discussion of scientific knowledge, this review aims to offer a systematic analysis of novelty measurements for scientific papers. We began by comparing the differences between scientific&nbsp;novelty and four similar concepts, including&nbsp;originality, scientific innovation, creativity, and scientific breakthrough. Next, we reviewed the types of scientific novelty. Then, we classified existing novelty measures according to data types and reviewed the&nbsp;measures for each type. Subsequently, we surveyed the approaches employed in validating novelty measures and examined the current tools and datasets associated with these measures. Finally, we proposed several open issues for future studies. },
  issn={ 1588-2861 },
}

@article{Li_2025,
  title={ Exploiting user comments for early detection of fake news prior to users’ commenting },
  volume={ 19 },
  DOI={ doi:10.1007/s11704-024-40674-6 },
  journal={ Frontiers of Computer Science },
  author={ Qiong NanQiang ShengJuan CaoYongchun ZhuDanding WangGuang YangJintao Li },
  year={ 2025 },
  pages={ 1-13 },
  month={ 01 },
  abstract={ Both accuracy and timeliness are key factors in detecting fake news on social media. However, most existing methods encounter an accuracy-timeliness dilemma: Content-only methods guarantee timeliness but perform moderately because of limited available information, while social context-based ones generally perform better but inevitably lead to latency because of social context accumulation needs. To break such a dilemma, a feasible but not well-studied solution is to leverage social contexts (e.g., comments) from historical news for training a detection model and apply it to newly emerging news without social contexts. This requires the model to (1) sufficiently learn helpful knowledge from social contexts, and (2) be well compatible with situations that social contexts are available or not. To achieve this goal, we propose to absorb and parameterize useful knowledge from comments in historical news and then inject it into a content-only detection model. Specifically, we design the Comments ASsisted FakENews Detection method (CAS-FEND), which transfers useful knowledge from a comment-aware teacher model to a content-only student model and detects newly emerging news with the student model. Experiments show that the CAS-FEND student model outperforms all content-only methods and even comment-aware ones with 1/4 comments as inputs, demonstrating its superiority for early detection. },
  issn={ 2095-2236 },
}

@article{Wang_2025,
  title={ The rise and potential of large language model based agents: a survey },
  volume={ 68 },
  DOI={ doi:10.1007/s11432-024-4222-0 },
  journal={ Science China Information Sciences },
  author={ Zhiheng XiWenxiang ChenXin GuoWei HeYiwen DingBoyang HongMing ZhangJunzhe WangSenjie JinEnyu ZhouRui ZhengXiaoran FanXiao WangLimao XiongYuhao ZhouWeiran Wang },
  year={ 2025 },
  pages={ 1-44 },
  month={ 01 },
  abstract={ For a long time, researchers have sought artificial intelligence (AI) that matches or exceeds human intelligence. AI agents, which are artificial entities capable of sensing the environment, making decisions, and taking actions, are seen as a means to achieve this goal. Extensive efforts have been made to develop AI agents, with a primary focus on refining algorithms or training strategies to enhance specific skills or particular task performance. The field, however, lacks a sufficiently general and powerful model to serve as a foundation for building general agents adaptable to diverse scenarios. With their versatile capabilities, large language models (LLMs) pave a promising path for the development of general AI agents, and substantial progress has been made in the realm of LLM-based agents. In this article, we conduct a comprehensive survey on LLM-based agents, covering their construction frameworks, application scenarios, and the exploration of societies built upon LLM-based agents. We also conclude some potential future directions and open problems in this flourishing field. },
  issn={ 1869-1919 },
}

@article{Wen_2025,
  title={ Tool learning with large language models: a survey },
  volume={ 19 },
  DOI={ doi:10.1007/s11704-024-40678-2 },
  journal={ Frontiers of Computer Science },
  author={ Changle QuSunhao DaiXiaochi WeiHengyi CaiShuaiqiang WangDawei YinJun XuJi-rong Wen },
  year={ 2025 },
  pages={ 1-21 },
  month={ 01 },
  abstract={ Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the “why” by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of “how”, we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area. },
  issn={ 2095-2236 },
}

@article{Sicuranza_2025,
  title={ Digital twins for intelligent cities: the case study of Matera },
  volume={ 11 },
  DOI={ doi:10.1007/s40860-025-00245-3 },
  journal={ Journal of Reliable Intelligent Environments },
  author={ Riccardo De BenedictisAmedeo CestaRiccardo PellegriniMatteo DiezDiego Maria PintoPaolo VenturaGiuseppe SteccaGiovanni FeliciAndreas ScalasMichela MortaraDaniela CabidduSimone PittalugaMichela SpagnuoloStefano SilvestriEmanuele DamianoMario Sicuranza },
  year={ 2025 },
  pages={ 1-19 },
  month={ 02 },
  abstract={ The digital twin (DT) paradigm provides a purpose-built digital representation of a physical system. DTs are often composed by several interconnected models, which need to be specifically tailored to fit the DT purposes. The objective of this work is the development of an urban digital twin (UDT) for the city of Matera, following urban intelligence (UI) paradigm. The latter leverages the multi-disciplinary integration and optimization of the city systems and subsystems to develop purpose-driven DTs and support the decision-making process. The UDT is intended to support governance, stakeholders, and citizens, integrating morphological data for the city representation, reliable simulation tools and data-driven methods for the city state prediction (e.g., traffic, solar irradiation), optimization algorithms for planning and emergency response, sensors for vehicle and pedestrian traffic volumes and environmental monitoring (e.g., pollutant distributions), and a participatory data collection process from the citizens. A Data Lake is used to make available to the UDT modules the data produced by the morphological representation, simulations/data-driven methods, sensors, and the participatory data. The Data Lake provides a standardized approach for the aggregation and extraction of information. Finally, an Urban Sensing Engine supports the decision-making process with artificial intelligence forms of reasoning, to effectively combine the information produced by the UDT components. },
  issn={ 2199-4676 },
}

@article{Wang_2025a,
  title={ A soft prompt learning method for medical text classification with simulated human cognitive capabilities },
  volume={ 58 },
  DOI={ doi:10.1007/s10462-025-11121-0 },
  journal={ Artificial Intelligence Review },
  author={ Yu WangLuyao ZhouWeimin ZhangFeifan ZhangYuan Wang },
  year={ 2025 },
  pages={ 1-30 },
  month={ 01 },
  abstract={ Medical text classification aims to identify the category to which a short medical text belongs. Recent research indicates that the “prompt-based learning” paradigm, especially the soft prompt, can improve the performance in text classification tasks by bridging the gap between pre-training objectives and downstream tasks. However, current research on soft prompt learning neglects the relation between the pseudo tokens and raw sentences when generating embeddings for the template. This study investigates, for the first time, how to simulate human cognitive processes in medical text classification tasks using soft prompt learning based on attention mechanisms. The proposed approach pays more attention to the parts of the raw sentence that are more relevant to the category label when generating embeddings for the pseudo tokens, resembling the reasoning process humans go through during text classification. Experiments conducted on two datasets, KUAKE-QIC and CHIP-CTC, indicate that the F1-macro scores of the proposed approach are 0.8064 and 0.8434, which outperform the benchmark and previous prompt learning approaches. In addition, the corresponding experiments also demonstrate the generalization ability and few-shot learning capability of the proposed method. },
  issn={ 1573-7462 },
}

@article{Deng_2025,
  title={ Affordances, constraints, and implications of ChatGPT in education from a social-ecological perspective: A data mining approach },
  DOI={ doi:10.1007/s10639-024-13237-2 },
  journal={ Education and Information Technologies },
  author={ Yuchun ZhongJie LianHao HuangHao Deng },
  year={ 2025 },
  pages={ 1-34 },
  month={ 02 },
  abstract={ This study investigated the affordances, constraints, and implications of ChatGPT in education using the affordance theory and social-ecological systems theory. We employed a data mining approach that blends social media analytics including sentiment analysis and topic modelling and qualitative analysis to extract viewpoints from a collection of datasets consisting of 33,456 tweets. Key findings indicate that 42.1% of analysed tweets conveyed a positive sentiment, 39.6% were neutral, and only 18.3% conveyed a negative sentiment. We also identified five categories of ChatGPT properties (e.g., text and data analysis, AI and machine learning) and an array of affordances of ChatGPT in education (e.g., facilitating student personalised learning, classroom instruction, provision of educational resources, curriculum changes, and assessment). Meanwhile, the findings revealed key concerns, including academic dishonesty, bias, and ethics that warrant attention. This study contributes to a real-time understanding of the impact of ChatGPT on education and informs researchers, educators, and policymakers to take a holistic approach to evaluating ChatGPT in educational practices. },
  issn={ 1573-7608 },
}

@article{Fischer_2025,
  title={ “ChatGPT says no”: agency, trust, and blame in Twitter discourses after the launch of ChatGPT },
  volume={ 5 },
  DOI={ doi:10.1007/s43681-023-00414-1 },
  journal={ AI and Ethics },
  author={ Dan HeatonElena NicheleJeremie ClosJoel E. Fischer },
  year={ 2025 },
  pages={ 653-675 },
  month={ 01 },
  abstract={ ChatGPT, a chatbot using the GPT-n series large language model, has surged in popularity by providing conversation, assistance, and entertainment. This has raised questions about its agency and resulting implications on trust and blame, particularly when concerning its portrayal on social media platforms like Twitter. Understanding trust and blame is crucial for gauging public perception, reliance on, and adoption of AI-driven tools like ChatGPT. To explore ChatGPT’s perceived status as an algorithmic social actor and uncover implications for trust and blame through agency and transitivity, we examined 88,058 tweets about ChatGPT, published in a ‘hype period’ between November 2022 and March 2023, using Corpus Linguistics and Critical Discourse Analysis, underpinned by Social Actor Representation. Notably, ChatGPT was presented in tweets as a social actor on 87% of occasions, using personalisation and agency metaphor to emphasise its role in content creation, information dissemination, and influence. However, a dynamic presentation, oscillating between a creative social actor and an information source, reflected users’ uncertainty regarding its capabilities and, thus, blame attribution occurred. On 13% of occasions, ChatGPT was presented passively through backgrounding and exclusion. Here, the emphasis on ChatGPT’s role in informing and influencing underscores interactors’ reliance on it for information, bearing implications for information dissemination and trust in AI-generated content. Therefore, this study contributes to understanding the perceived social agency of decision-making algorithms and their implications on trust and blame, valuable to AI developers and policymakers and relevant in comprehending and dealing with power dynamics in today’s age of AI. },
  issn={ 2730-5961 },
}

@article{Addison_2025,
  title={ La VIDA: towards a motivated goal reasoning agent },
  volume={ 39 },
  DOI={ doi:10.1007/s10458-024-09685-2 },
  journal={ Autonomous Agents and Multi-Agent Systems },
  author={ Ursula Addison },
  year={ 2025 },
  pages={ 1-36 },
  month={ 01 },
  abstract={ An autonomous agent deployed to operate over extended horizons in uncertain environments will encounter situations for which it was not designed. A class of these situations involves an invalidation of agent goals and limited guidance in establishing a new set of goals to pursue. An agent will benefit from some mechanism that will allow it to pursue new goals under these circumstances such that the goals are broadly useful in its environment and take advantage of its existing skills while aligning with societal norms. We propose augmenting a goal reasoning agent, i.e., an agent that can deliberate on and self-select its goals, with a motivation system that can be used to both constrain and motivate agent behavior. A human-like motivation system coupled with a goal-self concordant selection technique allows the approach to be framed as an optimization problem in which the agent selects goals that have high utility while simultaneously in harmony with its motivations. Over the agent’s operational lifespan its motivation system adjusts incrementally to more closely reflect the reality of its goal reasoning and goal pursuit experiences. Experiments performed with an ablation testing technique comparing the average utility of goals achieved in the presence and absence of a motivation system suggest that the motivated version of the system leads to pursuing more useful goals than the baseline. },
  issn={ 1573-7454 },
}

@article{Das_2025,
  title={ A CTO-based GRU model for identifying emotions from textual data },
  DOI={ doi:10.1007/s10115-025-02354-0 },
  journal={ Knowledge and Information Systems },
  author={ Shyam Sunder Jannu SolomanBehilo SebNagaraju BaydetiDushmanta Kumar Das },
  year={ 2025 },
  pages={ 1-24 },
  month={ 02 },
  abstract={ Emotions play a crucial role in human communication, influencing interactions and decision-making processes. Integrating emotional awareness into machines is increasingly important, as it enables more natural and effective interactions between humans and machines. The proposed work focuses on emotion detection using the ISEAR dataset. During preprocessing, word-intensity lexicon is utilized to extract the features, which were then categorized into positive and negative groups. A Class Topper Optimization (CTO) based Gated Recurrent Unit (GRU) model is proposed to classify the sentences into two groups, effectively framing the task as a binary classification problem. The performance of the proposed model is evaluated against other models using various metrics and resulted in 98.87% accuracy, 98.51% precision, 100% recall, 95.58% specificity and 99.25% F1 score, mean squared error 2.23%, root mean squared error 15.99% and mean absolute error 8.57% and outperformed some existing models, providing more accurate and reliable classifications. This advancement in emotion detection is significant, as it enhances the ability to understand and respond to human emotions, which is essential for applications in fields such as mental health, human-computer interactions and social media analysis. },
  issn={ 0219-3116 },
}

@article{Takahashi_2025,
  title={ TANGAN: solving Tangram puzzles using generative adversarial network },
  volume={ 55 },
  DOI={ doi:10.1007/s10489-025-06364-x },
  journal={ Applied Intelligence },
  author={ Fernanda Miyuki YamadaHarlen Costa BatageloJoão Paulo GoisHiroki Takahashi },
  year={ 2025 },
  pages={ 1-27 },
  month={ 03 },
  abstract={ While humans show remarkable proficiency in solving visual puzzles, machines often fall short due to the complex combinatorial nature of such tasks. Consequently, there is a growing interest in developing computational methods for the automatic solution of different puzzles, especially through deep learning approaches. The Tangram, an ancient Chinese puzzle, challenges players to arrange seven polygonal pieces to construct different patterns. Despite its apparent simplicity, solving the Tangram is considered an NP-complete problem, being a challenge even for the most sophisticated algorithms. Moreover, ensuring the generality and adaptability of machine learning models across different Tangram arrangements and complexities is an ongoing research problem. In this paper, we introduce a generative model specifically designed to solve the Tangram. Our model competes favorably with previous methods regarding accuracy while delivering fast inferences. It incorporates a novel loss function that integrates pixel-based information with geometric features, promoting a deeper understanding of the spatial relationships between pieces. Unlike previous approaches, our model takes advantage of the geometric properties of the Tangram to formulate a solving strategy, exploiting its inherent properties only through exposure to training data rather than through direct instruction. Extending the proposed loss function, we present a novel evaluation metric as a better fitting measure for assessing Tangram solutions than previous metrics. We further provide a new dataset containing more samples than others reported in the literature. Our findings highlight the potential of deep learning approaches in geometric problem domains. },
  issn={ 1573-7497 },
}

